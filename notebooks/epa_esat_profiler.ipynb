{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5686853f-4655-4e79-af04-fe24aac1756e",
   "metadata": {},
   "source": [
    "## Modeling Profiler\n",
    "\n",
    "Using the ESAT simulator to evaluate potential approaches to optimize modeling very large datasets.\n",
    "\n",
    "The first approach will look at implementing and validating the following workflow:\n",
    "1. Create a subset dataset of the input by randomly selecting N values from the input/uncertainty.\n",
    "2. Train a single model on that data until convergence.\n",
    "3. Use the factor profile H matrix to calculate a W for the complete dataset.\n",
    "4. Calculate Q(full)\n",
    "5. Take a new subset of the data, restart training with the prior H.\n",
    "6. Repeat until Q(full) is no longer decreasing.\n",
    "\n",
    "Run full dataset model with the same random seed and evaluate the difference in loss and factor profiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ac55f-9259-4ce1-b87e-3302d2ed00f9",
   "metadata": {},
   "source": [
    "#### Code Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9de6b9e-89a5-4db8-94d4-2226605d9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from esat.data.datahandler import DataHandler\n",
    "from esat.model.batch_sa import BatchSA\n",
    "from esat.model.sa import SA\n",
    "from esat.data.analysis import ModelAnalysis, BatchAnalysis\n",
    "from esat_eval.simulator import Simulator\n",
    "from esat.estimator import FactorEstimator\n",
    "from esat_eval.factor_catalog import FactorCatalog, Factor\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import min_weight_full_bipartite_matching\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.colors as pc\n",
    "import plotly.io as pio\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb2b9c-98ee-4482-a0f9-c06b1eeb651f",
   "metadata": {},
   "source": [
    "#### Synthetic Dataset\n",
    "\n",
    "Generate a synthetic dataset where the factor profiles and contributions are pre-determined for model output analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f6ed6eb-46b4-4d3d-8085-19009c083e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synethic dataset parameters\n",
    "seed = 1\n",
    "syn_factors = 6                # Number of factors in the synthetic dataset\n",
    "syn_features = 40              # Number of features in the synthetic dataset\n",
    "syn_samples = 50000             # Number of samples in the synthetic dataset\n",
    "outliers = True                # Add outliers to the dataset\n",
    "outlier_p = 0.10               # Decimal percent of outliers in the dataset\n",
    "outlier_mag = 1.25                # Magnitude of outliers\n",
    "contribution_max = 2           # Maximum value of the contribution matrix (W) (Randomly sampled from a uniform distribution)\n",
    "noise_mean_min = 0.03          # Min value for the mean of noise added to the synthetic dataset, used to randomly determine the mean decimal percentage of the noise for each feature.\n",
    "noise_mean_max = 0.05          # Max value for the mean of noise added to the synthetic dataset, used to randomly determine the mean decimal percentage of the noise for each feature.\n",
    "noise_scale = 0.1              # Scale of the noise added to the synthetic dataset\n",
    "uncertainty_mean_min = 0.04    # Min value for the mean uncertainty of a data feature, used to randomly determine the mean decimal percentage for each feature in the uncertainty dataset. \n",
    "uncertainty_mean_max = 0.06    # Max value for the mean uncertainty of a data feature, used to randomly determine the mean decimal percentage for each feature in the uncertainty dataset. \n",
    "uncertainty_scale = 0.01       # Scale of the uncertainty matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cd06d50-6afb-4cdf-a20c-3a487b8a7a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29-Apr-25 15:14:28 - Synthetic profiles generated\n"
     ]
    }
   ],
   "source": [
    "# Initialize the simulator with the above parameters\n",
    "simulator = Simulator(seed=seed,\n",
    "                      factors_n=syn_factors,\n",
    "                      features_n=syn_features,\n",
    "                      samples_n=syn_samples,\n",
    "                      outliers=outliers,\n",
    "                      outlier_p=outlier_p,\n",
    "                      outlier_mag=outlier_mag,\n",
    "                      contribution_max=contribution_max,\n",
    "                      noise_mean_min=noise_mean_min,\n",
    "                      noise_mean_max=noise_mean_max,\n",
    "                      noise_scale=noise_scale,\n",
    "                      uncertainty_mean_min=uncertainty_mean_min,\n",
    "                      uncertainty_mean_max=uncertainty_mean_max,\n",
    "                      uncertainty_scale=uncertainty_scale\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68bb55be-07a1-44a3-98a0-d91855b9d81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example command for passing in a custom factor profile matrix, instead of the randomly generated profile matrix.\n",
    "# my_profile = np.ones(shape=(syn_factors, syn_features))\n",
    "# simulator.generate_profiles(profiles=my_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b5983ad-0d02-4938-9943-31bf0f889414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to customize the factor contributions. Curve_type options: 'uniform', 'decreasing', 'increasing', 'logistic', 'periodic'\n",
    "# simulator.update_contribution(factor_i=0, curve_type=\"logistic\", scale=0.1, frequency=0.5)\n",
    "# simulator.update_contribution(factor_i=1, curve_type=\"periodic\", minimum=0.0, maximum=1.0, frequency=0.5, scale=0.1)\n",
    "# simulator.update_contribution(factor_i=2, curve_type=\"increasing\", minimum=0.0, maximum=1.0, scale=0.1)\n",
    "# simulator.update_contribution(factor_i=3, curve_type=\"decreasing\", minimum=0.0, maximum=1.0, scale=0.1)\n",
    "# simulator.plot_synthetic_contributions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d605314-c34a-434d-a178-81b6f6b3766d",
   "metadata": {},
   "source": [
    "#### Load Data\n",
    "Assign the processed data and uncertainty datasets to the variables V and U. These steps will be simplified/streamlined in a future version of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ee36d66-7066-44d1-ae0b-b8b3e2413bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29-Apr-25 15:14:28 - Synthetic data generated\n",
      "29-Apr-25 15:14:28 - Synthetic uncertainty data generated\n",
      "29-Apr-25 15:14:28 - Synthetic dataframes completed\n",
      "29-Apr-25 15:14:29 - Synthetic source apportionment instance created.\n"
     ]
    }
   ],
   "source": [
    "syn_input_df, syn_uncertainty_df = simulator.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a0b90b7-c68c-4d64-af4b-47033406e5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = DataHandler.load_dataframe(input_df=syn_input_df, uncertainty_df=syn_uncertainty_df)\n",
    "V, U = data_handler.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f532c5-1e71-4966-8edf-e8f68fa0bd57",
   "metadata": {},
   "source": [
    "#### Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af6db49d-8c25-467e-8456-0b53bb7aba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_col = \"Date\"                  # the index of the input/uncertainty datasets\n",
    "# factors = syn_factors               # the number of factors\n",
    "factors = 6\n",
    "method = \"ls-nmf\"                   # \"ls-nmf\", \"ws-nmf\"\n",
    "models = 20                         # the number of models to train\n",
    "init_method = \"col_means\"           # default is column means \"col_means\", \"kmeans\", \"cmeans\"\n",
    "init_norm = True                    # if init_method=kmeans or cmeans, normalize the data prior to clustering.\n",
    "seed = 42                           # random seed for initialization\n",
    "max_iterations = 20000              # the maximum number of iterations for fitting a model\n",
    "converge_delta = 0.1                # convergence criteria for the change in loss, Q\n",
    "converge_n = 25                     # convergence criteria for the number of steps where the loss changes by less than converge_delta\n",
    "verbose = True                      # adds more verbosity to the algorithm workflow on execution.\n",
    "optimized = True                    # use the Rust code if possible\n",
    "parallel = True                     # execute the model training in parallel, multiple models at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d60233-99a4-42de-9e94-6ba1ac84afce",
   "metadata": {},
   "source": [
    "### Train Batch Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20f6bb60-a22a-4be5-a9eb-0f5f93d61f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_W(V, U, H):\n",
    "    H[H <= 0.0] = 1e-8\n",
    "    W = np.matmul(V * np.divide(1, U), H.T)\n",
    "    return W\n",
    "\n",
    "def q_loss(V, U, H, W):\n",
    "    residuals = (V-np.matmul(W, H))/U\n",
    "    return np.sum(residuals)\n",
    "\n",
    "def mse(V, U, H, W):\n",
    "    WH = np.matmul(W, H)\n",
    "    residuals = ((V-WH)/U)**2\n",
    "    return np.sum(residuals)/V.size\n",
    "\n",
    "def compare_H(H1, H2):\n",
    "    correlation_matrix = np.zeros((H1.shape[0], H2.shape[0]))\n",
    "    for i in range(H1.shape[0]):\n",
    "        f1 = H1[i].astype(float)\n",
    "        for j in range(H2.shape[0]):\n",
    "            f2 = H2[j].astype(float)\n",
    "            corr_matrix = np.corrcoef(f2, f1)\n",
    "            corr = corr_matrix[0, 1]\n",
    "            r_sq = corr ** 2\n",
    "            correlation_matrix[i,j] = r_sq\n",
    "    return correlation_matrix\n",
    "            \n",
    "def plot_correlations(matrix):\n",
    "    header = [f\"Factor {i}\" for i in range(matrix.shape[0])]\n",
    "    fig = go.Figure(data=[go.Table(header=dict(values=header), cells=dict(values=matrix))])\n",
    "    fig.show()\n",
    "\n",
    "def prepare_data(V, U, i_selection):\n",
    "    _V = pd.DataFrame(V.copy()[i_selection,:])\n",
    "    _U = pd.DataFrame(U.copy()[i_selection,:])\n",
    "    \n",
    "    for f in _V.columns:\n",
    "        _V[f] = pd.to_numeric(_V[f])\n",
    "        _U[f] = pd.to_numeric(_U[f])\n",
    "    return _V.to_numpy(), _U.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a498165d-af9f-4a2f-baf4-71a575001f71",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83c5e3ec-9c15-4b07-9a21-7fe2b8248fff",
   "metadata": {},
   "source": [
    "### Version 2 of the FactorCatalog\n",
    "\n",
    "FactorCatalog V2 takes a more robust approach to grouping factor profiles from multiple models with the grouping occuring after all factor profiles have been collected. The updated procedure\n",
    "is designed to be used to investigate potential solutions for creating models for very large datasets using subsets of the data. The algorithm is described as:\n",
    "1. Specify your hyper-parameters: samples_n, batches_n, models_n, random_seed, correlation_threshold, factors_k\n",
    "2. For each batch in batches_n.\n",
    "3. Create a subset dataset using samples_n randomly selected values from V/U.\n",
    "4. Created a batchSA instance of models_n, using random_seed and factors_k.\n",
    "5. Add each output factor to the FactorCatalog (factor model_i, fit_rmse, factor_i, H)\n",
    "6. Once all batches are completed, cluster the factor collection using a constrained k-means cluster function.\n",
    "7. Score the models based upon a heuristic, such as the sum of (cluster_cor_avg*cluster_members)\n",
    "8. Evaluate the clustered profile matrix, using the cluster centroid values, using the complete dataset.\n",
    "\n",
    "The primary modification of the FactorCatalog is to use the constrained k-means clustering function for grouping 'like' factor profiles. The procedure will work by:\n",
    "1. Starting with the factors_k=clusters_n, calculate the correlation of all factors/points to the centroids, by model.\n",
    "2. Initialize the clusters by randomly creating clusters or by selecting clusters_n 'dissimilar' factors.\n",
    "3. Randomly shuffle the model order for factor assignment:\n",
    "   1. Assign the factors to the clusters by order of correlation, closer to 1.0 goes first.\n",
    "   2. If more than one factor in a model would be assigned to the same cluster assign the factor with the highest cor to that cluster and then repeat excluding that cluster(s) until all factors are assigned.\n",
    "   3. If any given factor does not have a correlation above the specified threshold, create a new cluster centered at that point.\n",
    "4. At the end of each assignment iteration, remove any cluster which has no members.\n",
    "5. Once max_assignment_n iterations is reached stop, or when a reassignment doesn't change.\n",
    "\n",
    "The constrained k-means clustered is a standard clustering approach with the exception that the distance is calculated as 1/r2 of the point to the cluster centroid. Two other differences are a) the number of clusters can increase and decrease depending on correlation threshold and by the constraint that a model can only contribute one factor to any given cluster.\n",
    "\n",
    "Once all the factors for all models have been clustered, the FactorCatalog models can be scored based upon the heuristic stated in stage A7. The best model, or any selected model, factor profile matrix (H) can then be selected for final evaluation. The factor profile H is not what was produced by the model, but is the mean values of the FactorCatalog's factor (the centroid of the cluster that those factors were assigned to). This approach allows for the factor profile values to be provided as a distributed of possible values for each feature, or demonstrating potential uncertainty in the factor profile. The clustered factor profile is then used to fit the full dataset, but keeping H constant. The loss can then be evaluated against what is calculated for a long-running brute force approach. \n",
    "\n",
    "An evaluation of the impact on the model/W matrix and loss given a random selection, MC, simulation of the factor profile would be an interesting next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d6855a-bfd9-4ff7-a864-c0a0933b9f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Factor:\n",
    "    def __init__(self,\n",
    "                 factor_id,\n",
    "                 profile,\n",
    "                 model_id\n",
    "                ):\n",
    "        self.factor_id = factor_id\n",
    "        self.profile = profile\n",
    "        self.model_id = model_id\n",
    "        self.cluster_id = None\n",
    "        self.cor = None\n",
    "\n",
    "    def assign(self, cluster_id, cor):\n",
    "        self.cluster_id = cluster_id\n",
    "        self.cor = cor\n",
    "\n",
    "    def deallocate(self):\n",
    "        self.cluster_id = None\n",
    "        self.cor = None\n",
    "\n",
    "    def distance(self, cluster):\n",
    "        f1 = np.array(self.profile).astype(float)\n",
    "        f2 = np.array(cluster).astype(float)\n",
    "        corr_matrix = np.corrcoef(f2, f1)\n",
    "        corr = corr_matrix[0, 1]\n",
    "        r_sq = corr ** 2\n",
    "        return r_sq\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self,\n",
    "                 model_id):\n",
    "        self.model_id = model_id\n",
    "        self.factors = []\n",
    "\n",
    "        self.score = None\n",
    "        \n",
    "    def add_factor(self, factor):\n",
    "        self.factors.append(factor)\n",
    "\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(self,\n",
    "                 cluster_id,\n",
    "                 centroid: np.ndarray\n",
    "                ):\n",
    "        self.cluster_id = cluster_id\n",
    "        self.centroid = centroid\n",
    "        self.factors = []\n",
    "        self.count = 0\n",
    "\n",
    "        self.mean_r2 = 0\n",
    "        self.std = 0\n",
    "        self.min_values = np.full(len(centroid), np.nan)\n",
    "        self.max_values = np.full(len(centroid), np.nan)\n",
    "        # print(f\"Initilaized cluster {self.cluster_id} with centroid: {self.centroid}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.count\n",
    "\n",
    "    def add(self, factor: Factor, cor: float):\n",
    "        factor.assign(cluster_id=self.cluster_id, cor=cor)\n",
    "        self.factors.append(factor)\n",
    "        self.count += 1\n",
    "        self.min_values = np.minimum(self.min_values, factor.profile)\n",
    "        self.max_values = np.maximum(self.max_values, factor.profile)\n",
    "        self.mean_r2 = np.mean([factor.cor for factor in self.factors])\n",
    "        self.std = np.std([factor.profile for factor in self.factors], axis=0)\n",
    "\n",
    "    def purge(self):\n",
    "        # print(f\"Purging Cluster: {self.cluster_id}, Centroid: {self.centroid}\")\n",
    "        for factor in self.factors: factor.deallocate()\n",
    "        self.factors = []\n",
    "        self.count = 0\n",
    "        self.mean_r2 = 0\n",
    "        self.std = 0\n",
    "        self.min_values = np.full(len(self.centroid), np.nan)\n",
    "        self.max_values = np.full(len(self.centroid), np.nan)\n",
    "\n",
    "    def recalculate(self):\n",
    "        if len(self.factors) > 0:\n",
    "            factor_matrix = np.array([factor.profile for factor in self.factors])\n",
    "            new_centroid = np.mean(factor_matrix, axis=0)\n",
    "            # print(f\"Recalculating Cluster: {self.cluster_id}, Centroid: {new_centroid}\")\n",
    "            self.centroid = new_centroid\n",
    "\n",
    "    def plot(self):\n",
    "        n_features = len(self.centroid)\n",
    "        factor_matrix = np.array([factor.profile for factor in self.factors])\n",
    "        \n",
    "        box_plot = go.Figure()\n",
    "        for i in range(n_features):\n",
    "            box_plot.add_trace(go.Box(\n",
    "                y=factor_matrix[:,i], \n",
    "                boxpoints=\"all\", \n",
    "                jitter=0.5,\n",
    "                whiskerwidth=0.2,\n",
    "                fillcolor=cls,\n",
    "                marker_size=2,\n",
    "                line_width=1, \n",
    "                name=f\"Feature {i+1}\")\n",
    "            )\n",
    "        box_plot.add_trace(go.Scatter(\n",
    "            x=np.range(n_features), \n",
    "            y=self.centroid, \n",
    "            name=\"Centroid\",\n",
    "            mode='markers',\n",
    "            marker=dict(color='red', size=10)\n",
    "        ))\n",
    "        box_plot.update_layout(title=\"Clustered Factor Profile\", width=1200, height=800)\n",
    "        box_plot.show()\n",
    "\n",
    "        \n",
    "class BatchFactorCatalog:\n",
    "    def __init__(self,\n",
    "                 n_factors: int,\n",
    "                 n_features: int,\n",
    "                 threshold: float = 0.8,\n",
    "                 seed: int = 42\n",
    "                ):\n",
    "        self.n_factors = n_factors\n",
    "        self.n_features = n_features\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        self.models = {}\n",
    "        self.model_count = 0\n",
    "        self.factors = {}\n",
    "        self.factor_count = 0\n",
    "\n",
    "        # Min and max values for all factor vectors, used for random initialization of the centroids in clustering\n",
    "        self.factor_min = None\n",
    "        self.factor_max = None\n",
    "\n",
    "        self.clusters = []\n",
    "        self.dropped_clusters = []\n",
    "\n",
    "        self.state = {}\n",
    "\n",
    "    def results(self):\n",
    "        results = {}\n",
    "        for cluster in self.clusters:\n",
    "            results[cluster.cluster_id] = {\n",
    "                \"count\": len(cluster),\n",
    "                \"mean_r2\": cluster.mean_r2,\n",
    "                \"std\": cluster.std\n",
    "            }\n",
    "        return results\n",
    "\n",
    "    def plot(self):\n",
    "        palette = pc.qualitative.Plotly  # You can replace 'Plotly' with 'Set3', 'Pastel', etc.\n",
    "        colors = [palette[i % len(palette)] for i in range(len(self.clusters))]\n",
    "\n",
    "        all_factors = np.array([v.profile for k, v in self.factors.items()])\n",
    "        factor_assignment = np.array([v.cluster_id for k, v in self.factors.items()])\n",
    "        all_centroids = np.array([cluster.centroid for cluster in self.clusters])\n",
    "\n",
    "        factor_pca = PCA(n_components=3)\n",
    "        pca_results = factor_pca.fit_transform(all_factors)\n",
    "        \n",
    "        pca_centroids = factor_pca.transform(all_centroids)\n",
    "        \n",
    "        df_pca = pd.DataFrame(pca_results, columns=['PC1', 'PC2', 'PC3'])\n",
    "        df_pca[\"Cluster\"] = factor_assignment\n",
    "\n",
    "        n_clusters = len(all_centroids)\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        for cluster in range(n_clusters):\n",
    "            cluster_data = df_pca[df_pca['Cluster'] == cluster]\n",
    "            if len(cluster_data) == 0:\n",
    "                continue\n",
    "            color = colors[cluster]\n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=cluster_data['PC1'],\n",
    "                y=cluster_data['PC2'],\n",
    "                z=cluster_data['PC3'],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=5,\n",
    "                    color=color,  # Assign unique color to each cluster\n",
    "                    opacity=0.8\n",
    "                ),\n",
    "                name=f\"Cluster {cluster}\"\n",
    "            ))\n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=[pca_centroids[cluster, 0]],\n",
    "                y=[pca_centroids[cluster, 1]],\n",
    "                z=[pca_centroids[cluster, 2]],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=5,\n",
    "                    color=color,  # Same color as the cluster\n",
    "                    symbol='x',\n",
    "                    opacity=0.8\n",
    "                ),\n",
    "                name=f\"Centroid {cluster}\"\n",
    "            ))\n",
    "        \n",
    "        # Update layout for better visualization\n",
    "        fig.update_layout(\n",
    "            title=\"Cluster Centroids\",\n",
    "            scene=dict(\n",
    "                xaxis_title=\"PCA1\",\n",
    "                yaxis_title=\"PCA2\",\n",
    "                zaxis_title=\"PCA3\"\n",
    "            ),\n",
    "            showlegend=False,\n",
    "            height=1200,\n",
    "            margin=dict(l=0, r=0, b=0, t=40)\n",
    "        )\n",
    "        \n",
    "        # Show the plot\n",
    "        fig.show()\n",
    "\n",
    "    def animiate(self, to_file: bool=False, base_matrix = None):\n",
    "        palette = pc.qualitative.Plotly  # You can replace 'Plotly' with 'Set3', 'Pastel', etc.\n",
    "        colors = [palette[i % len(palette)] for i in range(len(self.clusters))]\n",
    "        \n",
    "        all_factors = np.array([v.profile for k, v in self.factors.items()])\n",
    "        factor_assignment = np.array([v.cluster_id for k, v in self.factors.items()])\n",
    "        all_centroids = np.array([cluster.centroid for cluster in self.clusters])\n",
    "        \n",
    "        factor_pca = PCA(n_components=3)\n",
    "        pca_results = factor_pca.fit_transform(all_factors)\n",
    "        \n",
    "        pca_centroids = factor_pca.transform(all_centroids)\n",
    "\n",
    "        plot_base = False\n",
    "        if base_matrix not None:\n",
    "            pcs_base = factor_pca.transform(base_matrix)\n",
    "            df_base = pd.DataFrame(pcs_base, columns=['PCA1', 'PCA2', 'PCA3'])\n",
    "            plot_base = True\n",
    "        \n",
    "        df_pca = pd.DataFrame(pca_results, columns=['PCA1', 'PCA2', 'PCA3'])\n",
    "        df_pca[\"Cluster\"] = factor_assignment\n",
    "\n",
    "        frames = []\n",
    "        state0 = None\n",
    "        color_map = {}\n",
    "        total_clusters = len(self.clusters)\n",
    "        color_palette = generate_cluster_colors(total_clusters)\n",
    "        \n",
    "        for i in range(len(self.state)):\n",
    "            i_centroids = self.state[i][\"centroids\"]\n",
    "            factor_assignments = self.state[i][\"assignment\"]\n",
    "            pca_centroids = factor_pca.transform(i_centroids)\n",
    "            df_pca = pd.DataFrame(pca_results, columns=['PCA1', 'PCA2', 'PCA3'])\n",
    "            df_pca[\"Cluster\"] = factor_assignments\n",
    "            df_pca[\"text\"] = \"Factor Profile\"\n",
    "        \n",
    "            df_centroids = pd.DataFrame(pca_centroids, columns=['PCA1', 'PCA2', 'PCA3'])\n",
    "            assigned_centroids, cluster_size = np.unique(factor_assignments, return_counts=True)\n",
    "            df_centroids = df_centroids.iloc[assigned_centroids]\n",
    "            df_centroids[\"Cluster\"] = np.arange(len(assigned_centroids))\n",
    "            df_centroids[\"text\"] = \"Centroid\"\n",
    "        \n",
    "            # Update color map for new clusters\n",
    "            for cluster in np.unique(assigned_centroids):\n",
    "                if cluster not in color_map:\n",
    "                    color_map[cluster] = color_palette[len(color_map) % len(color_palette)]\n",
    "        \n",
    "            # Assign colors to points based on the fixed color map\n",
    "            colors = [color_map[cluster] for cluster in assigned_centroids]\n",
    "            \n",
    "            data = []\n",
    "            data.append(go.Scatter3d(\n",
    "                x=df_pca[\"PCA1\"],\n",
    "                y=df_pca[\"PCA2\"],\n",
    "                z=df_pca[\"PCA3\"],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=5,\n",
    "                    color=df_pca[\"Cluster\"],  # Color points by class\n",
    "                    colorscale=colors,  # Choose a colorscale\n",
    "                    colorbar=dict(title=\"Cluster\")  # Add a colorbar\n",
    "                ),\n",
    "                text=df_pca[\"text\"],\n",
    "                hoverinfo='text'\n",
    "            ))\n",
    "            assigned_cluster_size = cluster_size * 5\n",
    "            data.append(go.Scatter3d(\n",
    "                x=df_centroids['PCA1'],\n",
    "                y=df_centroids['PCA2'],\n",
    "                z=df_centroids['PCA3'],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=assigned_cluster_size,\n",
    "                    color=df_centroids[\"Cluster\"],\n",
    "                    colorscale=colors,  # Same color as the cluster\n",
    "                    opacity=0.3\n",
    "                ),\n",
    "                text=df_centroids[\"text\"],\n",
    "                hoverinfo='text'\n",
    "            ))\n",
    "            data.append(go.Scatter3d(\n",
    "                x=df_centroids['PCA1'],\n",
    "                y=df_centroids['PCA2'],\n",
    "                z=df_centroids['PCA3'],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=3,\n",
    "                    color=df_centroids[\"Cluster\"],\n",
    "                    colorscale=colors,  # Same color as the cluster\n",
    "                    symbol='x',\n",
    "                ),\n",
    "                text=df_centroids[\"text\"],\n",
    "                hoverinfo='text'\n",
    "            ))\n",
    "            if plot_base:\n",
    "                data.append(go.Scatter3d(\n",
    "                    x=df_base['PCA1'],\n",
    "                    y=df_base['PCA2'],\n",
    "                    z=df_base['PCA3'],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=3,\n",
    "                        color=\"red,\n",
    "                    )\n",
    "                ))\n",
    "            \n",
    "            if i == 0:\n",
    "                state0 = data\n",
    "            frames.append(\n",
    "                go.Frame(data=data, \n",
    "                         layout=go.Layout(\n",
    "                             annotations=[\n",
    "                                dict(\n",
    "                                    x=1,\n",
    "                                    y=1,\n",
    "                                    showarrow=False,\n",
    "                                    text=f\"Iteration: {i + 1}/{len(factor_catalog.state)}\",\n",
    "                                    xref=\"paper\",\n",
    "                                    yref=\"paper\",\n",
    "                                    font=dict(size=14)\n",
    "                                )\n",
    "                            ]\n",
    "                        ),\n",
    "                         name=str(i)\n",
    "                        )\n",
    "            )\n",
    "        df_pca0 = pd.DataFrame(pca_results, columns=['PC1', 'PC2', 'PC3'])\n",
    "        camera = dict(\n",
    "            eye=dict(x=1.25, y=1.25, z=1.25)  \n",
    "        )\n",
    "        fig = go.Figure(\n",
    "            data=state0,\n",
    "            layout=go.Layout(\n",
    "                title=\"Factor Profile Clustering\",\n",
    "                height=1000,\n",
    "                width=1000,\n",
    "                scene=dict(\n",
    "                    xaxis=dict(range=[df_pca0['PC1'].min()-0.25, df_pca0['PC1'].max()+0.25], autorange=False),\n",
    "                    yaxis=dict(range=[df_pca0['PC2'].min()-0.25, df_pca0['PC2'].max()+0.25], autorange=False),\n",
    "                    zaxis=dict(range=[df_pca0['PC3'].min()-0.25, df_pca0['PC3'].max()+0.25], autorange=False),\n",
    "                    aspectmode=\"manual\",\n",
    "                    aspectratio=dict(x=1, y=1, z=1)\n",
    "                ),\n",
    "                updatemenus=[dict(\n",
    "                    type=\"buttons\",\n",
    "                    showactive=False,\n",
    "                    buttons=[\n",
    "                        dict(label=\"Play\",\n",
    "                             method=\"animate\",\n",
    "                             args=[None, dict(frame=dict(duration=500, redraw=True), fromcurrent=True, mode=\"immediate\")]),\n",
    "                        dict(label=\"Pause\",\n",
    "                             method=\"animate\",\n",
    "                             args=[[None], dict(frame=dict(duration=0, redraw=False), mode=\"immediate\")]),\n",
    "                        dict(\n",
    "                            args=[[0], dict(frame=dict(duration=0, redraw=True), mode=\"immediate\")],\n",
    "                            label=\"Reset\",\n",
    "                            method=\"animate\"),\n",
    "                    ]\n",
    "                )],\n",
    "                annotations=[\n",
    "                    dict(\n",
    "                        x=1,\n",
    "                        y=1,\n",
    "                        showarrow=False,\n",
    "                        text=\"Iteration: NA\",\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\",\n",
    "                        font=dict(size=14)\n",
    "                    )\n",
    "                ],\n",
    "                showlegend=False\n",
    "            ),\n",
    "            frames=frames\n",
    "        )\n",
    "        # Show the figure\n",
    "        if to_file:\n",
    "            pio.write_html(fig, file=\"factor_clustering.html\", auto_open=False)\n",
    "        else:\n",
    "            fig.show()\n",
    "        \n",
    "    def add_model(self, model: SA, norm: bool = True):\n",
    "        model_id = self.model_count\n",
    "        model_factor_ids = []\n",
    "        norm_H = model.H / np.sum(model.H, axis=0)\n",
    "        i_model = Model(model_id=model_id)\n",
    "        for i in range(model.H.shape[0]):\n",
    "            factor_id = self.factor_count\n",
    "            self.factor_count += 1\n",
    "            model_factor_ids.append(factor_id)\n",
    "            i_H = norm_H if norm else model.H \n",
    "            factor = Factor(factor_id=factor_id, profile=i_H[i], model_id=model_id)\n",
    "            \n",
    "            i_model.add_factor(factor)\n",
    "            self.factors[factor_id] = factor\n",
    "            self.update_ranges(i_H[i])\n",
    "            \n",
    "        self.models[str(model_id)] = i_model\n",
    "        self.model_count += 1\n",
    "\n",
    "    def compare(self, matrix):\n",
    "        compare_results = {}\n",
    "        for i in range(matrix.shape[0]):\n",
    "            i_H = matrix[i]\n",
    "            i_cor = 0.0\n",
    "            best_cluster = None\n",
    "            for cluster in self.clusters:\n",
    "                cluster_cor = self.distance(i_H, cluster.centroid)\n",
    "                if cluster_cor > i_cor:\n",
    "                    i_cor = cluster_cor\n",
    "                    best_cluster = cluster.cluster_id\n",
    "            compare_results[i] = {\"cluster_id\": best_cluster, \"r2\": i_cor}\n",
    "        return compare_results\n",
    "\n",
    "    def score(self):\n",
    "        # iterate over all models, get the membership count the cluster that each factor is mapped to.\n",
    "        for model_id, model in self.models.items():\n",
    "            model_score = 0.0\n",
    "            for factor in model.factors:\n",
    "                factor_score = len(self.clusters[factor.cluster_id])\n",
    "                model_score += factor_score\n",
    "            model.score = model_score\n",
    "\n",
    "    def update_ranges(self, factor):\n",
    "        if self.factor_min is None and self.factor_max is None:\n",
    "            self.factor_min = copy.copy(factor)\n",
    "            self.factor_max = copy.copy(factor)\n",
    "        else:\n",
    "            self.factor_min = np.minimum(self.factor_min, factor)\n",
    "            self.factor_max = np.maximum(self.factor_max, factor)\n",
    "\n",
    "    def initialize_clusters(self):\n",
    "        for k in range(self.n_factors):\n",
    "            new_centroid = np.zeros(self.n_features)\n",
    "            for i in range(self.n_features):\n",
    "                i_v = self.rng.uniform(low=self.factor_min[i], high=self.factor_max[i])\n",
    "                new_centroid[i] = i_v\n",
    "            cluster = Cluster(cluster_id=k, centroid=new_centroid)\n",
    "            self.clusters.append(cluster)\n",
    "\n",
    "    def purge_clusters(self):\n",
    "        for cluster in self.clusters:\n",
    "            cluster.purge()\n",
    "\n",
    "    def distance(self, factor1, factor2):\n",
    "        f1 = np.array(factor1).astype(float)\n",
    "        f2 = np.array(factor2).astype(float)\n",
    "        corr_matrix = np.corrcoef(f2, f1)\n",
    "        corr = corr_matrix[0, 1]\n",
    "        r_sq = corr ** 2\n",
    "        return r_sq\n",
    "\n",
    "    def calculate_centroids(self):\n",
    "        new_centroid_matrix = []\n",
    "        for cluster in self.clusters: \n",
    "            if cluster is not None:\n",
    "                cluster.recalculate()\n",
    "                new_centroid_matrix.append(cluster.centroid)\n",
    "        return np.array(new_centroid_matrix)\n",
    "\n",
    "    def cluster_cleanup(self):\n",
    "        drop_clusters = []\n",
    "        for i in range(len(self.clusters)):\n",
    "            cluster_i = self.clusters[i]\n",
    "            if cluster_i is None:\n",
    "                for j in range(i+1, len(self.clusters)):\n",
    "                    cluster_j = self.clusters[j]\n",
    "                    ij_cor = self.distance(cluster_i.centroid, cluster_j.centroid)\n",
    "                    if ij_cor > self.threshold:\n",
    "                        smaller_cluster = i if len(cluster_i) < len(cluster_j) else j\n",
    "                        if smaller_cluster not in drop_clusters:\n",
    "                            drop_clusters.append(smaller_cluster)\n",
    "        new_clusters = []\n",
    "        new_centroid_matrix = []\n",
    "        for i in range(len(self.clusters)):\n",
    "            if i not in drop_clusters:\n",
    "                new_clusters.append(self.clusters[i])\n",
    "                new_centroid_matrix.append(self.clusters[i].centroid)\n",
    "            else:\n",
    "                new_clusters.append(None)\n",
    "                self.dropped_clusters.append(i)\n",
    "        self.clusters = new_clusters\n",
    "        return np.array(new_centroid_matrix)\n",
    "\n",
    "    def save_state(self, iteration):\n",
    "        factor_assignment = np.array([v.cluster_id for k, v in factor_catalog.factors.items()])\n",
    "        all_centroids = np.array([cluster.centroid for cluster in factor_catalog.clusters])\n",
    "        self.state[iteration] = {\"assignment\":factor_assignment, \"centroids\": all_centroids}\n",
    "\n",
    "    def run(self, max_iterations: int = 20):\n",
    "        self.initialize_clusters()        \n",
    "        centroids = self.calculate_centroids()\n",
    "        converged = False\n",
    "        current_iter = 0\n",
    "        with tqdm(total=max_iterations*len(self.models), desc=\"Running clustering. N Clusters: NA, Added: NA\") as pbar:\n",
    "            while not converged:\n",
    "                if current_iter >= max_iterations:\n",
    "                    logger.info(f\"Factor clustering did not converge after {max_iterations} iterations.\")\n",
    "                    break\n",
    "                self.purge_clusters()\n",
    "    \n",
    "                model_list = self.rng.permutation(list(self.models.keys()))\n",
    "                for model_i in model_list:\n",
    "                    pbar.update(1)\n",
    "                    model_factors = [factor.factor_id for factor in self.models[model_i].factors]\n",
    "                    factor_dist = {}\n",
    "                    factor_hi = {}\n",
    "                    # Calculate distances for all factors in the model to all centroids and then order the distances.\n",
    "                    for factor_i in model_factors:\n",
    "                        distances = [(j, self.distance(self.factors[factor_i].profile, cluster.centroid)) for j, cluster in enumerate(self.clusters) if cluster is not None]\n",
    "                        distances.sort(key=lambda x: x[1], reverse=True)\n",
    "                        factor_dist[str(factor_i)] = distances\n",
    "                        factor_hi[str(factor_i)] = distances[0]\n",
    "                    already_assigned = []\n",
    "                    factor_hi = dict(sorted(factor_hi.items(), key=lambda x: x[1], reverse=True))\n",
    "                    # Assign factors to clusters, if model hasn't contributed to the cluster already and if the correlation is above the threshold\n",
    "                    for factor_id in factor_hi.keys():\n",
    "                        # iterate through list of clusters in order of highest correlation.\n",
    "                        cluster_idx = -1\n",
    "                        for cluster_i, correlation_i in factor_dist[factor_id]:\n",
    "                            if cluster_i not in already_assigned and correlation_i >= self.threshold:\n",
    "                                cluster_idx = cluster_i\n",
    "                                break\n",
    "                        if cluster_idx != -1:\n",
    "                            self.clusters[cluster_idx].add(factor=self.factors[int(factor_id)], cor=factor_hi[factor_id][1])\n",
    "                            already_assigned.append(cluster_idx)\n",
    "                        else:\n",
    "                            new_cluster_id = self.dropped_clusters.pop() if len(self.dropped_clusters) > 0 else len(self.clusters)\n",
    "                                \n",
    "                            new_cluster = Cluster(cluster_id=new_cluster_id, centroid=self.factors[int(factor_id)].profile)\n",
    "                            new_cluster.add(factor=self.factors[int(factor_id)], cor=1.0)\n",
    "                            if new_cluster_id == len(self.clusters):\n",
    "                                self.clusters.append(new_cluster)\n",
    "                            else:\n",
    "                                self.clusters[new_cluster_id] = new_cluster\n",
    "                            already_assigned.append(new_cluster_id)\n",
    "    \n",
    "                    # Recalculate centroids of clusters\n",
    "                    self.save_state(iteration=current_iter)\n",
    "                    new_centroids = self.calculate_centroids()\n",
    "                    new_centroids = self.cluster_cleanup()\n",
    "\n",
    "                    if new_centroids.shape == centroids.shape:\n",
    "                        if np.sum(new_centroids - centroids) == 0.0:\n",
    "                            converge = True\n",
    "                    pbar.set_description(f\"Running clustering. N Clusters: {len(centroids)}, Added: {len(new_centroids) - len(centroids)}\")\n",
    "                    centroids = new_centroids\n",
    "                current_iter += 1\n",
    "        self.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ca56a-a1a7-47fe-ab68-c743fab19038",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "rng = np.random.default_rng(seed)\n",
    "batch_size = 2000\n",
    "max_batches = 10\n",
    "i_batches = 0\n",
    "n_models = 10\n",
    "max_iter = 20000\n",
    "\n",
    "i_H = None\n",
    "i_selection = rng.choice(syn_samples, size=batch_size, replace=False, shuffle=True)\n",
    "i_V, i_U = prepare_data(V=V, U=U, i_selection=i_selection)\n",
    "\n",
    "factor_catalog = BatchFactorCatalog(n_factors=factors, n_features=i_V.shape[1], threshold=0.8, seed=42)\n",
    "\n",
    "t0 = time.time()\n",
    "change_p = 0.1\n",
    "with tqdm(range(max_batches*2), desc=\"Generating subset profiles\") as pbar:\n",
    "    for i in range(max_batches):\n",
    "        if i > 0:\n",
    "            j_selection = rng.choice(syn_samples, size=int(batch_size*change_p), replace=False, shuffle=True)\n",
    "            idx_change = rng.choice(batch_size, size=int(batch_size*change_p), replace=False, shuffle=True)\n",
    "            i_selection[idx_change] = j_selection\n",
    "            i_V, i_U = prepare_data(V=V, U=U, i_selection=i_selection)\n",
    "\n",
    "        batch_sa = BatchSA(V=i_V, U=i_U, factors=factors, models=n_models, method=method, seed=rng.integers(low=0, high=1e8), max_iter=max_iter,\n",
    "                            converge_delta=converge_delta, converge_n=converge_n, verbose=False)\n",
    "        _ = batch_sa.train()\n",
    "        pbar.update(1)\n",
    "        for sa in batch_sa.results:\n",
    "            factor_catalog.add_model(model=sa, norm=True)\n",
    "        \n",
    "        pbar.set_description(f\"Generating subset profiles.\")\n",
    "t1 = time.time()\n",
    "logger.info(f\"Runtime: {((t1-t0)/60):.2f} min(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c070656-72c6-4b5d-a72e-5be2d225f6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = time.time()\n",
    "factor_catalog.run(max_iterations=50)\n",
    "t3 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6006ad8-760d-4792-8658-9c4acce24535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# factor_catalog.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbcc9c1-3077-4be1-ac27-b1db8de90601",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_scores = {}\n",
    "for model_id, model in factor_catalog.models.items():\n",
    "    model_factors = []\n",
    "    for factor in model.factors:\n",
    "        model_factors.append({\"id\": factor.factor_id, \"cluster_id\": factor.cluster_id, \"cluster_count\": len(factor_catalog.clusters[factor.cluster_id])})\n",
    "    model_scores[model_id] = {\"score\": model.score, \"factors\": model_factors}\n",
    "model_scores = dict(sorted(model_scores.items(), key=lambda item: item[1][\"score\"], reverse=True))\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1006dc1-00db-4949-9e9a-a40ddda6153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_unique(fg, count_threshold: int = 1):\n",
    "    model_scores = {}\n",
    "    cluster_count = {}\n",
    "    for model_id, model in fg.models.items():\n",
    "        model_factors = []\n",
    "        for factor in model.factors:\n",
    "            model_factors.append({\"id\": factor.factor_id, \"cluster_id\": factor.cluster_id, \"cluster_count\": len(fg.clusters[factor.cluster_id])})\n",
    "            if factor.cluster_id not in cluster_count.keys():\n",
    "                cluster_count[factor.cluster_id] = len(fg.clusters[factor.cluster_id])\n",
    "        model_scores[model_id] = {\"score\": model.score, \"factors\": model_factors}\n",
    "    model_scores = dict(sorted(model_scores.items(), key=lambda item: item[1][\"score\"], reverse=True))\n",
    "\n",
    "    added_map = []\n",
    "    unique_models = {}\n",
    "    for model_id, model in model_scores.items():\n",
    "        model_mapping = {\"score\": model[\"score\"]}\n",
    "        cluster_mapping = {}\n",
    "        f_map = []\n",
    "        for factor in model[\"factors\"]:\n",
    "            cluster_mapping[factor[\"id\"]] = factor[\"cluster_id\"]\n",
    "            f_map.append(factor[\"cluster_id\"])\n",
    "        f_map = sorted(f_map)\n",
    "        if len(added_map) == 0:\n",
    "            added_map.append(f_map)\n",
    "            unique_models[model_id] = cluster_mapping\n",
    "        else:\n",
    "            add = False\n",
    "            for i_map in added_map:\n",
    "                if f_map != i_map:\n",
    "                    dif = list(set(f_map) - set(i_map))\n",
    "                    for d in dif:\n",
    "                        if cluster_count[d] > count_threshold:\n",
    "                            add = True\n",
    "            if add:\n",
    "                added_map.append(f_map)\n",
    "                unique_models[model_id] = cluster_mapping\n",
    "    return unique_models\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda33273-19f1-4a70-9b65-a0cf82925e10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_models = select_unique(fg=factor_catalog)\n",
    "unique_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f307af9c-06ec-44de-b898-718f9a9f4024",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = list(unique_models.keys())[0]\n",
    "i_factors = list(unique_models[i].values())\n",
    "i_H = np.array([cluster.centroid for cluster in factor_catalog.clusters if cluster.cluster_id in i_factors])\n",
    "i_H = i_H / i_H.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1d82ebf-b3cd-4e25-aa4a-796f73aabc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29-Apr-25 16:28:45 - ------------\t\tModel Details\t\t-----------\n",
      "29-Apr-25 16:28:45 - \tMethod: ls-nmf\t\t\t\tFactors: 6\n",
      "29-Apr-25 16:28:45 - \tNumber of Features: 40\t\t\tNumber of Samples: 50000\n",
      "29-Apr-25 16:28:45 - \tRandom Seed: 42\n",
      "29-Apr-25 16:28:45 - ---------------\t\tModel Results\t\t--------------\n",
      "29-Apr-25 16:28:45 - \tQ(true): 13857819.0000\t\t\tQ(robust): 11416581.0000\n",
      "29-Apr-25 16:28:45 - \tMSE(true): 6.9289\t\t\tMSE(robust): 5.7083\n",
      "29-Apr-25 16:28:45 - \tConverged: True\t\t\t\tConverge Steps: 11786\n",
      "29-Apr-25 16:28:45 - ------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 48min 12s\n",
      "Wall time: 1h 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "base_sa = SA(factors=factors, method=method, V=V, U=U, seed=seed, verbose=True)\n",
    "base_sa.initialize(H=None, W=None)\n",
    "_ = base_sa.train(max_iter=50000, converge_delta=converge_delta, converge_n=converge_n)\n",
    "base_sa.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d54240-98b4-4c3e-ad34-2013a96b5782",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t4 = time.time()\n",
    "final_sa1 = SA(factors=factors, method=method, V=V, U=U, seed=seed, verbose=True)\n",
    "final_sa1.initialize(H=i_H, W=None)\n",
    "_ = final_sa1.train(max_iter=50000, converge_delta=converge_delta, converge_n=converge_n, hold_h=True)\n",
    "t5 = time.time()\n",
    "final_sa1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363c65ca-03df-489d-8f3b-3438cdc23705",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t6 = time.time()\n",
    "final_sa2 = SA(factors=factors, method=method, V=V, U=U, seed=seed, verbose=True)\n",
    "final_sa2.initialize(H=final_sa1.H, W=final_sa1.W)\n",
    "_ = final_sa2.train(max_iter=50000, converge_delta=converge_delta, converge_n=converge_n, hold_h=False)\n",
    "t7 = time.time()\n",
    "final_sa2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b184b735-275f-433b-ae9f-beb6a9443ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_runtime = ((t1-t0) + (t3-t2) + (t5-t4) + (t7-t6))/60\n",
    "print(f\"Total Profiler Runtime: {total_runtime:.2f} min(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4599a1-58ed-47f8-8689-495ca31a5180",
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = list(unique_models.keys())[1]\n",
    "i_factors = list(unique_models[i1].values())\n",
    "i_H1 = np.array([cluster.centroid for cluster in factor_catalog.clusters if cluster.cluster_id in i_factors])\n",
    "i_H1 = i_H1 / i_H1.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb37a5da-d958-4b73-a725-fdc5d3d556c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t4 = time.time()\n",
    "final_sa3 = SA(factors=factors, method=method, V=V, U=U, seed=seed, verbose=True)\n",
    "final_sa3.initialize(H=i_H1, W=None)\n",
    "_ = final_sa3.train(max_iter=50000, converge_delta=converge_delta, converge_n=converge_n, hold_h=True)\n",
    "t5 = time.time()\n",
    "final_sa3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4659b3fd-1364-4281-8ba8-6048140dd875",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t6 = time.time()\n",
    "final_sa4 = SA(factors=factors, method=method, V=V, U=U, seed=seed, verbose=True)\n",
    "final_sa4.initialize(H=final_sa3.H, W=final_sa3.W)\n",
    "_ = final_sa4.train(max_iter=50000, converge_delta=converge_delta, converge_n=converge_n, hold_h=False)\n",
    "t7 = time.time()\n",
    "final_sa4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a979fe-efee-4cec-9221-ce5926975bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81164b9b-465d-4587-9b50-f32fba3e84ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f56fe2-889f-4fbd-b8e6-8cce523be476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
