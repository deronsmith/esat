{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa905fd2-fa08-40f6-8a0c-b40e7927ca5c",
   "metadata": {},
   "source": [
    "## EPA ESAT Algorithm Details\n",
    "\n",
    "#### Summary\n",
    "The details provided in this notebook are focused on describing two algorithms which are capable of meeting the constraints of PMF5, produce models which have a high correlation with the outputs of PMF5, and are able to generate models with a loss value that matches or is lower than PMF5. The algorithm used in PMF5, Multi-Linear Engine 2 (ME-2), was built upon the original ME-1 algorithm that is detailed in the publication by Pentti Paatero [*The Multilinear Engine: A Table-Driven, Least Squares Program for Solving Multilinear \n",
    "Problems, including the n-Way Parallel Factor Analysis Mode*](https://www.jstor.org/stable/1390831). The algorithm uses a modified conjugate gradient method using projection to maintain non-negativity of the factor profile matrix. The algorithm falls under the category of a semi-NMF algorithm, as not all the matrices are required to be positive. The reason for not fully enforcing non-negativity on the matrices is to help prevent a higher positive value bias due to having uncertainty and small values in the input datasets. Another reason was to allow datasets which may have small negative values in the inputs because of how the data was collected, where the input data represents a distribution rather than a single data point.\n",
    "\n",
    "#### Algorithm Requirements\n",
    "The constraints and requirements of the PMF5 algorithm which had to be considered in the new ESAT algorithms are:\n",
    " - The loss function had to remain the same as the ME2 loss function.\n",
    " - The output of ESAT must be able to produce results which have a high correlation with the ME2 output. Reproducing the output of ME2 exactly is highly improbable due to any differences in the update procedure and the randomness of the starting state.\n",
    "   - A correlation of greater than 0.9 averaged across the factor profile, factor contributions and the concentration output.\n",
    "   - A correlation of greater than 0.9 for the factor profile.\n",
    " - The algorithm needs to properly function when negative values are present in the input data, and allow for negative values to be present in the factor contribution matrix.\n",
    " - The output model needs to have a loss value that is comparable to PMF5.\n",
    " - The algorithm should be as fast or faster than ME2.\n",
    "#### Loss Function\n",
    "The loss function that is used in ME2, and is described in the PMF5 User's Manual is defined as:\n",
    "$$ Q = \\sum_{i=1}^n \\sum_{j=1}^m \\bigg[ \\frac{V_{ij} - \\sum_{k=1}^K W_{ik} H_{kj}}{U_{ij}} \\bigg]^2 $$\n",
    "here $V$ is the input data matrix of features (columns=$M$) by samples (rows=$N$), $U$ is the uncertainty matrix of the input data matrix, $W$ is the factor contribution matrix of samples by factors=$k$, $H$ is the factor profile of factors by features.\n",
    "\n",
    "#### Convergence\n",
    "These algorithms all use a similar convergence criteria as the stopping condition. PMF5 has a tiered approach which is described in the User's Manual. The two ESAT algorithms offers two parameters which can be used to adjust the convergence criteria and allow for tuning of the final output. The two parameters are *converge_delta* and *converge_n*, and is simply the number of steps, *converge_n* where the change in $Q$ is less than *converge_delta* the model is considered converged and updates stop. These values in testing are typically set to *converge_delta* = 0.1 and *converge_n* = 10 (these values were chosen to speed up model convergence during testing and development).\n",
    "\n",
    "#### Initial Conditions\n",
    "The performance of ESAT algorithms is very sensitive to the initial conditions of the model, in this case the choices for the $W$ and $H$ matrices. There are multiple methods for initialization that are typically used for NMF. The method used by PMF5/ME2 is unknown. For ESAT, we have provided three different methods of initialization:\n",
    " - Random sampling from a normal distribution using the square root of the mean values of the input dataset $V$ by row $N$ for $W$ and by column $M$ for $H$. This is the default method in most NMF packages.\n",
    " - K-Means clustering, where the input dataset is normalized (can also be clustered without normalization) and $k$ clusters calculated. Allocation of a factor to a cluster is set to 1.0 and all other values are equal to $\\frac{1}{k}$.\n",
    " - C-Means clustering, also known as fuzzy k-means clustering. Which is similar to K-Means but assignment to a cluster is not a binary value but continuous as calculated by the distance to the cluster centroids and the ratio of those distances.\n",
    "Other methods of initialization exist but these are what we are currently providing in NMF-PY.\n",
    "\n",
    "### NMF-PY Algorithms\n",
    "We have implemented two algorithms, one of which fully satisfies the algorithm requirements stated above and another which satisfies all by the condition of allowing for negative values.\n",
    "\n",
    "#### LS-NMF\n",
    "The first algorithm option we provide is a well documented algorithm called *LS-NMF*, least-squares nmf, and is available in the R NMF package. The ls-nmf algorithm is documented in [*LS-NMF: A modified non-negative matrix factorization algorithm utilizing uncertainty estimates*](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-7-175) and the R NMF package can be found at [https://cran.r-project.org/web/packages/NMF/index.html](https://cran.r-project.org/web/packages/NMF/index.html). The ESAT versions of these algorithms converts the uncertainty of $U$ into weights defined as $Uw = \\frac{1}{U^2}$. The update equations then become:\n",
    "\n",
    "$$ H_{t+1} = H_t \\circ \\frac{W_t (V \\circ Uw)}{W_t {((W_{t}H_{t}) \\circ Uw)}} $$\n",
    "\n",
    "$$ W_{t+1} = W_t \\circ \\frac{(V \\circ Uw) H_{t+1}}{((W_{t}H_{t+1})\\circ Uw )H_{t+1}} $$\n",
    "\n",
    "The ls-nmf algorithm requires that all matrices be non-negative but is able to produce models with a lower loss value than PMF5 and is significantly faster than ME2. We include this algorithm as an option for when full non-negativity is permitted in the models because of the algorithm's performance and efficiency.\n",
    "\n",
    "#### Weighted Semi-NMF\n",
    "The weighted semi-nmf (ws-nmf) algorithm is a more complicated algorithm that more closely resembles the ME1 algorithm. The ws-nmf algorithm satisfies all of the requirements for a replacement algorithm of PMF5. The implemented algorithm was developed utilizing elements from two different publications, neither of which provided the complete algorithm we required for NMF-PY. [*Semi-NMF and Weighted Semi-NMF Algorithms Comparison*](https://www2.dc.ufscar.br/~marcela/anaisKDMiLe2014/artigos/FULL/kdmile2014_Artigo15.pdf) provides an overview of the semi-nmf algorithm and part of the update equation for ws-nmf. [*Convex and Semi-Nonnegative Matrix Factorizations*](https://ieeexplore.ieee.org/abstract/document/4685898) provides details on a complete algorithm for the non-weighted semi-nmf algorithm. Using these details we developed a complete update algorithm which may not yet have been published, further literature review is necessary to determine the novelty of the algorithm. \n",
    "\n",
    "As in the ls-nmf algorithm, the uncertainties are converted to weights $Uw$. In both algorithms, the loss function remains the same and uses the uncertainty and not the weights to maintain consistency with PMF5. The update equations for ws-nmf are:\n",
    "\n",
    "$$ W_{t+1,i} = (H^{T}Uw_{i}^{d}H)^{-1}(H^{T}Uw_{i}^{d}V_{i})$$\n",
    "$$ H_{t+1,i} = H_{t, i}\\sqrt{\\frac{((V^{T}Uw)W_{t+1})_{i}^{+} + [H_{t}(W_{t+1}^{T}Uw W)^{-}]_{i}}{((V^{T}Uw)W_{t+1})_{i}^{-} + [H_{t}(W_{t+1}^{T}Uw W)^{+}]_{i}}}$$\n",
    "\n",
    "Each matrix requires a separate calculation for each sample=$N$ for $W$ and each feature=$M$ for $H$ and is indicated by the $i$ index, increasing the computational complexity of the algorithm. $Uw_{i}^{d}$ is the diagonal matrix created from the $i$th column or row of $Uw$ depending on which matrix is being updated. The first section of the update equation for $W_{t+1,i}$ requires calculating the inverse which is only possible if the determinant is not equal to zero, in which case the pseudo-inverse is used. The calculation of $H_{t+1}$ requires several additional steps. The positive and negative values from $W$ are separated with $W^{-} = \\frac{(|W| - W)}{2.0}$ and $W^{+} = \\frac{(|W| + W)}{2.0}$. \n",
    "\n",
    "#### Optimization\n",
    "The algorithms are intended to be used directly as a python package, through Jupyter notebooks and eventually through a web application. With this in mind, the performance of the code must be considered during implementation. Here are a few approaches that have been taken to optimize the code/algorithm (with performance metrics shown later):\n",
    " 1. Using Python 3.12\n",
    " 2. Use of parallelization for batch modeling, fitting multiple models at a time.\n",
    " 3. The algorithms have also be written in Rust, a low level language, which provides increased memory efficiency and decreased runtime.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2328a458-c62a-402f-a80f-4a27ef19162a",
   "metadata": {},
   "source": [
    "### LS-NMF Example\n",
    "Here is an example of how to use the code and generate either a single or multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c44876bb-9c73-4584-86a5-34cb7e6b5230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook imports\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a4e03c-53b2-418a-8f62-32b1f11895ad",
   "metadata": {},
   "source": [
    "##### Sample Datasets\n",
    "PMF5 comes with three sample datasets which we will use in the code demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1500c943-19f6-424a-ad2e-76372b0ae12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baton Rouge Dataset\n",
    "br_input_file = os.path.join(\"D:\\\\\", \"projects\", \"esat\", \"data\", \"Dataset-BatonRouge-con.csv\")\n",
    "br_uncertainty_file = os.path.join(\"D:\\\\\", \"projects\", \"esat\", \"data\", \"Dataset-BatonRouge-unc.csv\")\n",
    "br_output_path = os.path.join(\"D:\\\\\", \"projects\", \"esat\", \"output\", \"BatonRouge\")\n",
    "# Baltimore Dataset\n",
    "b_input_file = os.path.join(\"D:\\\\\", \"projects\", \"esat\", \"data\", \"Dataset-Baltimore_con.txt\")\n",
    "b_uncertainty_file = os.path.join(\"D:\\\\\", \"projects\", \"esat\", \"data\", \"Dataset-Baltimore_unc.txt\")\n",
    "b_output_path = os.path.join(\"D:\\\\\", \"projects\", \"esat\", \"output\", \"Baltimore\")\n",
    "# Saint Louis Dataset\n",
    "sl_input_file = os.path.join(\"D:\\\\\", \"projects\", \"esat\", \"data\", \"Dataset-StLouis-con.csv\")\n",
    "sl_uncertainty_file = os.path.join(\"D:\\\\\", \"projects\", \"esat\", \"data\", \"Dataset-StLouis-unc.csv\")\n",
    "sl_output_path = os.path.join(\"D:\\\\\", \"projects\", \"esat\", \"output\", \"StLouis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9293bbd-7ec7-42cb-95bf-a6183cc8c28c",
   "metadata": {},
   "source": [
    "##### Code Imports\n",
    "We import the modules for the model and a datahandler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84489faf-2582-4f4b-9f6a-db0ffe091659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.data.datahandler import DataHandler\n",
    "from python.model.sa import SA\n",
    "from python.model.batch_sa import BatchSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261b9e92-94c3-4417-b971-2e6b3c218898",
   "metadata": {},
   "source": [
    "##### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2cb597f-f6d4-476b-be0b-f233cc3fe9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_col = \"Date\"                  # the index of the input/uncertainty datasets\n",
    "factors = 6                         # the number of factors\n",
    "method = \"ls-nmf\"                   # \"ls-nmf\", \"ws-nmf\"\n",
    "init_method = \"col_means\"           # default is column means \"col_means\", \"kmeans\", \"cmeans\"\n",
    "init_norm = True                    # if init_method is either kmeans or cmeans, whether to normalize the data prior to clustering.\n",
    "seed = 42                           # seed = 26586, most comparable model to PMF5 currently found\n",
    "max_iterations = 20000              # the maximum number of iterations for fitting a model\n",
    "converge_delta = 0.1                # convergence criteria for the change in loss, Q\n",
    "converge_n = 10                     # convergence criteria for the number of steps where the loss changes by less than converge_delta\n",
    "dataset = \"br\"                      # \"br\": Baton Rouge, \"b\": Baltimore, \"sl\": St Louis\n",
    "verbose = True                      # adds more verbosity to the algorithm workflow on execution.\n",
    "optimized = True                    # use the Rust code if possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf732cf7-f26d-4b53-a883-dcb185f839a5",
   "metadata": {},
   "source": [
    "##### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "404d6f80-7104-4c5e-b7be-4033cfad5d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-Mar-24 08:43:52 - Input and output configured successfully\n"
     ]
    }
   ],
   "source": [
    "# Loading the Baton Rouge dataset\n",
    "dh_br = DataHandler(\n",
    "    input_path=br_input_file,\n",
    "    uncertainty_path=br_uncertainty_file,\n",
    "    index_col=index_col,\n",
    "    sn_threshold=2.0\n",
    ")\n",
    "V_br, U_br = dh_br.get_data()            # Cleaned input dataset (numpy array), Cleaned uncertainty dataset (numpy array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396ec5a1-f6bc-4382-a81d-d694b1d1decf",
   "metadata": {},
   "source": [
    "##### Initialize and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48d01416-1c41-4d88-b8c0-7bd100e0e6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-Mar-24 08:43:53 - Model: -1, Seed: 42, Q(true): 68313.7773, Q(robust): 58644.709, Steps: 1956/20000, Converged: True, Runtime: 1.21 sec\n"
     ]
    }
   ],
   "source": [
    "# Training a single model\n",
    "sa_br = SA(V=V_br, U=U_br, factors=factors, method=method, seed=seed, optimized=optimized, verbose=verbose)\n",
    "sa_br.initialize(init_method=init_method, init_norm=init_norm, fuzziness=5.0)\n",
    "sa_br.train(max_iter=max_iterations, converge_delta=converge_delta, converge_n=converge_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d48ba2-f6ba-47ec-8725-432bdb223097",
   "metadata": {},
   "source": [
    "Here a single model was created that used the optimized Rust code with the ws-nmf algorithm. 940 iterations were taken before the convergence criteria was met with a resulting loss value of $Q=84264.11$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51afbb5a-a04b-4be1-928d-a45edafe58c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-Mar-24 08:43:58 - Model: 1, Q(true): 65069.5744, Q(robust): 55644.9141, Seed: 8925, Converged: True, Steps: 2300/20000\n",
      "21-Mar-24 08:43:58 - Model: 2, Q(true): 65541.8666, Q(robust): 55585.2486, Seed: 77395, Converged: True, Steps: 1291/20000\n",
      "21-Mar-24 08:43:58 - Model: 3, Q(true): 65049.2052, Q(robust): 55589.223, Seed: 65457, Converged: True, Steps: 1348/20000\n",
      "21-Mar-24 08:43:58 - Model: 4, Q(true): 66051.8651, Q(robust): 56544.1801, Seed: 43887, Converged: True, Steps: 1134/20000\n",
      "21-Mar-24 08:43:58 - Model: 5, Q(true): 63928.0347, Q(robust): 54497.1779, Seed: 43301, Converged: True, Steps: 1495/20000\n",
      "21-Mar-24 08:43:58 - Model: 6, Q(true): 63840.4524, Q(robust): 54423.8403, Seed: 85859, Converged: True, Steps: 2343/20000\n",
      "21-Mar-24 08:43:58 - Model: 7, Q(true): 66533.3494, Q(robust): 56896.0526, Seed: 8594, Converged: True, Steps: 1011/20000\n",
      "21-Mar-24 08:43:58 - Model: 8, Q(true): 65055.9772, Q(robust): 55644.0452, Seed: 69736, Converged: True, Steps: 2167/20000\n",
      "21-Mar-24 08:43:58 - Model: 9, Q(true): 63894.2365, Q(robust): 54459.2191, Seed: 20146, Converged: True, Steps: 2286/20000\n",
      "21-Mar-24 08:43:58 - Model: 10, Q(true): 66038.0929, Q(robust): 56519.5267, Seed: 9417, Converged: True, Steps: 1677/20000\n",
      "21-Mar-24 08:43:58 - Results - Best Model: 6, Q(true): 63840.4524, Q(robust): 54423.8403, Converged: True\n",
      "21-Mar-24 08:43:58 - Runtime: 0.08 min(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 4.76 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, '')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Training multiple models\n",
    "models = 10                   # number of models to create\n",
    "parallel = True               # execute training in parallel\n",
    "\n",
    "batch_br = BatchSA(V=V_br, U=U_br, factors=factors, models=models, method=method, seed=seed, \n",
    "                   init_method=init_method, init_norm=init_norm,\n",
    "                   max_iter=max_iterations, converge_delta=converge_delta,\n",
    "                   converge_n=converge_n, parallel=parallel, optimized=optimized,\n",
    "                   verbose=verbose\n",
    "                   )\n",
    "batch_br.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6923ef86-6971-4718-b006-a66a8283e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "# batch_name = f\"test-batch-f{factors}\"\n",
    "# br_full_output_path = os.path.join(\"..\", \"..\", \"..\", \"data\", \"output\")\n",
    "# batch_br.save(batch_name=batch_name, output_directory=br_full_output_path, pickle_batch=False, header=dh_br.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c130c8e7-6049-48bb-a329-49076e819545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for comparing to PMF5 outputs\n",
    "from tests.factor_comparison import FactorComp\n",
    "from python.metrics import calculate_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb7ff442-b916-434e-84c2-cea3e951f634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating correlation between factors from each epoch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 20.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of permutations for 6 factors: 720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating average correlation for all permutations for each epoch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:38<00:00,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 - Model: 4, Best permutations: ['Factor 1', 'Factor 2', 'Factor 4', 'Factor 3', 'Factor 5', 'Factor 6'], Average R2: 0.9824235591369881, \n",
      "Profile R2 Avg: 0.9982128176460442, Contribution R2 Avg: 0.9669323425344882, WH R2 Avg: 0.9821255172304317\n",
      "Profile R2: [0.9999105473090671, 0.9964333821721015, 0.9991743875437269, 0.997882039478663, 0.9965428938349589, 0.9993336555377472], \n",
      "Contribution R2: [0.9857350637800086, 0.9651437188264306, 0.9688929688927138, 0.9710191489603949, 0.9856446696769419, 0.9251584850704394], \n",
      "WH R2: [0.9878041694450094, 0.9781218790703416, 0.9759456998648128, 0.9871898739887661, 0.987355839275463, 0.9763356417381971]\n",
      "\n",
      "PMF5 Q(true): 64509.9296875, SA Model 4 Q(true): 63928.03469764667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare the results to the PMF5 output on the same dataset and the same number of factors\n",
    "br_pmf_profile_file = os.path.join(\"D:\\\\\", \"projects\", \"esat\", \"data\", \"factor_test\", f\"br{factors}f_profiles.txt\")\n",
    "br_pmf_contribution_file = os.path.join(\"D:\\\\\", \"projects\", \"esat\", \"data\", \"factor_test\", f\"br{factors}f_contributions.txt\")\n",
    "br_pmf_residuals_file = os.path.join(\"D:\\\\\", \"projects\", \"esat\", \"data\", \"factor_test\",\n",
    "                                          f\"br{factors}f_residuals.txt\")\n",
    "br_profile_comparison = FactorComp(batch_sa=batch_br, pmf_profile_file=br_pmf_profile_file,\n",
    "                                    pmf_contribution_file=br_pmf_contribution_file, factors=factors,\n",
    "                                    features=dh_br.features, residuals_path=br_pmf_residuals_file)\n",
    "br_pmf_q = calculate_Q(br_profile_comparison.pmf_residuals.values, dh_br.uncertainty_data_processed)\n",
    "br_profile_comparison.compare(PMF_Q=br_pmf_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899a29af-9711-45d8-b174-f6932d2d3766",
   "metadata": {},
   "source": [
    "Here we see that the most comparable NMF-PY model was model 5 from our batch of 10, with an average $R^2=0.848$ with a the breakdown for each factor and each of the matrices, plus the product matrix $WH$. Additionally, we are provided the factor mapping from the NMF-PY model to the PMF5 model, since the order of factors is never guaranteed to match. We are only comparing the outputs of NMF-PY to the single best performing model of PMF5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c41392bf-d512-499f-9239-0302df0e97c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-Mar-24 08:46:55 - Model: 1, Q(true): 65069.5744, Q(robust): 55644.9141, Seed: 8925, Converged: True, Steps: 2300/20000\n",
      "21-Mar-24 08:46:55 - Model: 2, Q(true): 65541.8666, Q(robust): 55585.2486, Seed: 77395, Converged: True, Steps: 1291/20000\n",
      "21-Mar-24 08:46:55 - Model: 3, Q(true): 65049.2052, Q(robust): 55589.223, Seed: 65457, Converged: True, Steps: 1348/20000\n",
      "21-Mar-24 08:46:55 - Model: 4, Q(true): 66051.8651, Q(robust): 56544.1801, Seed: 43887, Converged: True, Steps: 1134/20000\n",
      "21-Mar-24 08:46:55 - Model: 5, Q(true): 63928.0347, Q(robust): 54497.1779, Seed: 43301, Converged: True, Steps: 1495/20000\n",
      "21-Mar-24 08:46:55 - Model: 6, Q(true): 63840.4524, Q(robust): 54423.8403, Seed: 85859, Converged: True, Steps: 2343/20000\n",
      "21-Mar-24 08:46:55 - Model: 7, Q(true): 66533.3494, Q(robust): 56896.0526, Seed: 8594, Converged: True, Steps: 1011/20000\n",
      "21-Mar-24 08:46:55 - Model: 8, Q(true): 65055.9772, Q(robust): 55644.0452, Seed: 69736, Converged: True, Steps: 2167/20000\n",
      "21-Mar-24 08:46:55 - Model: 9, Q(true): 63894.2365, Q(robust): 54459.2191, Seed: 20146, Converged: True, Steps: 2286/20000\n",
      "21-Mar-24 08:46:55 - Model: 10, Q(true): 66038.0929, Q(robust): 56519.5267, Seed: 9417, Converged: True, Steps: 1677/20000\n",
      "21-Mar-24 08:46:55 - Results - Best Model: 6, Q(true): 63840.4524, Q(robust): 54423.8403, Converged: True\n",
      "21-Mar-24 08:46:55 - Runtime: 0.07 min(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 4.46 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, '')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Training multiple models using ls-nmf\n",
    "models = 10                   # number of models to create\n",
    "parallel = True               # execute training in parallel\n",
    "method2 = \"ls-nmf\"\n",
    "\n",
    "batch_br2 = BatchSA(V=V_br, U=U_br, factors=factors, models=models, method=method2, seed=seed, \n",
    "                    init_method=init_method, init_norm=init_norm,\n",
    "                    max_iter=max_iterations, converge_delta=converge_delta,\n",
    "                    converge_n=converge_n, parallel=parallel, optimized=optimized,\n",
    "                    verbose=verbose\n",
    "                   )\n",
    "batch_br2.train()\n",
    "# br2_full_output_path = f\"br2_nmf-output-f{factors}.json\"\n",
    "# batch_br2.save(output_name=br2_full_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c018085f-4b76-47f8-8bac-9c80b492ea71",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "FactorComp.__init__() got an unexpected keyword argument 'batch_nmf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m br2_profile_comparison \u001b[38;5;241m=\u001b[39m \u001b[43mFactorComp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_nmf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_br2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpmf_profile_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbr_pmf_profile_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mpmf_contribution_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbr_pmf_contribution_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfactors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdh_br\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresiduals_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbr_pmf_residuals_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m br2_pmf_q \u001b[38;5;241m=\u001b[39m calculate_Q(br2_profile_comparison\u001b[38;5;241m.\u001b[39mpmf_residuals\u001b[38;5;241m.\u001b[39mvalues, dh_br\u001b[38;5;241m.\u001b[39muncertainty_data_processed)\n\u001b[0;32m      5\u001b[0m br2_profile_comparison\u001b[38;5;241m.\u001b[39mcompare(PMF_Q\u001b[38;5;241m=\u001b[39mbr2_pmf_q)\n",
      "\u001b[1;31mTypeError\u001b[0m: FactorComp.__init__() got an unexpected keyword argument 'batch_nmf'"
     ]
    }
   ],
   "source": [
    "br2_profile_comparison = FactorComp(batch_nmf=batch_br2, pmf_profile_file=br_pmf_profile_file,\n",
    "                                    pmf_contribution_file=br_pmf_contribution_file, factors=factors,\n",
    "                                    features=dh_br.features, residuals_path=br_pmf_residuals_file)\n",
    "br2_pmf_q = calculate_Q(br2_profile_comparison.pmf_residuals.values, dh_br.uncertainty_data_processed)\n",
    "br2_profile_comparison.compare(PMF_Q=br2_pmf_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f44d24-829d-410d-b86f-d7d166dd36e1",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eed8b28-4b5e-4f18-b1b2-062b24aff14b",
   "metadata": {},
   "source": [
    "##### Q/Loss Comparison\n",
    "Here the three different datasets were tested using both algorithms. 200 models were created for each dataset, each factor count, and each algorithm. The most compariable Q and $R^2$ are shown, along with the best performing (lowest Q) model Q is shown. The NaN were models that had not yet been completed. Each model requires a comparison of all permutations of the factor ordering to the PMF5 output, making comparison of factors > 5 time consuming to complete for 200 models. The $R^2$ value is the total average, the comparison of the factor profile (H matrix), the factor contribution (W matrix), and the model concentration output (WH)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d7040-f040-49f4-b2e0-be72c820a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_analysis_file = \"q_analysis.json\"\n",
    "q_analysis = {}\n",
    "with open(q_analysis_file, \"r\") as data_file:\n",
    "    q_analysis = json.load(data_file)\n",
    "q_df = pd.DataFrame(q_analysis.values())\n",
    "q_df[\"dataset\"].replace({\"br\": \"Baton Rogue\", \"b\": \"Baltimore\", \"sl\": \"Saint Louis\"}, inplace=True)\n",
    "q_df.rename(columns={\"Q(ls-nmf-R2)\": \"R2(ls-nmf)\", \"Q(ws-nmf-R2)\": \"R2(ws-nmf)\"}, inplace=True)\n",
    "q_df = q_df.round(decimals=4)\n",
    "q_fig = ff.create_table(q_df)\n",
    "q_fig.update_layout(width=1200, height=400)\n",
    "q_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18119589-27de-4b92-9bef-d47ad3e9dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = q_df[\"dataset\"] + \" \" + q_df[\"factors\"].astype(str) + \"f\"\n",
    "q_df[\"index\"] = labels\n",
    "\n",
    "q_sub = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=(\"Q/Loss Value\", \"Correlation to PMF (R2)\"))\n",
    "\n",
    "# runtime_df = runtime_df.sort_values(by=[\"dataset\", \"factors\"], ascending=False)\n",
    "pmf_trace = go.Bar(x=q_df[\"index\"], y=q_df[\"Q(pmf)\"], name=\"PMF\")\n",
    "ws_trace = go.Bar(x=q_df[\"index\"], y=q_df[\"Q(ws-nmf)\"], name=\"WS-NMF\")\n",
    "ls_trace = go.Bar(x=q_df[\"index\"], y=q_df[\"Q(ls-nmf)\"], name=\"LS-NMF\")\n",
    "q_sub.add_trace(pmf_trace, 1, 1)\n",
    "q_sub.add_trace(ws_trace, 1, 1)\n",
    "q_sub.add_trace(ls_trace, 1, 1)\n",
    "\n",
    "ws_trace2 = go.Scatter(x=q_df[\"index\"], y=q_df[\"R2(ws-nmf)\"], name=\"R2 WS-NMF\", mode='markers')\n",
    "ls_trace2 = go.Scatter(x=q_df[\"index\"], y=q_df[\"R2(ls-nmf)\"], name=\"R2 LS-NMF\", mode='markers')\n",
    "ws_mean2 = go.Scatter(x=q_df[\"index\"], y=[q_df[\"R2(ws-nmf)\"].mean()] * len(labels), name=\"R2 WS-NMF Mean\", mode='lines', line=dict(width=1, color='purple'))\n",
    "ls_mean2 = go.Scatter(x=q_df[\"index\"], y=[q_df[\"R2(ls-nmf)\"].mean()] * len(labels), name=\"R2 LS-NMF Mean\", mode='lines', line=dict(width=1, color='orange'))\n",
    "\n",
    "q_sub.add_trace(ws_trace2, row=2, col=1)\n",
    "q_sub.add_trace(ls_trace2, row=2, col=1)\n",
    "q_sub.add_trace(ws_mean2, row=2, col=1)\n",
    "q_sub.add_trace(ls_mean2, row=2, col=1)\n",
    "\n",
    "q_sub.update_layout(height=600, width=1200, title_text=\"Q/Loss Comparison with Correlation\")\n",
    "q_sub.show()\n",
    "\n",
    "print(f\"Mean R2 - ls-nmf: {round(q_df['R2(ls-nmf)'].mean(),4)}, ws-nmf: {round(q_df['R2(ws-nmf)'].mean(),4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8473ce6-7374-4050-8603-75b7d9a67874",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_dif = copy.copy(q_df)\n",
    "q_dif[\"Q dif(ls-nmf)\"] = q_dif[\"Q(pmf)\"] / q_df[\"Q(ls-nmf)\"]\n",
    "q_dif[\"Q dif(ws-nmf)\"] = q_dif[\"Q(pmf)\"] / q_df[\"Q(ws-nmf)\"]\n",
    "q_dif = q_dif.round(decimals=4)\n",
    "\n",
    "q_dif_fig = ff.create_table(q_dif[[\"dataset\", \"factors\", \"R2(ls-nmf)\", \"R2(ws-nmf)\", \"Q dif(ls-nmf)\", \"Q dif(ws-nmf)\"]])\n",
    "q_dif_fig.update_layout(width=1200, height=400)\n",
    "q_dif_fig.show()\n",
    "print(f\"Min Dif - ls-nmf: {q_dif['Q dif(ls-nmf)'].min()}, ws-nmf: {q_dif['Q dif(ws-nmf)'].min()}\")\n",
    "print(f\"Max Dif - ls-nmf: {q_dif['Q dif(ls-nmf)'].max()}, ws-nmf: {q_dif['Q dif(ws-nmf)'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faad297-6fc6-4f62-8927-dc6f52401d1f",
   "metadata": {},
   "source": [
    "##### Runtime Comparison\n",
    "\n",
    "Here the runtime performance was measured for creating 10 models having comparable loss. The python code was run on the parallelized Rust functions and all values are in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b2e6d1-aaab-4972-9f29-2fd16ab564c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_analysis_file = \"runtime_analysis.json\"\n",
    "runtime_analysis = {}\n",
    "with open(runtime_analysis_file, \"r\") as data_file:\n",
    "    runtime_analysis = json.load(data_file)\n",
    "runtime_df = pd.DataFrame(runtime_analysis.values())\n",
    "runtime_df[\"dataset\"].replace({\"br\": \"Baton Rogue\", \"b\": \"Baltimore\", \"sl\": \"Saint Louis\"}, inplace=True)\n",
    "runtime_df = runtime_df.sort_values(by=[\"dataset\", \"factors\"])\n",
    "runtime_df = runtime_df.round(decimals=2)\n",
    "runtime_fig = ff.create_table(runtime_df)\n",
    "runtime_fig.update_layout(width=1200, height=600)\n",
    "\n",
    "runtime_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b07c3e2-9712-45e2-af40-0a9114748656",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = runtime_df[\"dataset\"] + \" \" + runtime_df[\"factors\"].astype(str) + \"f\"\n",
    "runtime_df[\"index\"] = labels\n",
    "\n",
    "runtime_sub = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=(\"Runtime(sec)\", \"Q/Loss Value\"))\n",
    "\n",
    "rt_pmf_trace = go.Bar(x=runtime_df[\"index\"], y=runtime_df[\"pmf-runtime\"], name=\"PMF\")\n",
    "rt_ws_trace = go.Bar(x=runtime_df[\"index\"], y=runtime_df[\"ws-nmf-runtime\"], name=\"WS-NMF\")\n",
    "rt_ls_trace = go.Bar(x=runtime_df[\"index\"], y=runtime_df[\"ls-nmf-runtime\"], name=\"LS-NMF\")\n",
    "runtime_sub.add_trace(rt_pmf_trace, 1, 1)\n",
    "runtime_sub.add_trace(rt_ws_trace, 1, 1)\n",
    "runtime_sub.add_trace(rt_ls_trace, 1, 1)\n",
    "\n",
    "pmf_trace2 = go.Scatter(x=runtime_df[\"index\"], y=runtime_df[\"pmf-Q\"], name=\"Q PMF\", mode='lines')\n",
    "ws_trace2 = go.Scatter(x=runtime_df[\"index\"], y=runtime_df[\"ws-nmf-Q\"], name=\"Q WS-NMF\", mode='lines')\n",
    "ls_trace2 = go.Scatter(x=runtime_df[\"index\"], y=runtime_df[\"ls-nmf-Q\"], name=\"Q LS-NMF\", mode='lines')\n",
    "runtime_sub.add_trace(pmf_trace2, row=2, col=1)\n",
    "runtime_sub.add_trace(ws_trace2, row=2, col=1)\n",
    "runtime_sub.add_trace(ls_trace2, row=2, col=1)\n",
    "\n",
    "runtime_sub.update_layout(height=600, width=1200, title_text=\"Runtime Comparison with Q Values\")\n",
    "runtime_sub.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542346bb-8963-4b39-a7df-dae290e215ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = runtime_df[\"dataset\"] + \" \" + runtime_df[\"factors\"].astype(str) + \"f\"\n",
    "runtime_df[\"index\"] = labels\n",
    "\n",
    "# runtime_df = runtime_df.sort_values(by=[\"dataset\", \"factors\"], ascending=False)\n",
    "pmf_trace = go.Bar(x=runtime_df[\"index\"], y=runtime_df[\"pmf-runtime\"], name=\"PMF\")\n",
    "ws_trace = go.Bar(x=runtime_df[\"index\"], y=runtime_df[\"ws-nmf-runtime\"], name=\"WS-NMF\")\n",
    "ls_trace = go.Bar(x=runtime_df[\"index\"], y=runtime_df[\"ls-nmf-runtime\"], name=\"LS-NMF\")\n",
    "\n",
    "runtime_fig2 = go.Figure(data=[pmf_trace, ws_trace, ls_trace])\n",
    "runtime_fig2.update_layout(barmode='group')\n",
    "runtime_fig2.layout.title = \"Runtime Comparison - 20 Models (sec)\"\n",
    "runtime_fig2.layout.height = 500\n",
    "runtime_fig2.layout.width = 1200\n",
    "runtime_fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84067a3a-967c-4d32-92fa-c0e1caffc89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_runtime_metric = (runtime_df['ls-nmf-runtime'] / runtime_df['factors']).mean()\n",
    "ws_runtime_metric = (runtime_df['ws-nmf-runtime'] / runtime_df['factors']).mean()\n",
    "pmf_runtime_metric = (runtime_df['pmf-runtime'] / runtime_df['factors']).mean()\n",
    "print(f\"Ratio of runtime to factor count - LS-NMF: {round(ls_runtime_metric,3)}, WS-NMF: {round(ws_runtime_metric,3)}, PMF: {round(pmf_runtime_metric,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cbaa33-2626-4df0-9d00-21bc9ce14c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_runtime_analysis_file = \"type_runtime_analysis.json\"\n",
    "type_runtime_analysis = {}\n",
    "with open(type_runtime_analysis_file, \"r\") as data_file:\n",
    "    type_runtime_analysis = json.load(data_file)\n",
    "type_runtime_df = pd.DataFrame(type_runtime_analysis.values())\n",
    "type_runtime_df[\"dataset\"].replace({\"br\": \"Baton Rogue\", \"b\": \"Baltimore\", \"sl\": \"Saint Louis\"}, inplace=True)\n",
    "type_runtime_df = type_runtime_df.sort_values(by=[\"dataset\", \"factors\"])\n",
    "type_runtime_fig = ff.create_table(type_runtime_df)\n",
    "type_runtime_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c265a662-cf0e-4aed-943b-9812444ca1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_labels = type_runtime_df[\"dataset\"] + \" \" + type_runtime_df[\"factors\"].astype(str) + \"f\"\n",
    "type_runtime_df[\"index\"] = type_labels\n",
    "\n",
    "ws_py_trace = go.Bar(x=type_runtime_df[\"index\"], y=type_runtime_df[\"ws-nmf-py-runtime\"], name=\"WS-NMF(PY)\")\n",
    "ls_pu_trace = go.Bar(x=type_runtime_df[\"index\"], y=type_runtime_df[\"ls-nmf-py-runtime\"], name=\"LS-NMF(PY)\")\n",
    "ws_trace = go.Bar(x=type_runtime_df[\"index\"], y=type_runtime_df[\"ws-nmf-rust-runtime\"], name=\"WS-NMF(RUST)\")\n",
    "ls_trace = go.Bar(x=type_runtime_df[\"index\"], y=type_runtime_df[\"ls-nmf-rust-runtime\"], name=\"LS-NMF(RUST)\")\n",
    "\n",
    "type_runtime_fig2 = go.Figure(data=[ws_py_trace, ws_trace, ls_pu_trace, ls_trace])\n",
    "type_runtime_fig2.update_layout(barmode='group')\n",
    "type_runtime_fig2.layout.title = \"Runtime Type Comparison - 10 Models (sec)\"\n",
    "type_runtime_fig2.layout.height = 500\n",
    "type_runtime_fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc37e3d7-137d-4944-9fc4-3d59b8a61d26",
   "metadata": {},
   "source": [
    "#### Factor Comparison\n",
    "\n",
    "Here we look at the best model produced by PMF5 for the Baton Rouge dataset using 6 factors, that is the model with the lowest Q(robust), and compare that model to the most comparable models from the LS-NMF and WS-NMF algorithms. Each algorithm generated 200 models and each of those model's output was compared to the PMF5 model, checking the $R^2$ of each model factor permutation (since mapping the factor order may be different from NMF-PY to PMF5). $R^2$ is calculated for the factor profile ($H$), the factor contributions ($W$), and the model product output ($WH$). The best model is determined as the model with the highest $R^2$ average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce9859b-58e7-4042-aa96-aedf7c5ddb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PMF5 input files are converted into pandas dataframes \n",
    "factors = 6\n",
    "\n",
    "pmf_profile_file = os.path.join(\"D:\\\\\", \"projects\", \"nmf_py\", \"data\", \"factor_test\", f\"br{factors}f_profiles.txt\")\n",
    "pmf_contribution_file = os.path.join(\"D:\\\\\", \"projects\", \"nmf_py\", \"data\", \"factor_test\", f\"br{factors}f_contributions.txt\")\n",
    "pmf_residuals_file = os.path.join(\"D:\\\\\", \"projects\", \"nmf_py\", \"data\", \"factor_test\", f\"br{factors}f_residuals.txt\")\n",
    "\n",
    "pmf_output = FactorComp(pmf_profile_file=pmf_profile_file, pmf_contribution_file=pmf_contribution_file, residuals_path=pmf_residuals_file, factors=factors, features=dh_br.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab91603-dfe9-4e43-b7df-d2961a059a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LS-NMF and WS-NMF data pre-processing\n",
    "\n",
    "profile_comparison_file = os.path.join(\"profile_compare_analysis_2.json\")\n",
    "profile_json = {}\n",
    "with open(profile_comparison_file, \"r\") as p_file:\n",
    "    profile_json = json.load(p_file)\n",
    "    \n",
    "ls_key = \"br-6-ls-nmf\"\n",
    "ws_key = \"br-6-ws-nmf\"\n",
    "\n",
    "zero_threshold = 1e-6\n",
    "\n",
    "species = pmf_output.pmf_profiles_df[\"species\"]\n",
    "profile_columns = [ f\"Factor {i}\" for i in range(1, 7)]\n",
    "\n",
    "ls_mapping = profile_json[ls_key][\"factor_mapping\"]\n",
    "ws_mapping = profile_json[ws_key][\"factor_mapping\"]\n",
    "\n",
    "ls_profiles = pd.DataFrame(np.array(profile_json[ls_key][\"model_profiles\"]).T, columns=profile_columns)\n",
    "ls_profiles[ls_profiles < zero_threshold] = 0.0\n",
    "ls_profiles[\"species\"] = species\n",
    "ws_profiles = pd.DataFrame(np.array(profile_json[ws_key][\"model_profiles\"]).T, columns=profile_columns)\n",
    "ws_profiles[ws_profiles < zero_threshold] = 0.0\n",
    "ws_profiles[\"species\"] = species\n",
    "\n",
    "datetimestamps = pmf_output.pmf_contribution_df[\"Datetime\"].tolist()\n",
    "ls_contributions = pd.DataFrame(np.array(profile_json[ls_key][\"model_contributions\"]), columns=profile_columns)\n",
    "ls_contributions[\"Datetime\"] = datetimestamps\n",
    "ws_contributions = pd.DataFrame(np.array(profile_json[ws_key][\"model_contributions\"]), columns=profile_columns)\n",
    "ws_contributions[\"Datetime\"] = datetimestamps\n",
    "\n",
    "input_df = dh_br.input_data\n",
    "input_df[\"Date\"] = pd.to_datetime(datetimestamps, format=\"%m/%d/%y %H:%M\")\n",
    "input_df.set_index(\"Date\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334c7fb0-07f0-4224-861f-ec787e59176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.data.test_tools import CompareAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf69f6fd-0e5d-4a62-990e-891ac44cfc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca = CompareAnalyzer(input_df=dh_br.input_data, \n",
    "                     pmf_profile_df=pmf_output.pmf_profiles_df, ls_profile_df=ls_profiles, ws_profile_df=ws_profiles, \n",
    "                     pmf_contributions_df=pmf_output.pmf_contribution_df, ls_contributions_df=ls_contributions, ws_contributions_df=ws_contributions,\n",
    "                     ls_mapping=ls_mapping, ws_mapping=ws_mapping,\n",
    "                     features=species,\n",
    "                     datetimestamps=datetimestamps\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4ae432-07be-4728-86a9-4eb2897b5628",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca.plot_factor_contribution(feature_i=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba1b39f-9635-42b0-b689-36ecef2df73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca.plot_fingerprints(ls_nmf_r2=profile_json[ls_key][\"model_profile_r2\"], ws_nmf_r2=profile_json[ws_key][\"model_profile_r2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e300e5-2ed2-4b49-8de4-6a639f2b731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca.plot_factors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4d4e2a-91eb-4603-b502-c97bddaf8e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca.plot_feature_timeseries(factor_n=3, feature_n=range(0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee674cb-700b-4f5b-b47b-3023d12fe834",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca.feature_histogram(feature_i=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e20a2-1444-4f97-b3d5-9ea76ef71742",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca.timeseries_plot(feature_i=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a4a76-d940-4871-abd3-e424b29b07da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee6e1d7-4b41-4eb4-a062-345f7317405f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
