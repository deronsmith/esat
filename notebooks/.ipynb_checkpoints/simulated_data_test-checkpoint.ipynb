{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "435e85dc-d1f5-46f7-a592-5f8156dd88a6",
   "metadata": {},
   "source": [
    "### Factor Identification Test\n",
    "\n",
    "The following set of functions are to test how well the algorithms (PMF, LS-NMF, WS-NMF) are able to capture the factor/source contributions to the input data set by generating simulated data for each factor and providing the product dataset to the algorithms. The outputs are reviewed and compared to the original 'true' factor matrices.\n",
    "\n",
    "The simulated datasets are generated by specifying the number of factors/sources (k), the number of features (m), the number of samples (n), the min and max % of noise to be applied to the input dataset, and the min and max % uncertainty to be used to create the uncertainty dataset. The pre-processing steps include: \n",
    "1. For each feature, select at random which factors contribute.\n",
    "2. Each factor that contributes determine the ratio of contribution for those factors.\n",
    "3. Based upon the ratio, create a concentration amount for each factor/feature.\n",
    "4. Input dataset is created by summing all factor matrices together and adding random noise between min_noise_p (%) and max_noise_p (%), for each cell in the input dataset.\n",
    "5. Calcualte the uncertainty dataset from a random value between min_uncertainty_p (%) and max_uncertainty_p (%) for each cell in the input dataset.\n",
    "\n",
    "The outputs are compared by checking factor mapping $R^2$ and the resulting time-series $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "705f4ac6-a0da-4dde-81a9-7a677387c227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook imports\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a471f9e-60f4-49ed-96f6-c5a1e9365b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.datahandler import DataHandler\n",
    "from src.model.nmf import NMF\n",
    "from src.model.model import BatchNMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e2ad53e-a595-46c0-9fad-7fd83062214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 6    # Number of factors\n",
    "m = 40   # Number of features\n",
    "n = 100  # Number of samples\n",
    "min_noise = 0.0    # Minimum noise applied to combined input dataset cell\n",
    "max_noise = 1.0     # Maximum noise applied to combined input dataset cell\n",
    "min_unc = 5.0       # Minimum uncertainty of a cell value\n",
    "max_unc = 6.0       # Maximum uncertainty of a cell value\n",
    "seed = 42           # Random seed\n",
    "\n",
    "min_value = 1e-2\n",
    "rng = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51cfd455-52f8-47a6-9cf3-e98af1ac7b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# factor_matrices = [ np.zeros(shape=(m, n)) for _k in range(k) ]     # Set all factor matrices to zero\n",
    "# for feature in range(m):\n",
    "#     feature_factors = rng.choice(k, rng.integers(2, k), replace=False)\n",
    "#     for ff in feature_factors:\n",
    "#         ff_contributions = rng.uniform(1e-3, rng.integers(1, 10) * 1.0, size=(n))\n",
    "#         factor_matrices[ff][feature] = ff_contributions\n",
    "# base_input = np.sum(factor_matrices, axis=0)                                # Add together all the factors to form the base input dataset\n",
    "# noise_matrix = (rng.uniform(min_noise, max_noise, size=(m, n)) / 100) + 1   # Generate uniform sampling of noise to be applied to the base input dataset\n",
    "# data_matrix = np.multiply(base_input, noise_matrix)                         # Add noise to the base input dataset\n",
    "# uncertainty_values = (rng.uniform(min_unc, max_unc, size=(m, n)) / 100)     # Generate uniform sampling of uncertainty used to create the uncertainty matrix\n",
    "# uncertainty_matrix = np.multiply(data_matrix, uncertainty_values)           # Create uncertainty dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af536f8f-c074-44c9-87dc-aaf77e366680",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_H = rng.uniform(1e-3, 1.0, size=(k, m))\n",
    "true_H = np.divide(true_H, np.sum(true_H, axis=0))\n",
    "true_W = rng.uniform(min_value, 10.0, size=(n, k))\n",
    "base_input = np.matmul(true_W, true_H).T\n",
    "noise_matrix = (rng.uniform(min_noise, max_noise, size=(m, n)) / 100) + 1   # Generate uniform sampling of noise to be applied to the base input dataset\n",
    "data_matrix = np.multiply(base_input, noise_matrix)                         # Add noise to the base input dataset\n",
    "uncertainty_values = (rng.uniform(min_unc, max_unc, size=(m, n)) / 100)     # Generate uniform sampling of uncertainty used to create the uncertainty matrix\n",
    "uncertainty_matrix = np.multiply(data_matrix, uncertainty_values)           # Create uncertainty dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa8a2697-83b9-4a77-8118-9b8e3f62e2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = 6\n",
    "models = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57e0d4ea-0619-4701-84a8-a0bb4b8fb7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30-Jun-23 11:38:24 - Results - Best Model: 18, Converged: True, Q: 22036.2405319985\n",
      "30-Jun-23 11:38:24 - Runtime: 0.83 min(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.42 s\n",
      "Wall time: 50.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training multiple models\n",
    "method = \"ws-nmf\"\n",
    "\n",
    "batch_br = BatchNMF(V=data_matrix, U=uncertainty_matrix, max_iter=20000, converge_delta=0.01, converge_n=10, factors=factors, models=models, method=method, seed=seed, verbose=True)\n",
    "batch_br.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2feaecb-640a-41e9-8f05-93433b105839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57285982320169"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh = batch_br.results[batch_br.best_epoch]['wh']\n",
    "res = np.corrcoef(base_input, wh)\n",
    "res_cor = res[0, 1]\n",
    "r2 = res_cor**2\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb80cf5e-e90f-4070-9749-8ae67165fbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30-Jun-23 12:01:09 - Results - Best Model: 6, Converged: False, Q: 17008.173073219332\n",
      "30-Jun-23 12:01:09 - Runtime: 0.29 min(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 766 ms\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training multiple models\n",
    "method = \"ls-nmf\"\n",
    "\n",
    "batch_br2 = BatchNMF(V=data_matrix, U=uncertainty_matrix, max_iter=40000, converge_delta=0.0001, converge_n=10, factors=7, models=models, method=method, seed=seed, verbose=True)\n",
    "batch_br2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad6a865b-e158-43b4-b852-543f6f8bae02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5189430467995664e-05"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh2 = batch_br2.results[batch_br2.best_epoch]['wh']\n",
    "res2 = np.corrcoef(base_input, wh2)\n",
    "res_cor2 = res2[0, 1]\n",
    "r2_2 = res_cor2**2\n",
    "r2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e396458-d8f3-4c27-bd65-22ff6dd269ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.optimization import FactorSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0c6dbec-11d2-4054-bb86-01a21963a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fs = FactorSearch(seed=seed, data=data_matrix, uncertainty=uncertainty_matrix)\n",
    "# fs.search(min_factor=2, max_factor=12, max_iterations=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85631ed9-3a63-4065-be6a-d9f8e849822e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mround\u001b[39m(v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241m.\u001b[39mresults\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[0;32m      2\u001b[0m q_delta \u001b[38;5;241m=\u001b[39m [ q_values[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m q_values[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(q_values))]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fs' is not defined"
     ]
    }
   ],
   "source": [
    "q_values = [round(v['Q'],2) for k, v in fs.results.items()]\n",
    "q_delta = [ q_values[i-1] - q_values[i] for i in range(1, len(q_values))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e1ee8f-8156-4ca9-9537-93952071d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "x = [i for i in range(1, len(q_values)+1)]\n",
    "x2 = [i for i in range(2, len(q_delta)+2)]\n",
    "factor_graph = go.Figure(data=[go.Scatter(x=x, y=q_values, name=\"Q\"), go.Scatter(x=x2, y=q_delta, name=\"Q Delta\")])\n",
    "factor_graph.update_layout(title=\"Factor Count vs Loss (Q)\", xaxis_title=\"Factors\", yaxis_title=\"Loss (Q)\")\n",
    "factor_graph.layout.height = 600\n",
    "factor_graph.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e972970-cd5c-42d4-998e-5e8862f41503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710e9785-702b-4450-abdb-389da7db7e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c8cd85-d99f-4285-921d-45e8e0fe225c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f561da-3adc-4056-ba41-28741293bb29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
