{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5686853f-4655-4e79-af04-fe24aac1756e",
   "metadata": {},
   "source": [
    "## ESAT K Estimation Workflow 3 - Input Perturbation Model Evaluation\n",
    "\n",
    "This notebook implements an input perturbation workflow for model evaluation and testing for k-estimate metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ac55f-9259-4ce1-b87e-3302d2ed00f9",
   "metadata": {},
   "source": [
    "#### Code Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9de6b9e-89a5-4db8-94d4-2226605d9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from esat.data.datahandler import DataHandler\n",
    "from esat.model.sa import SA\n",
    "from esat.model.batch_sa import BatchSA\n",
    "from esat.data.analysis import ModelAnalysis, BatchAnalysis\n",
    "from esat.error.bootstrap import Bootstrap\n",
    "from esat_eval.simulator import Simulator\n",
    "from esat_eval.factor_comparison import FactorCompare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e80dbc-702b-4fca-b800-0660033a251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synethic dataset parameter value ranges\n",
    "syn_factors_min = 3\n",
    "syn_factors_max = 8\n",
    "\n",
    "syn_features_min = 15\n",
    "syn_features_max = 45\n",
    "\n",
    "syn_samples_min = 200\n",
    "syn_samples_max = 1000\n",
    "\n",
    "outliers = True\n",
    "outliers_p_min = 0.05\n",
    "outliers_p_max = 0.1\n",
    "outliers_mag_min = 1.1\n",
    "outliers_mag_max = 2\n",
    "\n",
    "noise_mean_min = 0.05\n",
    "noise_mean_max = 0.15\n",
    "noise_scale = 0.01\n",
    "\n",
    "uncertainty_mean_min = 0.05\n",
    "uncertainty_mean_max = 0.15\n",
    "uncertainty_scale = 0.01\n",
    "\n",
    "contr_curve_min_range = [0.0, 1.0]\n",
    "contr_curve_max_range = [2.0, 5.0]\n",
    "contr_curve_scale_range = [0.1, 0.5]\n",
    "\n",
    "random_seed = 337\n",
    "k_coef = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2408991-45a6-4a5c-ac14-388f79cbab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e045f7a-8a9a-4316-b60e-018935328241",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372a6a6a-612a-4d00-a3ae-be12aecb0e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulator with the above parameters\n",
    "def generate_synthetic_data(true_factor):\n",
    "    n_features = rng.integers(low=syn_features_min, high=syn_features_max, size=1)[0]\n",
    "    n_samples = rng.integers(low=syn_samples_min, high=syn_samples_max, size=1)[0]\n",
    "    i_outlier_p = round(rng.uniform(low=outliers_p_min, high=outliers_p_max, size=1)[0], 2)\n",
    "    i_outlier_mag = round(rng.uniform(low=outliers_mag_min, high=outliers_mag_max, size=1)[0], 2)\n",
    "    contribution_max = round(rng.uniform(low=1.0, high=10.0, size=1)[0], 2)\n",
    "    print(f\"True Factors: {true_factor}, Features: {n_features}, Samples: {n_samples}, Outliers %: {i_outlier_p}, Outliers Magnitude: {i_outlier_mag}, Contribution Max: {contribution_max}\")\n",
    "    simulator = Simulator(seed=rng.integers(low=0, high=10, size=1)[0],\n",
    "                          factors_n=true_factor,\n",
    "                          features_n=n_features,\n",
    "                          samples_n=n_samples,\n",
    "                          outliers=outliers,\n",
    "                          outlier_p=i_outlier_p,\n",
    "                          outlier_mag=i_outlier_mag,\n",
    "                          contribution_max=contribution_max,\n",
    "                          noise_mean_min=noise_mean_min,\n",
    "                          noise_mean_max=noise_mean_max,\n",
    "                          noise_scale=noise_scale,\n",
    "                          uncertainty_mean_min=uncertainty_mean_min,\n",
    "                          uncertainty_mean_max=uncertainty_mean_max,\n",
    "                          uncertainty_scale=uncertainty_scale,\n",
    "                          verbose=False\n",
    "                         )\n",
    "    curved_factors_count = rng.integers(low=0, high=true_factor, size=1)[0]\n",
    "    curved_factor_list = rng.choice(list(range(true_factor)), size=curved_factors_count, replace=False)\n",
    "    for c_i in curved_factor_list:\n",
    "        # parameters not used by the curve type are ignored\n",
    "        i_curve_type = rng.choice(['uniform', 'decreasing', 'increasing', 'logistic', 'periodic'], size=1)[0]\n",
    "        i_curve_min = rng.uniform(low=contr_curve_min_range[0], high=contr_curve_min_range[1], size=1)[0]\n",
    "        i_curve_max = rng.uniform(low=contr_curve_max_range[0], high=contr_curve_max_range[1], size=1)[0]\n",
    "        i_curve_scale = rng.uniform(low=contr_curve_scale_range[0], high=contr_curve_scale_range[1], size=1)[0]\n",
    "        i_curve_frequency = rng.uniform(low=0.1, high=0.9, size=1)[0]\n",
    "        \n",
    "        # To keep all as uniform comment out the line below\n",
    "        # simulator.update_contribution(factor_i=c_i, curve_type=i_curve_type, scale=i_curve_scale, frequency=i_curve_frequency, minimum=i_curve_min, maximum=i_curve_max)\n",
    "    \n",
    "    syn_input_df, syn_uncertainty_df = simulator.get_data()\n",
    "    data_handler = DataHandler.load_dataframe(input_df=syn_input_df, uncertainty_df=syn_uncertainty_df)\n",
    "    data_handler.metrics\n",
    "    V, U = data_handler.get_data()\n",
    "    return V, U\n",
    "\n",
    "\n",
    "def run_perturbed(v, u, ifactors, random_seed, perturb_p = 0.25, perturb_v = 0.05, sa_model=None, models=10, max_iter=10000, converge_n=20, converge_delta=0.1, threshold: float=0.9):\n",
    "    # Runs a perturbed input batch instance\n",
    "    # Steps:\n",
    "    # 1. Create a SA instance using the provided iV, iU and true_k for the data and factor count (if one is not provided).\n",
    "    # 2. Using a general predefined % value (perturb_v) change and % instance change (perturb_p) for the dataset, or by feature:\n",
    "    #    a. Select perturb_p number of indecies from the input data matrix and change those values +/- up to perturb_v\n",
    "    # 3. With the perturbed dataset rerun the model using the base model H matrix.\n",
    "    # 4. Evaluate how much the profiles changes, mapping the profiles and calculating r2. A perturbed result will have % of profiles that didn't map, and the range of values for those profiles that did.\n",
    "    #    a. The metrics will be used to determine how much the solution profiles change with the perturbed datasets.\n",
    "    #    b. For each perturbed solution, a single metric of average r2 for the mapped profiles and the number of profiles that mapped.\n",
    "    # 5. Repeat for n number of models\n",
    "\n",
    "    # step 1\n",
    "    if sa_model is None:\n",
    "        sa_model = SA(V=v, U=u, factors=ifactors, seed=random_seed, verbose=False)\n",
    "        sa_model.initialize()\n",
    "        sa_model.train(max_iter=max_iter, converge_delta=converge_delta, converge_n=converge_n)\n",
    "\n",
    "    factor_mapping_p = []\n",
    "    factor_coef_mean = []\n",
    "    for i in range(models):\n",
    "        i_mask = rng.random(size=v.shape) > perturb_p\n",
    "        v_matrix = rng.uniform(low=0, high=perturb_v, size=v.shape)\n",
    "        pn_matrix = rng.random(size=v.shape)\n",
    "        pn_matrix[pn_matrix > 0.5] = 1.0\n",
    "        pn_matrix[pn_matrix <= 0.5] = -1.0\n",
    "        \n",
    "        i_V = copy.copy(v)\n",
    "        # for the cells where i_mask is True, the value of i_V is equal to i_V +/- v_matrix * i_V\n",
    "        i_V[i_mask] =  i_V[i_mask] + (pn_matrix[i_mask] * v_matrix[i_mask] * i_V[i_mask])\n",
    "        i_V[i_V < 0.0] = 1e-12\n",
    "\n",
    "        i_sa_model = SA(i_V, U=u, factors=ifactors, seed=random_seed, verbose=False)\n",
    "        i_sa_model.initialize(H=sa_model.H)\n",
    "        i_sa_model.train(max_iter=max_iter, converge_delta=converge_delta, converge_n=converge_n)\n",
    "\n",
    "        i_mapping = []\n",
    "        coef_mapping = []\n",
    "        for k in range(ifactors):\n",
    "            base_factor = sa_model.H[k]\n",
    "            k_factor = i_sa_model.H[k]\n",
    "            base_k_correlation = FactorCompare.calculate_correlation(factor1=base_factor, factor2=k_factor)\n",
    "            coef_mapping.append(base_k_correlation)\n",
    "            if base_k_correlation >= threshold:\n",
    "                i_mapping.append(1)\n",
    "            else:\n",
    "                i_mapping.append(0)\n",
    "        factor_mapping_p.append(np.sum(i_mapping)/ifactors)\n",
    "        factor_coef_mean.append(np.round(np.mean(coef_mapping), 4))\n",
    "    results = {\n",
    "        \"k\": ifactors,\n",
    "        \"seed\": random_seed,\n",
    "        \"perturb %\": perturb_p,\n",
    "        \"perturb v\": perturb_v,\n",
    "        \"factor_mapping\": factor_mapping_p,\n",
    "        \"factor_coef_mean\": factor_coef_mean\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_batch_perturb(k, iV, iU, min_k: int = 2, max_k: int = 10, perturb_p=0.25, perturb_v=0.05, models: int = 100, max_iter=10000, converge_n=20, converge_delta=0.1, threshold: float = 0.9):\n",
    "    results = {\n",
    "        \"true_k\": k,\n",
    "        \"models\": models,\n",
    "        \"min_k\": min_k,\n",
    "        \"max_k\": max_k,\n",
    "        \"k_results\": []\n",
    "    }\n",
    "    for ifactors in range(min_k, max_k+1):\n",
    "        i_seed = rng.integers(low=0, high=1e10, endpoint=True, size=1)[0]\n",
    "        i_result = run_perturbed(v=iV, u=iU, ifactors=ifactors, random_seed=i_seed, perturb_p=perturb_p, perturb_v=perturb_v, models=models, max_iter=max_iter, converge_n=converge_n, converge_delta=converge_delta, threshold=threshold)\n",
    "        results[\"k_results\"].append(i_result)\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf75aee-32ec-49d5-afc9-2dc5760ede24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Factors: 7, Features: 15, Samples: 855, Outliers %: 0.09, Outliers Magnitude: 1.32, Contribution Max: 4.98\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "true_k = 7\n",
    "iV, iU = generate_synthetic_data(true_factor=true_k)\n",
    "\n",
    "n_models = 10\n",
    "threshold = 0.9\n",
    "min_factors = 2\n",
    "max_factors = 10\n",
    "\n",
    "perturb_p = 0.10\n",
    "perturb_v = 0.05\n",
    "\n",
    "batch_estimate = run_batch_perturb(k=true_k, iV=iV, iU=iU, perturb_p=perturb_p, perturb_v=perturb_v, min_k=min_factors, max_k=max_factors, models=n_models, max_iter=10000, converge_n=20, converge_delta=0.1, threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7939ea-d6ce-4c9c-8c3d-9aeec7d87ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_labels = [f\"Factor {i}\" for i in range(min_factors, max_factors+1)]\n",
    "mapping_mean = []\n",
    "coef_mean = []\n",
    "for i in range(max_factors-min_factors+1):\n",
    "    i_fm = batch_estimate[\"k_results\"][i][\"factor_mapping\"]\n",
    "    i_cm = batch_estimate[\"k_results\"][i][\"factor_coef_mean\"]\n",
    "    mapping_mean.append(i_fm)\n",
    "    coef_mean.append(i_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1628b9e7-70ec-4ae2-9ee5-98e05e1f72de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882b8c85-86b3-4957-a694-ed0681e55011",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "for i in range(max_factors-min_factors+1):\n",
    "    i_results = coef_mean[i]\n",
    "    i_label = factor_labels[i]\n",
    "    batch_fig.add_trace(go.Box(y=i_results, name=i_label), secondary_y=False)\n",
    "batch_fig.add_trace(go.Scatter(x=factor_labels, y=np.mean(mapping_mean, axis=1)* 100, name=\"Average % Mapped\"), secondary_y=True)\n",
    "batch_fig.update_layout(title=\"Input Perturbation Results\", width=1200, height=800)\n",
    "batch_fig.update_yaxes(secondary_y=False, title_text=\"R2\")\n",
    "batch_fig.update_yaxes(secondary_y=True, title_text=\"%\")\n",
    "batch_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44335811-069c-4a0f-a84c-671fdbab2f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610165b1-4d9a-462d-843a-e450fb00ee3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
