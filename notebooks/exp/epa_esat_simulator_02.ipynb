{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5686853f-4655-4e79-af04-fe24aac1756e",
   "metadata": {},
   "source": [
    "## Environmental Source Apportionment Toolkit (ESAT) Simulator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ec139f-6a92-417b-9b19-b58409d85e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When running from Google Colab or another Jupyter notebook cloud environment, the ESAT python package may need to be installed.\n",
    "# If the python package file is available locally, run a pip install for the specific wheel for your current OS/Arch\n",
    "\n",
    "#installation of wheel for the first time\n",
    "#!pip install \"path to wheel file\"\n",
    "\n",
    "#installation of an updated wheel over an already existing wheel\n",
    "#!pip install --force-reinstall \"path to wheel file\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ac55f-9259-4ce1-b87e-3302d2ed00f9",
   "metadata": {},
   "source": [
    "#### Code Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9de6b9e-89a5-4db8-94d4-2226605d9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from esat.data.datahandler import DataHandler\n",
    "from esat.model.batch_sa import BatchSA\n",
    "from esat.data.analysis import ModelAnalysis, BatchAnalysis\n",
    "from esat_eval.simulator import Simulator\n",
    "from esat.estimator import FactorEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb2b9c-98ee-4482-a0f9-c06b1eeb651f",
   "metadata": {},
   "source": [
    "#### Synthetic Dataset\n",
    "\n",
    "Generate synthetic input (V) and uncertainty (U) datasets for model analysis. V and U are generated in the following sequence:\n",
    "\n",
    "1.\tFeature profiles are defined and/or randomly generated (H); if the latter, for each feature, a random number of factors between 1 and K are chosen as sources for that feature. For each contributing factor, a random contribution (uniform value between 0 and 1) is assigned. If one or more predefined factor profiles (a row of H) are provided by the user, they are assigned to H in order of occurrence and overwrite the corresponding randomly generated row of H.\n",
    "2.\tSample concentrations are defined and/or randomly generated (W); if the latter, each cell of W is set to a random uniform number between 0 and contribution_max\n",
    "3.\tV1 is calculated as the product W x H\n",
    "4.\tA noise matrix (N) is created by selecting values from a normal distribution with a randomly selected mean noise (uniform distribution between noise_mean_min and noise_mean_max) for each feature, and standard deviation = noise_scale. The randomly selected mean noise for a feature has a 50% chance to be multiplied by -1 to allow for the reduction of values in V1. Then the Hadamard product (element-wise matrix multiplication) of V1 and N is used to calculate V: V1 + V1◦N -> V\n",
    "5.  Outliers are added to V if outliers=True. A number of elements in V (a proportion = outlier_p) are randomly selected and each one has a 50% chance to become V*outlier_mag, and a 50% chance to become V/outlier_mag\n",
    "\n",
    "6.\tAn uncertainty matrix (U1) is created by selecting values from a normal distribution with a randomly selected mean uncertainty (uniform distribution between uncertainty_mean_min and uncertainty_mean_max) for each feature, and standard deviation = uncertainty_scale. Then the Hadamard product of V and U1 is used to calculate U: V◦U1 -> U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6ed6eb-46b4-4d3d-8085-19009c083e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synethic dataset parameters\n",
    "seed = 85\n",
    "syn_factors = 4              # Number of factors in the synthetic dataset\n",
    "syn_features = 40             # Number of features in the synthetic dataset\n",
    "syn_samples = 850            # Number of samples in the synthetic dataset\n",
    "outliers = True               # Add outliers to the dataset\n",
    "outlier_p = 0.05               # Proportion of outliers\n",
    "outlier_mag = 1.5             # Magnitude of outliers; a multipier of the data value\n",
    "contribution_max = 20         # Each cell of W (dimension of samples x factors) is a random unform number between 0 and contribution_max\n",
    "noise_mean_min = 0.1          # Min value for the mean noise (as a proportion) applied to a specific feature (column of V)\n",
    "noise_mean_max = 0.2        # Max value for the mean noise (as a proportion) applied to a specific feature (column of V)\n",
    "noise_scale = 0.02            # Standard deviation of the randomly assigned mean noise for a specific feature; should be 1/3 or less of the minimum noise value to avoid negative noise\n",
    "uncertainty_mean_min = 0.1   # Min value for the mean uncertainty (as a proportion) applied to a specific feature (column of V)\n",
    "uncertainty_mean_max = 0.4    # Max value for the mean uncertainty (as a proportion) applied to a specific feature (column of V)\n",
    "uncertainty_scale = 0.01      # Standard deviation of the randomly assigned mean uncertainty for a specific feature; should be 1/3 or less of the minimum uncertainty value to avoid negative uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd06d50-6afb-4cdf-a20c-3a487b8a7a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulator with the above parameters\n",
    "simulator = Simulator(seed=seed,\n",
    "                      factors_n=syn_factors,\n",
    "                      features_n=syn_features,\n",
    "                      samples_n=syn_samples,\n",
    "                      outliers=outliers,\n",
    "                      outlier_p=outlier_p,\n",
    "                      outlier_mag=outlier_mag,\n",
    "                      contribution_max=contribution_max,\n",
    "                      noise_mean_min=noise_mean_min,\n",
    "                      noise_mean_max=noise_mean_max,\n",
    "                      noise_scale=noise_scale,\n",
    "                      uncertainty_mean_min=uncertainty_mean_min,\n",
    "                      uncertainty_mean_max=uncertainty_mean_max,\n",
    "                      uncertainty_scale=uncertainty_scale\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bb55be-07a1-44a3-98a0-d91855b9d81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example command for passing in a custom factor profile matrix instead of a randomly generated profile\n",
    "# my_profile = np.ones(shape=(syn_factors, syn_features))\n",
    "# simulator.generate_profiles(profiles=my_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5983ad-0d02-4938-9943-31bf0f889414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to customize the factor contributions. Curve_type options: 'uniform', 'decreasing', 'increasing', 'logistic', 'periodic' ; uniform by default\n",
    "# simulator.update_contribution(factor_i=0, curve_type=\"logistic\", scale=0.1, frequency=0.5)\n",
    "# simulator.update_contribution(factor_i=1, curve_type=\"periodic\", minimum=0.0, maximum=1.0, frequency=0.5, scale=0.1)\n",
    "# simulator.update_contribution(factor_i=2, curve_type=\"increasing\", minimum=0.0, maximum=1.0, scale=0.1)\n",
    "# simulator.update_contribution(factor_i=3, curve_type=\"decreasing\", minimum=0.0, maximum=1.0, scale=0.1)\n",
    "simulator.plot_synthetic_contributions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d605314-c34a-434d-a178-81b6f6b3766d",
   "metadata": {},
   "source": [
    "#### Load Data\n",
    "Assign the processed data and uncertainty datasets to the variables V and U. These steps will be simplified/streamlined in a future version of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee36d66-7066-44d1-ae0b-b8b3e2413bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_input_df, syn_uncertainty_df = simulator.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0b90b7-c68c-4d64-af4b-47033406e5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = DataHandler.load_dataframe(input_df=syn_input_df, uncertainty_df=syn_uncertainty_df)\n",
    "V, U = data_handler.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412787cb-62cf-4a86-9a2d-f368030e11db",
   "metadata": {},
   "source": [
    "#### Input/Uncertainty Data Metrics and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d28851-eee3-4682-817e-00ce63e5617b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the input data metrics, including signal to noise ratio of the data and variability of feature concentrations\n",
    "data_handler.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac7440-7f97-4b81-a2ac-6e539f294132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concentration/Uncertainty scatterplot for a specific feature; feature indexing starts at 0\n",
    "data_handler.plot_data_uncertainty(feature_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c24b313-37ca-412f-b07b-dcd00021773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concentration plot comparing features; feature indexing starts at 0\n",
    "data_handler.plot_feature_data(x_idx=0, y_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dcbac4-80d1-4627-9608-bb7973e36c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature timeseries; a list [] of features separated by commas; feature indexing starts at 0\n",
    "data_handler.plot_feature_timeseries(feature_selection=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0524672b-b998-4526-b561-cf0cf25b6538",
   "metadata": {},
   "source": [
    "#### Factor Estimator\n",
    "With real datasets, the actual number of factors/sources is typically unknown. The researcher can make a best guess, or run multiple batches using different numbers of factors. To assist with this process, the Factor Estimator algorithm generates an array models (samples parameter; suggested value ~ 1000) using various numbers of factors (between min_factors and max_factors; suggest 2-10) and random W and H initilaizations. Cross-validation is used, where 10% of the input data are randomly excluded from fitting the model solutions (a different 10% for each sample). \"Test MSE\" is calculated using each solution to predict the 10% excluded values, and is then used to calculate other metrics that are useful for estimating the correct factor count. We suggest examining the largest \"Delta Ratio\" and \"K estimate\" values as pointers to what factor counts appear to fit the data best.\n",
    "\n",
    "Delta MSE:\n",
    "\n",
    "     (Test MSE at k-1) - (Test MSE at k)\n",
    "\n",
    "Delta Ratio:\n",
    "\n",
    "     1.01*(Maximum Delta MSE)*(Delta MSE at k)/(Delta MSE at k+1)\n",
    "\n",
    "\n",
    "K Estimate\n",
    "\n",
    "     (Minimum Test MSE)/[(Test MSE at k)*k^1.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c4757f-5d8f-414d-92d0-7158a4d56b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and run factor estimator\n",
    "samples = 100\n",
    "min_factors = 2\n",
    "max_factors = 10\n",
    "factor_est = FactorEstimator(V=V, U=U)\n",
    "results = factor_est.run(samples=samples, min_factors=min_factors, max_factors=max_factors)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64336f47-4e42-4d5b-87f2-150cb567d244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results of the factor search; the red vertical dashed line is the factor count with the maximum Delta Ratio,\n",
    "# the black vertical dashed line is true number of factors (only plotted when sythetic data have been generated).\n",
    "factor_est.plot(actual_count=syn_factors)\n",
    "estimated_factors = factor_est.estimated_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f532c5-1e71-4966-8edf-e8f68fa0bd57",
   "metadata": {},
   "source": [
    "#### Model Training Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd4b336-67a8-4dd1-a758-ed895d933dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_col = \"Date\"                  # the index of the input/uncertainty datasets\n",
    "# factors = syn_factors             # the number of factors set by the initial synthetic parameters above\n",
    "factors = 5                         # the number of factors in the model\n",
    "method = \"ls-nmf\"                   # \"ls-nmf\", \"ws-nmf\"\n",
    "models = 20                         # the number of models to train\n",
    "init_method = \"col_means\"           # default is column means \"col_means\", \"kmeans\", \"cmeans\"\n",
    "init_norm = True                    # if init_method=kmeans or cmeans, normalize the data prior to clustering.\n",
    "seed = 332                           # random seed for initialization\n",
    "max_iterations = 20000              # the maximum number of iterations for fitting a model\n",
    "converge_delta = 0.1                # convergence criteria for the change in loss, Q\n",
    "converge_n = 25                     # convergence criteria for the number of steps where the loss changes by less than converge_delta\n",
    "verbose = True                      # adds more verbosity to the algorithm workflow on execution.\n",
    "optimized = True                    # use the Rust code if possible\n",
    "parallel = True                     # execute the model training in parallel, multiple models at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f561829-6c19-4eeb-b8d3-3f83180b9794",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018eb975-f9dd-4e73-bf1d-8c5b696e6dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Training multiple models, optional parameters are commented out. Model with lowest QTrue shown at the end (using zero indexing for model number)\n",
    "sa_models = BatchSA(V=V, U=U, factors=factors, models=models, method=method, seed=seed, max_iter=max_iterations,\n",
    "                    init_method=init_method, init_norm=init_norm,\n",
    "                    converge_delta=converge_delta, converge_n=converge_n, \n",
    "                    parallel=parallel, optimized=optimized,\n",
    "                    verbose=True\n",
    "                   )\n",
    "_ = sa_models.train()\n",
    "\n",
    "lowest_Q_model = sa_models.best_model\n",
    "lowest_Q_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682471c-d3d6-4f5e-9fc1-5bb33f1d8fdd",
   "metadata": {},
   "source": [
    "#### Batch Analysis\n",
    "\n",
    "These methods allow plotting/reviewing results of models fit via the BatchSA training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db50f8b6-a67f-451b-bdb5-9a075632e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform batch model analysis\n",
    "batch_analysis = BatchAnalysis(batch_sa=sa_models, data_handler=data_handler)\n",
    "# Plot the loss of the models over iterations\n",
    "batch_analysis.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bb4e83-0fe9-464d-afcc-bee53123fafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss distribution for the batch models\n",
    "batch_analysis.plot_loss_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c65c8d-7826-420d-a4f7-67bc0fcae49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the concentration and residuals for each model for a specified feature with starting index of 0\n",
    "batch_analysis.plot_temporal_residuals(feature_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1215ea-feba-4c9f-9377-77097fd16408",
   "metadata": {},
   "source": [
    "### Compare to Synthetic Data\n",
    "\n",
    "Compare the set of batch models to the original synthetic factor data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eafe023-e783-4c17-bf08-7242f375cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator.compare(batch_sa=sa_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f83183-e723-49fe-888c-cfa1e43d7595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the factor/feature mapping of the best model by lowest QTrue\n",
    "simulator.plot_comparison(model_i=lowest_Q_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfd4755-3533-4cc0-9aa3-4a82a2233691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Simulator instance, saves the instance as a pickle file and saves the synthetic profiles, contributions, data and uncertainty as csv files.\n",
    "# sim_name = \"synthetic\"\n",
    "# sim_output_dir = \"D:/git/esat/notebooks/\"\n",
    "# simulator.save(sim_name=sim_name, output_directory=sim_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316925f4-4e66-4a32-a086-310168b28d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a previously saved Simulator instance\n",
    "# simulator_file = \"D:/git/esat/notebooks/esat_simulator.pkl\"\n",
    "# simulator_2 = Simulator.load(file_path=simulator_file)\n",
    "# simulator_2.factor_compare.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34bd6f8-b78e-42de-8277-912bb142c51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selet the model with highest correlations to the sythetic profiles (using zero indexing for the model number)\n",
    "highest_corr_model = simulator.factor_compare.best_model\n",
    "highest_corr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7737fe0f-cf3c-40ed-8b9b-a9aa4a1dfa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mapping results for best model by highest average correlation with sythetic data\n",
    "simulator.plot_comparison(model_i=highest_corr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c57f33-ef70-4fae-b47d-04220b773598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Model Analysis module for the model with lowest QTrue or the model with highest correlation to sythetic data\n",
    "\n",
    "#sa_model = sa_models.results[lowest_Q_model]\n",
    "#model_analysis = ModelAnalysis(datahandler=data_handler, model=sa_model, selected_model=lowest_Q_model)\n",
    "\n",
    "sa_model = sa_models.results[highest_corr_model]\n",
    "model_analysis = ModelAnalysis(datahandler=data_handler, model=sa_model, selected_model=highest_corr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d2bff7-b36a-4837-95a9-1ff2bd90c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Analysis shows the scaled residual histogram for a specified feature (with zero indexing), along with metrics and distribution curves. The abs_threshold parameter specifies the condition for the returned values of the function call as those residuals which exceed the absolute value of that threshold.\n",
    "abs_threshold = 3.0\n",
    "threshold_residuals = model_analysis.plot_residual_histogram(feature_idx=5, abs_threshold=abs_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df0f1b-747b-4519-8d74-b30b2cfd3db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"List of Absolute Scaled Residual Greather than: {abs_threshold}. Count: {threshold_residuals.shape[0]}\")\n",
    "threshold_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462d33c4-51b4-4c1d-b01e-7b9667638e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model output statistics for the estimated V, including SE: Standard Error metrics, and 3 normal distribution tests of the residuals (KS Normal is used in PMF5)\n",
    "model_analysis.calculate_statistics()\n",
    "model_analysis.statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d23a88-47f5-4692-ba9b-07e8630d5486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model feature observed vs predicted plot with regression and one-to-one lines. Feature/Column specified by index (using zero indexing).\n",
    "model_analysis.plot_estimated_observed(feature_idx=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102a0280-d275-46d5-973e-48fa55fd51af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model feature timeseries analysis plot showing the observed vs predicted values of the feature, along with the residuals shown below. Feature/column specified by index (using zero indexing).\n",
    "model_analysis.plot_estimated_timeseries(feature_idx=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da54785-9c7f-4396-b776-f086612ace02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top Plot: factor profile showing the sum of concentrations over all samples for each feature (blue bars) and the percentage of the feature for this factor (red dot);\n",
    "# summing this percent for a specific feature over all model factors would = 100; bottom plot shows the normalized contributions by date (values are resampled at a daily timestep for timeseries consistency)\n",
    "# Factor specified by index, starting at 1\n",
    "model_analysis.plot_factor_profile(factor_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e332d6-75b2-4682-ba27-612dfaf26b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model factor fingerprint specifies the feature percentage of each factor.\n",
    "model_analysis.plot_factor_fingerprints(grouped=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8bb85b-b48e-4031-b861-d6f6af5ad090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor G-Space plot shows the normalized contributions of one factor vs another factor. Factor specified by index (starting at 1).\n",
    "model_analysis.plot_g_space(factor_1=3, factor_2=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e299347b-a88c-432b-a68f-270ba15f35ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor contribution pie chart shows the percentage of factor contributions for the specified feature, and the corresponding normalized contribution of each factor for that feature (bottom plot). Feature specified by index (starting at 0).\n",
    "model_analysis.plot_factor_contributions(feature_idx=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
