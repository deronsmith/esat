{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5686853f-4655-4e79-af04-fe24aac1756e",
   "metadata": {},
   "source": [
    "## ESAT K Estimation Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ac55f-9259-4ce1-b87e-3302d2ed00f9",
   "metadata": {},
   "source": [
    "#### Code Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9de6b9e-89a5-4db8-94d4-2226605d9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from esat.data.datahandler import DataHandler\n",
    "from esat.model.batch_sa import BatchSA\n",
    "from esat.data.analysis import ModelAnalysis, BatchAnalysis\n",
    "from esat_eval.simulator import Simulator\n",
    "from esat.estimator import FactorEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb2b9c-98ee-4482-a0f9-c06b1eeb651f",
   "metadata": {},
   "source": [
    "#### Synthetic Dataset\n",
    "\n",
    "Generate synthetic input (V) and uncertainty (U) datasets for model analysis. V and U are generated in the following sequence:\n",
    "\n",
    "1.\tFeature profiles are defined and/or randomly generated (H); if the latter, for each feature, a random number of factors between 1 and K are chosen as sources for that feature. For each contributing factor, a random contribution (uniform value between 0 and 1) is assigned. If one or more predefined factor profiles (a row of H) are provided by the user, they are assigned to H in order of occurrence and overwrite the corresponding randomly generated row of H.\n",
    "2.\tSample concentrations are defined and/or randomly generated (W); if the latter, each cell of W is set to a random uniform number between 0 and contribution_max\n",
    "3.\tV1 is calculated as the product W x H\n",
    "4.\tA noise matrix (N) is created by selecting values from a normal distribution with a randomly selected mean noise (uniform distribution between noise_mean_min and noise_mean_max) for each feature, and standard deviation = noise_scale. The randomly selected mean noise for a feature has a 50% chance to be multiplied by -1 to allow for the reduction of values in V1. Then the Hadamard product (element-wise matrix multiplication) of V1 and N is used to calculate V: V1 + V1◦N -> V\n",
    "5.  Outliers are added to V if outliers=True. A number of elements in V (a proportion = outlier_p) are randomly selected and each one has a 50% chance to become V*outlier_mag, and a 50% chance to become V/outlier_mag\n",
    "\n",
    "6.\tAn uncertainty matrix (U1) is created by selecting values from a normal distribution with a randomly selected mean uncertainty (uniform distribution between uncertainty_mean_min and uncertainty_mean_max) for each feature, and standard deviation = uncertainty_scale. Then the Hadamard product of V and U1 is used to calculate U: V◦U1 -> U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e80dbc-702b-4fca-b800-0660033a251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synethic dataset parameter value ranges\n",
    "syn_factors_min = 3\n",
    "syn_factors_max = 8\n",
    "\n",
    "syn_features_min = 5\n",
    "syn_features_max = 100\n",
    "\n",
    "syn_samples_min = 20\n",
    "syn_samples_max = 2000\n",
    "\n",
    "outliers = True\n",
    "outliers_p_min = 0.05\n",
    "outliers_p_max = 0.1\n",
    "outliers_mag_min = 1.1\n",
    "outliers_mag_max = 2\n",
    "\n",
    "noise_mean_min = 0.1\n",
    "noise_mean_max = 0.2\n",
    "noise_scale = 0.05\n",
    "\n",
    "uncertainty_mean_min = 0.1\n",
    "uncertainty_mean_max = 0.2\n",
    "uncertainty_scale = 0.05\n",
    "\n",
    "contr_curve_min_range = [0.0, 1.0]\n",
    "contr_curve_max_range = [2.0, 5.0]\n",
    "contr_curve_scale_range = [0.1, 0.5]\n",
    "\n",
    "random_seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2408991-45a6-4a5c-ac14-388f79cbab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "339782f0-72b1-4219-9b1b-1a5ede956101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulator with the above parameters\n",
    "def generate_synthetic_data(true_factor):\n",
    "    n_features = rng.integers(low=syn_features_min, high=syn_features_max, size=1)[0]\n",
    "    n_samples = rng.integers(low=syn_samples_min, high=syn_samples_max, size=1)[0]\n",
    "    i_outlier_p = round(rng.uniform(low=outliers_p_min, high=outliers_p_max, size=1)[0], 2)\n",
    "    i_outlier_mag = round(rng.uniform(low=outliers_mag_min, high=outliers_mag_max, size=1)[0], 2)\n",
    "    contribution_max = round(rng.uniform(low=1.0, high=10.0, size=1)[0], 2)\n",
    "    print(f\"True Factors: {true_factor}, Features: {n_features}, Samples: {n_samples}, Outliers %: {i_outlier_p}, Outliers Magnitude: {i_outlier_mag}, Contribution Max: {contribution_max}\")\n",
    "    simulator = Simulator(seed=rng.integers(low=0, high=1e10, size=1)[0],\n",
    "                          factors_n=true_factor,\n",
    "                          features_n=n_features,\n",
    "                          samples_n=n_samples,\n",
    "                          outliers=outliers,\n",
    "                          outlier_p=i_outlier_p,\n",
    "                          outlier_mag=i_outlier_mag,\n",
    "                          contribution_max=contribution_max,\n",
    "                          noise_mean_min=noise_mean_min,\n",
    "                          noise_mean_max=noise_mean_max,\n",
    "                          noise_scale=noise_scale,\n",
    "                          uncertainty_mean_min=uncertainty_mean_min,\n",
    "                          uncertainty_mean_max=uncertainty_mean_max,\n",
    "                          uncertainty_scale=uncertainty_scale,\n",
    "                          verbose=False\n",
    "                         )\n",
    "    curved_factors_count = rng.integers(low=0, high=true_factor, size=1)[0]\n",
    "    # print(f\"Factors Curve Update Count: {curved_factors_count}\")\n",
    "    curved_factor_list = rng.choice(list(range(true_factor)), size=curved_factors_count, replace=False)\n",
    "    # print(f\"Updating factors: {curved_factor_list} curve type\")\n",
    "    for c_i in curved_factor_list:\n",
    "        # parameters not used by the curve type are ignored\n",
    "        i_curve_type = rng.choice(['uniform', 'decreasing', 'increasing', 'logistic', 'periodic'], size=1)[0]\n",
    "        i_curve_min = rng.uniform(low=contr_curve_min_range[0], high=contr_curve_min_range[1], size=1)[0]\n",
    "        i_curve_max = rng.uniform(low=contr_curve_max_range[0], high=contr_curve_max_range[1], size=1)[0]\n",
    "        i_curve_scale = rng.uniform(low=contr_curve_scale_range[0], high=contr_curve_scale_range[1], size=1)[0]\n",
    "        i_curve_frequency = rng.uniform(low=0.1, high=0.9, size=1)[0]\n",
    "\n",
    "        # To keep all as uniform comment out the line below\n",
    "        simulator.update_contribution(factor_i=c_i, curve_type=i_curve_type, scale=i_curve_scale, frequency=i_curve_frequency, minimum=i_curve_min, maximum=i_curve_max)\n",
    "    \n",
    "    syn_input_df, syn_uncertainty_df = simulator.get_data()\n",
    "    data_handler = DataHandler.load_dataframe(input_df=syn_input_df, uncertainty_df=syn_uncertainty_df)\n",
    "    data_handler.metrics\n",
    "    V, U = data_handler.get_data()\n",
    "    return V, U\n",
    "\n",
    "def run_estimation(k, eV, eU, e_samples: int = 100, min_factors: int = 2, max_factors: int = 12, max_iterations: int = 5000, converge_delta: float = 1.0, converge_n: int = 20):\n",
    "    run_samples_n = (max_factors - min_factors) * e_samples\n",
    "    factor_est = FactorEstimator(V=eV, U=eU)\n",
    "    results = factor_est.run(samples=run_samples_n, min_factors=min_factors, max_factors=max_factors, max_iterations=max_iterations, converge_delta=converge_delta, converge_n=converge_n)\n",
    "    \n",
    "    results[\"Overall Score\"] = (results[\"Factors\"] * results[\"Delta Ratio\"] * results[\"Q(True)\"].min()) / results[\"Q(True)\"]\n",
    "    # Add rank for each metric\n",
    "    drordered_list = results.sort_values(\"Delta Ratio\", ascending=False).reset_index()\n",
    "    drrank = drordered_list.loc[drordered_list[\"Factors\"]==k].index[0]\n",
    "\n",
    "    kordered_list = results.sort_values(\"K Estimate\", ascending=False).reset_index()\n",
    "    krank = kordered_list.loc[kordered_list[\"Factors\"]==k].index[0]\n",
    "\n",
    "    os_ordered_list = results.sort_values(\"Overall Score\", ascending=False).reset_index()\n",
    "    os_rank = os_ordered_list.loc[os_ordered_list[\"Factors\"]==k].index[0]\n",
    "    \n",
    "    estimation = {\n",
    "        \"true K\": k,\n",
    "        \"delta ratio\": np.nanargmax(results[\"Delta Ratio\"].values) + min_factors,\n",
    "        \"K estimate\": np.nanargmax(results[\"K Estimate\"].values) + min_factors,\n",
    "        \"Overall Score\": np.nanargmax(results[\"Overall Score\"].values) + min_factors,\n",
    "        \"DR Rank\": drrank,\n",
    "        \"K Rank\": krank,\n",
    "        \"OS Rank\": os_rank,\n",
    "        \"features\": eV.shape[1],\n",
    "        \"samples\": eV.shape[0]\n",
    "    }\n",
    "    return estimation, results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d46e5231-1974-48f8-9e96-7b2ef06ad162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random test\n",
    "# true_factor = rng.integers(low=syn_factors_min, high=syn_factors_max, size=1)[0]\n",
    "# i_V, i_U = generate_synthetic_data(true_factor=true_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3ed7a31-fe8c-4a33-bf87-6bcdd0e43487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# estimation, results = run_estimation(k=true_factor, eV=i_V, eU=i_U, e_samples=10, min_factors=min_factors, max_factors=max_factors)\n",
    "# print(estimation)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee36d66-7066-44d1-ae0b-b8b3e2413bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Factors: 5, Features: 53, Samples: 1515, Outliers %: 0.06, Outliers Magnitude: 1.95, Contribution Max: 3.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rapid random sampling for factor estimation: 100%|███████████████████████████████| 100/100 [03:13<00:00,  1.94s/it]\n",
      "04-Sep-24 14:37:22 - Estimated factor count: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Factors: 7, Features: 73, Samples: 338, Outliers %: 0.1, Outliers Magnitude: 1.56, Contribution Max: 2.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rapid random sampling for factor estimation:  97%|███████████████████████████████ | 97/100 [00:31<00:00,  9.39it/s]"
     ]
    }
   ],
   "source": [
    "# Sampling parameters\n",
    "test_n = 2\n",
    "\n",
    "# Each iteration\n",
    "samples = 10\n",
    "min_factors = 2\n",
    "max_factors = 12\n",
    "\n",
    "\n",
    "est_list = []\n",
    "results_list = []\n",
    "for i in range(test_n):\n",
    "    i_factor = rng.integers(low=syn_factors_min, high=syn_factors_max, size=1)[0]\n",
    "    i_V, i_U = generate_synthetic_data(true_factor=i_factor)\n",
    "    i_est, i_result = run_estimation(k=i_factor, eV=i_V, eU=i_U, e_samples=samples, min_factors=min_factors, max_factors=max_factors, max_iterations=1000, converge_delta=1.0, converge_n=20)\n",
    "    est_list.append(i_est)\n",
    "    results_list.append(i_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0b90b7-c68c-4d64-af4b-47033406e5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(occurences, results):\n",
    "    accuracy_list = [0]*len(occurences)\n",
    "    for i in range(len(occurences)):\n",
    "        if occurences[i] == 0:\n",
    "            accuracy_list[i] = 0\n",
    "        else:\n",
    "            accuracy_list[i] = round(results[i]/occurences[i],2)\n",
    "    return np.multiply(100, accuracy_list)\n",
    "\n",
    "\n",
    "dr_results = [0]*(max_factors-min_factors)\n",
    "kest_results = [0]*(max_factors-min_factors)\n",
    "score_results = [0]*(max_factors-min_factors)\n",
    "k_runs = [0]*(max_factors-min_factors)\n",
    "for _result in est_list:\n",
    "    idx_k = _result['true K'] - min_factors\n",
    "    true_k = _result['true K']\n",
    "    k_runs[idx_k] += 1\n",
    "    if _result['delta ratio'] == true_k:\n",
    "        dr_results[idx_k] += 1\n",
    "    if _result['K estimate'] == true_k:\n",
    "        kest_results[idx_k] += 1\n",
    "    if _result['Overall Score'] == true_k:\n",
    "        score_results[idx_k] += 1\n",
    "\n",
    "labels = [f\"{i + min_factors} Factor(s)\" for i in range(max_factors-min_factors+1)]\n",
    "\n",
    "result_fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "result_fig.add_trace(go.Bar(name=\"Delta Ratio\", x=labels, y=accuracy(k_runs, dr_results)), secondary_y=False)\n",
    "result_fig.add_trace(go.Bar(name=\"K Estimate\", x=labels, y=accuracy(k_runs, kest_results)), secondary_y=False)\n",
    "result_fig.add_trace(go.Bar(name=\"Overal Score\", x=labels, y=accuracy(k_runs, score_results)), secondary_y=False)\n",
    "result_fig.add_trace(go.Scatter(name=\"K Runs\", x=labels, y=k_runs, mode='markers'), secondary_y=True)\n",
    "\n",
    "result_fig.update_layout(title=\"K Estimation Randomized Results\", barmode='group', height=600, width=1200)\n",
    "result_fig.update_yaxes(title_text=\"Accuracy (%)\", range=[0, 100.0], secondary_y=False)\n",
    "result_fig.update_yaxes(title_text=\"Run Count\", secondary_y=True)\n",
    "result_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b055eab-32ea-411e-be35-8ca7ed64b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Runs: {np.sum(k_runs)}\")\n",
    "print(f\"Delta Ratio Accuracy: {100*np.sum(dr_results)/np.sum(k_runs)}%\")\n",
    "print(f\"K Estimate Accuracy: {100*np.sum(kest_results)/np.sum(k_runs)}%\")\n",
    "print(f\"Overall Score Accuracy: {100*np.sum(score_results)/np.sum(k_runs)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df63dbc0-7967-4577-9d5f-e06c13c6b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "est_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f211c3a5-cf67-466c-8b41-8ea2029b8d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd9bd1c-1937-498c-ab5a-42cbfbe02f49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
