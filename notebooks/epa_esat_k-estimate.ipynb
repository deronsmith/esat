{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5686853f-4655-4e79-af04-fe24aac1756e",
   "metadata": {},
   "source": [
    "## ESAT K Estimation Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ac55f-9259-4ce1-b87e-3302d2ed00f9",
   "metadata": {},
   "source": [
    "#### Code Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9de6b9e-89a5-4db8-94d4-2226605d9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from esat.data.datahandler import DataHandler\n",
    "from esat.model.batch_sa import BatchSA\n",
    "from esat.data.analysis import ModelAnalysis, BatchAnalysis\n",
    "from esat_eval.simulator import Simulator\n",
    "from esat.estimator import FactorEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb2b9c-98ee-4482-a0f9-c06b1eeb651f",
   "metadata": {},
   "source": [
    "#### Synthetic Dataset\n",
    "\n",
    "Generate synthetic input (V) and uncertainty (U) datasets for model analysis. V and U are generated in the following sequence:\n",
    "\n",
    "1.\tFeature profiles are defined and/or randomly generated (H); if the latter, for each feature, a random number of factors between 1 and K are chosen as sources for that feature. For each contributing factor, a random contribution (uniform value between 0 and 1) is assigned. If one or more predefined factor profiles (a row of H) are provided by the user, they are assigned to H in order of occurrence and overwrite the corresponding randomly generated row of H.\n",
    "2.\tSample concentrations are defined and/or randomly generated (W); if the latter, each cell of W is set to a random uniform number between 0 and contribution_max\n",
    "3.\tV1 is calculated as the product W x H\n",
    "4.\tA noise matrix (N) is created by selecting values from a normal distribution with a randomly selected mean noise (uniform distribution between noise_mean_min and noise_mean_max) for each feature, and standard deviation = noise_scale. The randomly selected mean noise for a feature has a 50% chance to be multiplied by -1 to allow for the reduction of values in V1. Then the Hadamard product (element-wise matrix multiplication) of V1 and N is used to calculate V: V1 + V1◦N -> V\n",
    "5.  Outliers are added to V if outliers=True. A number of elements in V (a proportion = outlier_p) are randomly selected and each one has a 50% chance to become V*outlier_mag, and a 50% chance to become V/outlier_mag\n",
    "\n",
    "6.\tAn uncertainty matrix (U1) is created by selecting values from a normal distribution with a randomly selected mean uncertainty (uniform distribution between uncertainty_mean_min and uncertainty_mean_max) for each feature, and standard deviation = uncertainty_scale. Then the Hadamard product of V and U1 is used to calculate U: V◦U1 -> U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e80dbc-702b-4fca-b800-0660033a251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synethic dataset parameter value ranges\n",
    "syn_factors_min = 3\n",
    "syn_factors_max = 8\n",
    "\n",
    "syn_features_min = 5\n",
    "syn_features_max = 100\n",
    "\n",
    "syn_samples_min = 20\n",
    "syn_samples_max = 2000\n",
    "\n",
    "outliers = True\n",
    "outliers_p_min = 0.05\n",
    "outliers_p_max = 0.1\n",
    "outliers_mag_min = 1.1\n",
    "outliers_mag_max = 2\n",
    "\n",
    "noise_mean_min = 0.1\n",
    "noise_mean_max = 0.2\n",
    "noise_scale = 0.05\n",
    "\n",
    "uncertainty_mean_min = 0.1\n",
    "uncertainty_mean_max = 0.2\n",
    "uncertainty_scale = 0.05\n",
    "\n",
    "contr_curve_min_range = [0.0, 1.0]\n",
    "contr_curve_max_range = [2.0, 5.0]\n",
    "contr_curve_scale_range = [0.1, 0.5]\n",
    "\n",
    "# Sampling parameters\n",
    "random_seed = 42\n",
    "test_n = 1000\n",
    "\n",
    "# Each iteration\n",
    "samples = 250\n",
    "min_factors = 2\n",
    "max_factors = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2408991-45a6-4a5c-ac14-388f79cbab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "339782f0-72b1-4219-9b1b-1a5ede956101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulator with the above parameters\n",
    "def generate_synthetic_data(true_factor):\n",
    "    n_features = rng.integers(low=syn_features_min, high=syn_features_max, size=1)[0]\n",
    "    n_samples = rng.integers(low=syn_samples_min, high=syn_samples_max, size=1)[0]\n",
    "    i_outlier_p = round(rng.uniform(low=outliers_p_min, high=outliers_p_max, size=1)[0], 2)\n",
    "    i_outlier_mag = round(rng.uniform(low=outliers_mag_min, high=outliers_mag_min, size=1)[0], 2)\n",
    "    contribution_max = round(rng.uniform(low=1.0, high=10.0, size=1)[0], 2)\n",
    "    print(f\"True Factors: {true_factor}, Features: {n_features}, Samples: {n_samples}, Outliers %: {i_outlier_p}, Outliers Magnitude: {i_outlier_mag}, Contribution Max: {contribution_max}\")\n",
    "    simulator = Simulator(seed=rng.integers(low=0, high=1e10, size=1)[0],\n",
    "                          factors_n=true_factor,\n",
    "                          features_n=n_features,\n",
    "                          samples_n=n_samples,\n",
    "                          outliers=outliers,\n",
    "                          outlier_p=i_outlier_p,\n",
    "                          outlier_mag=i_outlier_mag,\n",
    "                          contribution_max=contribution_max,\n",
    "                          noise_mean_min=noise_mean_min,\n",
    "                          noise_mean_max=noise_mean_max,\n",
    "                          noise_scale=noise_scale,\n",
    "                          uncertainty_mean_min=uncertainty_mean_min,\n",
    "                          uncertainty_mean_max=uncertainty_mean_max,\n",
    "                          uncertainty_scale=uncertainty_scale\n",
    "                         )\n",
    "    curved_factors_count = rng.integers(low=0, high=true_factor, size=1)[0]\n",
    "    print(f\"Factors Curve Update Count: {curved_factors_count}\")\n",
    "    curved_factor_list = rng.choice(list(range(true_factor)), size=curved_factors_count, replace=False)\n",
    "    print(f\"Updating factors: {curved_factor_list} curve type\")\n",
    "    for c_i in curved_factor_list:\n",
    "        # parameters not used by the curve type are ignored\n",
    "        i_curve_type = rng.choice(['uniform', 'decreasing', 'increasing', 'logistic', 'periodic'], size=1)[0]\n",
    "        print(f\"New curve type: {i_curve_type}\")\n",
    "        i_curve_min = rng.uniform(low=contr_curve_min_range[0], high=contr_curve_min_range[1], size=1)[0]\n",
    "        i_curve_max = rng.uniform(low=contr_curve_max_range[0], high=contr_curve_max_range[1], size=1)[0]\n",
    "        i_curve_scale = rng.uniform(low=contr_curve_scale_range[0], high=contr_curve_scale_range[1], size=1)[0]\n",
    "        i_curve_frequency = rng.uniform(low=0.1, high=0.9, size=1)[0]\n",
    "        simulator.update_contribution(factor_i=c_i, curve_type=i_curve_type, scale=i_curve_scale, frequency=i_curve_frequency, minimum=i_curve_min, maximum=i_curve_max)\n",
    "    \n",
    "    syn_input_df, syn_uncertainty_df = simulator.get_data()\n",
    "    data_handler = DataHandler.load_dataframe(input_df=syn_input_df, uncertainty_df=syn_uncertainty_df)\n",
    "    data_handler.metrics\n",
    "    V, U = data_handler.get_data()\n",
    "    return V, U\n",
    "\n",
    "def run_estimation(k, eV, eU, e_samples: int = 100, min_factors: int = 2, max_factors: int = 12):\n",
    "    run_samples_n = (max_factors - min_factors) * 10\n",
    "    factor_est = FactorEstimator(V=eV, U=eU)\n",
    "    results = factor_est.run(samples=run_samples_n, min_factors=min_factors, max_factors=max_factors)\n",
    "    results[\"Overall Score\"] = (results[\"Factors\"] * results[\"Delta Ratio\"] * results[\"Q(True)\"].min()) / results[\"Q(True)\"]\n",
    "\n",
    "    estimation = {\n",
    "        \"true K\": k,\n",
    "        \"delta ratio\": np.nanargmax(results[\"Delta Ratio\"].values) + min_factors,\n",
    "        \"K estimate\": np.nanargmax(results[\"K Estimate\"].values) + min_factors,\n",
    "        \"Overall Score\": np.nanargmax(results[\"Overall Score\"].values) + min_factors,\n",
    "        \"features\": eV.shape[1],\n",
    "        \"samples\": eV.shape[0]\n",
    "    }\n",
    "    return estimation, results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d46e5231-1974-48f8-9e96-7b2ef06ad162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03-Sep-24 09:06:59 - Synthetic profiles generated\n",
      "03-Sep-24 09:06:59 - Synthetic factor 3 contribution updated as a random sampling from a normal distribution along a uniform curve.\n",
      "03-Sep-24 09:06:59 - Synthetic data generated\n",
      "03-Sep-24 09:06:59 - Synthetic uncertainty data generated\n",
      "03-Sep-24 09:06:59 - Synthetic dataframes completed\n",
      "03-Sep-24 09:06:59 - Synthetic source apportionment instance created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Factors: 3\n",
      "True Factors: 3, Features: 93, Samples: 1567, Outliers %: 0.09, Outliers Magnitude: 1.1, Contribution Max: 3.05\n",
      "Factors Curve Update Count: 1\n",
      "Updating factors: [2] curve type\n",
      "New curve type: uniform\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1567, 93)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random test\n",
    "true_factor = rng.integers(low=syn_factors_min, high=syn_factors_max, size=1)[0]\n",
    "print(f\"True Factors: {true_factor}\")\n",
    "i_V, i_U = generate_synthetic_data(true_factor=true_factor)\n",
    "i_V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3ed7a31-fe8c-4a33-bf87-6bcdd0e43487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# factor_est = FactorEstimator(V=i_V, U=i_U)\n",
    "# results = factor_est.run(samples=50, min_factors=2, max_factors=10)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee36d66-7066-44d1-ae0b-b8b3e2413bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rapid random sampling for factor estimation:  45%|████████████████▏                   | 45/100 [19:01<08:41,  9.48s/it]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimation, results = run_estimation(k=true_factor, eV=i_V, eU=i_U, e_samples=samples, min_factors=min_factors, max_factors=max_factors)\n",
    "estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0b90b7-c68c-4d64-af4b-47033406e5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b055eab-32ea-411e-be35-8ca7ed64b733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
