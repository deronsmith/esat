{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4abb0f2-d0cc-4ef8-b775-315cb746bad6",
   "metadata": {},
   "source": [
    "## Multilinear Engine Algorithm Review\n",
    "\n",
    "#### Summary\n",
    "The details of this algorithm have been obtained from the publication *The Multilinear Engine: A Table-Driven, Least Squares Program for Solving Multilinear Problems, including the n-Way Parallel Factor Analysis Model*, url: https://www.jstor.org/stable/1390831.\n",
    "\n",
    "#### The Pseudocode Algorithm\n",
    "The ME1 algorithm as described in section 7.1 of the publication.\n",
    "\n",
    "1. Input control information, structure table, and data. Initialize.\n",
    "    1. Reset the coefficients for inverse preconditioning of constrained variables:   $c_n = 1.0$.\n",
    "2. Initialize a sequence of CG steps.\n",
    "    1. Compute the weights   $w_m$.\n",
    "    2. Compute the Jacobian. Convert the Jacobian table into the Jacobian matrix for computing the preconditioning coefficients   $p_n = \\frac{1}{\\sum_{m}w_{m}j_{mn}^2}$.\n",
    "    3. Set   $\\rho = 0$\n",
    "3. Perform one CG step.The initial solution consists of   $f_n$.\n",
    "    1. Compute SE factors   $f_n$ (E9.4).\n",
    "    2. Compute the current fit $y_m$ (E2.1) and the initial object function: $ Q^{(1)} = \\sum_{m=1}^{M}w_m(x_m - y_m)^2 $\n",
    "    3. Compute the components of the Jacobian: $J^{(1)}$, $J^{(2)}$, and $J^{(3)}$ (E4.3, E9.5).\n",
    "    4. Compute the gradient $g = J^TW(x - y)$ (E9.7).\n",
    "    5. Compute the transformed gradient **z** by multiplying gradient components with preconditioning coefficients: $z_n = c_{n}p_{n}g_{n}$.\n",
    "    6. If $\\rho = 0$ (if this is the first step of the sequence), set $\\beta = 0$, $\\rho = g^Tz$, else set $\\beta = \\frac{g^Tz}{\\rho}$, $\\rho = g^{T}z$.\n",
    "    7. Update the accumulated step direction:   $t = \\beta{t} + z$\n",
    "    8. Compute   $\\tau = t^{T}J^{T}WJt$,   $\\omega = t^{T}J^{T}W(x - y)$. (E9.7)\n",
    "    9. Compute the initial approximation for the step length, $\\alpha = \\frac{\\omega}{\\tau}$\n",
    "    10. Compute $Q^{(2)} = Q(max(f + \\alpha{t}, 1))$ (E9.4, E2.1).\n",
    "    11. If $Q^{(2)} < Q^{(1)}$, compute nonlinearity-corrected step length as $\\alpha = \\frac{\\alpha}{(2 - (Q^{(1)} - Q^{(2)})\\frac{\\tau}{\\omega^2})}$\n",
    "    12. else repeatedly try decreasing $\\alpha$, computing $Q^{(2)} = Q(max(f = \\alpha{t}, 1))$, until $Q^{(2)} < Q^{(1)}$, go to Step 2,\n",
    "    13. If it was impossible to satisfy $Q^{(2)} < Q^{(1)}$, go to Step 2.\n",
    "    14. else accept the new solution, set $f = max(f + \\alpha{t}, 1)$.\n",
    "4. Update inverse preconditioning. Denote indexes $n$ of *nonviolating* ($f_n > I_n$) constrained factor elements by $u$, and indexes of violating ($f_n = I_n$) constrained factor elements of $v$.\n",
    "    1. Increase $c_u$, constrained by $c_u \\le 1$.\n",
    "    2. Decrease $c_v$, constrained by $c_v \\ge \\epsilon$.\n",
    "    3. Decrease $t_v$, constrained by $t_v \\ge \\epsilon$.\n",
    "5. Write monitoring output, test for convergence.\n",
    "    1. Output to screen and to log file $Q^{(2)}$ and $\\rho$, that is, the goodness-of-fit and the squared length of the preconditioned gradient.\n",
    "    2. If $|Q^{(1)} - Q^{(2)}| < e$ ($e$ and $k$ are user-specified parameters), and if this condition has been valid in $k$ consecutive steps, assume convergence and go to the end, Step 7.\n",
    "    3. If the maximum iteration count is exceeded, go to the end, Step 7.\n",
    "6. Test for continuing this CG sequence.\n",
    "    1. If convergence is slowing down, and/or the user-specified restart limits allow it, go back to initialiing anothe rCG sequence, Step 2.\n",
    "    2. Otherwise go back to performing the next CG step in the current sequence, Step 3.\n",
    "7. The $f_n$ are the solution. Write the results. If the user has requested so, compute and write the approximate Hessian matrix $J^{T}WJ$, the exact Hessian (E8.1), or the weighted Jacobian sparse matrix $W^\\frac{1}{2}J$ (for computing these matrices, first convert the Jacobian table into the Jacobian matrix. End the run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e47ca87-bb76-4d28-a4d1-1e4b314b8486",
   "metadata": {},
   "source": [
    "### Variables and Loss Function\n",
    "\n",
    "#### Variables\n",
    "These variables are defined in section 2.\n",
    "\n",
    "$x$: The input data which is of size $m$ features and $n$ samples.<br/>\n",
    "$m$: The index $m$ enumerates the equations that embody the model to be Solved.<br/>\n",
    "$x_m$: The data to be fitted, corresponding to a vector containing elements of $x$ which would be denoted as $x_{ijk}$.<br/>\n",
    "$y_m$: The fitted values, defined by the factor elements.<br/>\n",
    "$K_m$: The number of product terms in each equation.<br/>\n",
    "$k$: The index $k$ enumerates the products that are added together in order that their sum be an approximation of $x_m$.<br/>\n",
    "$f_n$: The vector $f$, consists of values $f_n(n=1, ..., N)$, is the collection of all factor elements of the model.<br/>\n",
    "$T_{mk}$: The index sets $T_{mk}$ defines one product in one equation.<br/>\n",
    "$\\sigma_m$: The uncertainties $\\sigma_m$ are used to generate the weights $w_m$.<br/>\n",
    "\n",
    "#### Loss Function\n",
    "As defined in section 2.<br/><br/>\n",
    "$ Q(x,f) = \\sum_{m=1}^{M}{w_m}{e_m^2} = \\sum_{m=1}^{M}{w_m}(x_m - y_m)^2$ (2.5)<br/><br/>\n",
    "$ \\hat{f} = arg min_f \\sum_{m=1}^{M}w_{m}e_m^2 = arg min_f \\sum_{m=1}^{M}(\\frac{e_m}{\\sigma_m})^2 $ (2.7)<br/><br/>\n",
    "$ \\hat{f} = arg min_f \\sum_{m=1}^{M}(\\frac{x_m - \\sum_{k=1}^{K_m}\\prod_{n\\in{T_{mk}}}f_n}{\\sigma_m})^2$ (2.8)<br/><br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58856b9c-a84f-4f09-84ad-8b431bf7a450",
   "metadata": {},
   "source": [
    "### Detailed Algorithm\n",
    "\n",
    "Taking the pseudo algorithm stated above but including the details necessary for implementation and variable assignment.\n",
    "\n",
    "#### Terminology\n",
    "\n",
    "*free factor element*: is used for the unknowns, the profile factors (H) and factor contributions (W).<br/>\n",
    "*constrained*: indicates such *free factor elements* which are required to be non-negative (H).<br/>\n",
    "*subexpression*: (SE) variables are defined in terms of the fixed and unknown elements.<br/>\n",
    "\n",
    "#### Detailed Variables\n",
    "\n",
    "Defining all variables that are specified in the pseudocode.<br/>\n",
    "$X$: Input dataset of size $m$ features by $n$ samples.<br/>\n",
    "$U$: Uncertainty dataset of size $m$ features by $n$ samples.<br/>\n",
    "$w$: Weights, which are initialially defined from $U$ and feature categories. $w_m = \\frac{1}{\\sqrt{U_m}}$<br/>\n",
    "$W$: Diagonal matrix consisting of all weights $w_m$, $m = 1, ..., M$.<br/>\n",
    "$y$: Fitted values which are the matrix product of $WH$.<br/>\n",
    "$n$: The number of samples (rows).<br/>\n",
    "$m$: The number of features (columns).<br/>\n",
    "$h$: The number of factors.<br/>\n",
    "\n",
    "$f_n$/$f_h$: Individual components of the loss function.<br/>\n",
    "$J$: The Jacobian matrix, which is compose of three elements $J^{(1)}$, $J^{(2)}$, and $J^{(3)}$. The Jacobian is calculated from it's components defined $J = \\begin{bmatrix}J^{(1)}&J^{(2)}\\end{bmatrix} \\begin{bmatrix}I\\\\J^{(3)}\\end{bmatrix} $<br/>\n",
    "$J^{(1)}$: Component of the Jacobian matrix which consists of the partial derivatives of $y_m$ with respect to the free factor elements while keeping the SE factor elements constant.<br/>\n",
    "$J^{(2)}$: Component of the Jacobian matrix which consists of the partial derivatives of $y_m$ with respect to the SE factor elements.<br/>\n",
    "$J^{(3)}$: Component of the Jacobian matrix which consists of the partial derivatives $\\frac{\\partial{f_h}}{\\partial{f_n}} (h = N+1, ...,  N+ N^{SE}, n = 1, ..., N)$.<br/>\n",
    "$I$: Vector which contains the lower limits for factor elements, for non-negatively constrained factor elements $l_n = 0$ and for unconstrained factor elements, $l_n = -\\infty$.<br/>\n",
    "$\\epsilon$: Denotes a small constant, such as $10^{-12}$.<br/>\n",
    "\n",
    "$c$: Coefficients for inverse preconditioning of constrained variables.<br/>\n",
    "$g$: The computed gradient.<br/>\n",
    "$p$: The preconditioning coefficients.<br/>\n",
    "$z$: The transformed gradient $g$ which is the multiplicative product of the inverse preconditioning coefficient, preconditioning coefficient, and the gradient.<br/>\n",
    "$\\rho$: A componet in the calculation of the step direction.<br/> \n",
    "$\\beta$: A componet in the calculation of the step direction.<br/>\n",
    "$t$: The step direction.<br/>\n",
    "$\\alpha_k$: The step length.<br/>\n",
    "$\\tau$: A component in the calculation of the step length.<br/>\n",
    "$\\omega$: A component in the calculation of the step length.<br/>\n",
    "$e$: Convergence change threshold.<br/>\n",
    "$k$: Convergence change step. When the change in $Q$ over $k$ iterations is less than $e$ the solution is considered converged.<br/>\n",
    "\n",
    "#### Pseudocode\n",
    "\n",
    "1. Primary Initialization.\n",
    "    1. Set $c_n = 1.0$\n",
    "    2. Input dataset $X$, uncertainty $U$, initialize $W$ and $H$ matrices.\n",
    "2. CG Step Initialization.\n",
    "    1. Compute the weights $w_m$.\n",
    "    2. Compute the Jacobian: $j_{mn} = \\frac{\\partial{y_m}}{\\partial{f_n}} = \\sum_{k|n\\in T_{mk}} \\prod_{l\\in T_{mk}n}f_l$. This can be defined as the change in the WH over the change in the weighted/uncertainty squared residuals. The preconditioning coefficients are calculated using the Jacobian: $p_n = \\frac{1}{\\sum_{m}w_{m}j_{mn}^{2}}$\n",
    "    3. Set $\\rho = 0$.\n",
    "3. Perform one CG step.\n",
    "    1. Compute SE factors $f_h = \\sum_{k=1}^{S_h}\\prod_{n \\in S_{nk}}f_n$\n",
    "    2. Compute current fit $y_m$ and the initial object function: $ Q^{(1)} = \\sum_{m=1}^{M}w_m(x_m - y_m)^2 $\n",
    "    3. <span style=\"color:red\">Compute components of the Jacobian: $j_{mn} = \\frac{\\partial{y_m}}{\\partial{f_n}} = \\sum_{k|n\\in T_{mk}} \\prod_{l\\in T_{mk}n}f_l$</span>\n",
    "    4. Compute the gradient:  $g = J^TW(x - y) = J^{(1)T}W(x - y) + J^{(3)T}(J^{(2)T}W(x - y))$ using the components of the Jacobian.\n",
    "    5. Compute the transformed gradient $z_n = c_{n}p_{n}g_{n}$.\n",
    "    6. If $\\rho = 0$, first step, set $\\beta = 0$, $\\rho = g^{T}z$, else set $\\beta = \\frac{g^{T}z}{\\rho}$, $\\rho = g^T{z}$.\n",
    "    7. Update the step direction: $t = \\beta t + z$.\n",
    "    8. Compute $\\tau = t^{T}J^{T}WJt$, $\\omega = t^{T}J^{T}W(x - y)$\n",
    "    9. Compute initial approximation for the step length, $\\alpha = \\frac{\\omega}{\\tau}$.\n",
    "    10. Compute $Q^{(2)} = Q(max(f + \\alpha t, 1))$. $f$ is calculated as show in step 3.1/A \n",
    "    11. If $Q^{(2)} < Q^{(1)}$, compute nonlinearity-corrected step length as: $\\alpha = \\frac{\\alpha}{(2 - (Q^{(1)} - Q^{(2)})\\frac{\\tau}{\\omega^2})}$\n",
    "    12. Else, repeatedly try decreasing $\\alpha$, computing $Q^{(2)} = Q(max(f + \\alpha t, 1))$, until $Q^{(2)} < Q^{(1)}$.\n",
    "    13. If it was impossible to satisfy $Q^{(2)} < Q^{(1)}$, go to Step 2.\n",
    "4. Update inverse preconditioning. Denote indexes $n$ of *nonviolating* ($f_n > I_n$) constrained factor elements by $u$, and indexes of violating ($f_n = I_n$) constrained factor elements by $v$.\n",
    "    1. Increase $c_u$, constrained by $c_u \\le 1$.\n",
    "    2. Decrease $c_v$, constrained by $c_v \\ge \\epsilon$.\n",
    "    3. Decrease $t_v$, constrained by $t_v \\ge \\epsilon$.\n",
    "5. Write monitoring output, test for convergence.\n",
    "    1. Output to screen and to log file $Q^{(2)}$ and $\\rho$, that is, the goodness-of-fit and the squared length of the preconditioned gradient.\n",
    "    2. If $|Q^{(1)} - Q^{(2)}| < e$ ($e$ and $k$ are user-specified parameters), and if this condition has been valid in $k$ consecutive steps, assume convergence and go to the end, Step 7.\n",
    "    3. If the maximum iteration count is exceeded, go to the end, Step 7.\n",
    "6. Test for continuing this CG sequence.\n",
    "    1. If convergence is slowing down, and/or the user-specified restart limits allow it, go back to initialiing anothe rCG sequence, Step 2.\n",
    "    2. Otherwise go back to performing the next CG step in the current sequence, Step 3.\n",
    "7. The $f_n$ are the solution. Write the results. If the user has requested so, compute and write the approximate Hessian matrix $J^{T}WJ$, the exact Hessian (E8.1), or the weighted Jacobian sparse matrix $W^\\frac{1}{2}J$ (for computing these matrices, first convert the Jacobian table into the Jacobian matrix. End the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9043ae9-cd91-4300-8515-d323a655c45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from esat.data.datahandler import DataHandler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d30e12-b044-415d-9443-a1217b695b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = os.path.join(\"D:\\\\\", \"projects\", \"nmf_py\", \"data\", \"Dataset-BatonRouge-con.csv\")\n",
    "uncertainty_file = os.path.join(\"D:\\\\\", \"projects\", \"nmf_py\", \"data\", \"Dataset-BatonRouge-unc.csv\")\n",
    "output_path = os.path.join(\"D:\\\\\", \"projects\", \"nmf_py\", \"output\", \"BatonRouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6581a4f8-cdf3-4368-ab41-be65156d8688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23-May-23 11:12:31 - Input and output configured successfully\n"
     ]
    }
   ],
   "source": [
    "dh = DataHandler(\n",
    "        input_path=input_file,\n",
    "        uncertainty_path=uncertainty_file,\n",
    "        output_path=output_path,\n",
    "        index_col='Date'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77231fae-a82f-41c7-8b2f-cc3250e176ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = dh.input_data_processed\n",
    "U = dh.uncertainty_data_processed\n",
    "\n",
    "m, n = V.shape\n",
    "n_components = 4\n",
    "seed = 42\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "V_avg = np.sqrt(np.mean(V, axis=0) / n_components)\n",
    "H = V_avg * rng.standard_normal(size=(n_components, n)).astype(V.dtype, copy=False)\n",
    "H = np.abs(H)\n",
    "\n",
    "V_avg2 = np.sqrt(np.mean(V, axis=1) / n_components)\n",
    "V_avg2 = V_avg.reshape(len(V_avg), 1)\n",
    "W = np.multiply(V_avg, rng.standard_normal(size=(m, n_components)).astype(V.dtype, copy=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
