{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5686853f-4655-4e79-af04-fe24aac1756e",
   "metadata": {},
   "source": [
    "## ESAT K Estimation Workflow 1 - Batch Model Evaluation\n",
    "\n",
    "This notebook implements a batch model evaluation approach to using solution profile variabilty to estimate optimal number of factors in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ac55f-9259-4ce1-b87e-3302d2ed00f9",
   "metadata": {},
   "source": [
    "#### Code Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9de6b9e-89a5-4db8-94d4-2226605d9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from esat.data.datahandler import DataHandler\n",
    "from esat.model.sa import SA\n",
    "from esat.model.batch_sa import BatchSA\n",
    "from esat.data.analysis import ModelAnalysis, BatchAnalysis\n",
    "from esat.error.bootstrap import Bootstrap\n",
    "from esat_eval.simulator import Simulator\n",
    "from esat.estimator import FactorEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e80dbc-702b-4fca-b800-0660033a251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synethic dataset parameter value ranges\n",
    "syn_factors_min = 3\n",
    "syn_factors_max = 8\n",
    "\n",
    "syn_features_min = 15\n",
    "syn_features_max = 45\n",
    "\n",
    "syn_samples_min = 200\n",
    "syn_samples_max = 1000\n",
    "\n",
    "outliers = True\n",
    "outliers_p_min = 0.05\n",
    "outliers_p_max = 0.1\n",
    "outliers_mag_min = 1.1\n",
    "outliers_mag_max = 2\n",
    "\n",
    "noise_mean_min = 0.05\n",
    "noise_mean_max = 0.15\n",
    "noise_scale = 0.01\n",
    "\n",
    "uncertainty_mean_min = 0.05\n",
    "uncertainty_mean_max = 0.15\n",
    "uncertainty_scale = 0.01\n",
    "\n",
    "contr_curve_min_range = [0.0, 1.0]\n",
    "contr_curve_max_range = [2.0, 5.0]\n",
    "contr_curve_scale_range = [0.1, 0.5]\n",
    "\n",
    "random_seed = 337\n",
    "k_coef = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2408991-45a6-4a5c-ac14-388f79cbab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e045f7a-8a9a-4316-b60e-018935328241",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372a6a6a-612a-4d00-a3ae-be12aecb0e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulator with the above parameters\n",
    "def generate_synthetic_data(true_factor):\n",
    "    n_features = rng.integers(low=syn_features_min, high=syn_features_max, size=1)[0]\n",
    "    n_samples = rng.integers(low=syn_samples_min, high=syn_samples_max, size=1)[0]\n",
    "    i_outlier_p = round(rng.uniform(low=outliers_p_min, high=outliers_p_max, size=1)[0], 2)\n",
    "    i_outlier_mag = round(rng.uniform(low=outliers_mag_min, high=outliers_mag_max, size=1)[0], 2)\n",
    "    contribution_max = round(rng.uniform(low=1.0, high=10.0, size=1)[0], 2)\n",
    "    print(f\"True Factors: {true_factor}, Features: {n_features}, Samples: {n_samples}, Outliers %: {i_outlier_p}, Outliers Magnitude: {i_outlier_mag}, Contribution Max: {contribution_max}\")\n",
    "    simulator = Simulator(seed=rng.integers(low=0, high=10, size=1)[0],\n",
    "                          factors_n=true_factor,\n",
    "                          features_n=n_features,\n",
    "                          samples_n=n_samples,\n",
    "                          outliers=outliers,\n",
    "                          outlier_p=i_outlier_p,\n",
    "                          outlier_mag=i_outlier_mag,\n",
    "                          contribution_max=contribution_max,\n",
    "                          noise_mean_min=noise_mean_min,\n",
    "                          noise_mean_max=noise_mean_max,\n",
    "                          noise_scale=noise_scale,\n",
    "                          uncertainty_mean_min=uncertainty_mean_min,\n",
    "                          uncertainty_mean_max=uncertainty_mean_max,\n",
    "                          uncertainty_scale=uncertainty_scale,\n",
    "                          verbose=False\n",
    "                         )\n",
    "    curved_factors_count = rng.integers(low=0, high=true_factor, size=1)[0]\n",
    "    curved_factor_list = rng.choice(list(range(true_factor)), size=curved_factors_count, replace=False)\n",
    "    for c_i in curved_factor_list:\n",
    "        # parameters not used by the curve type are ignored\n",
    "        i_curve_type = rng.choice(['uniform', 'decreasing', 'increasing', 'logistic', 'periodic'], size=1)[0]\n",
    "        i_curve_min = rng.uniform(low=contr_curve_min_range[0], high=contr_curve_min_range[1], size=1)[0]\n",
    "        i_curve_max = rng.uniform(low=contr_curve_max_range[0], high=contr_curve_max_range[1], size=1)[0]\n",
    "        i_curve_scale = rng.uniform(low=contr_curve_scale_range[0], high=contr_curve_scale_range[1], size=1)[0]\n",
    "        i_curve_frequency = rng.uniform(low=0.1, high=0.9, size=1)[0]\n",
    "        \n",
    "        # To keep all as uniform comment out the line below\n",
    "        # simulator.update_contribution(factor_i=c_i, curve_type=i_curve_type, scale=i_curve_scale, frequency=i_curve_frequency, minimum=i_curve_min, maximum=i_curve_max)\n",
    "    \n",
    "    syn_input_df, syn_uncertainty_df = simulator.get_data()\n",
    "    data_handler = DataHandler.load_dataframe(input_df=syn_input_df, uncertainty_df=syn_uncertainty_df)\n",
    "    data_handler.metrics\n",
    "    V, U = data_handler.get_data()\n",
    "    return V, U\n",
    "\n",
    "\n",
    "def run_bs(k, bV, bU, bseed, bs_instances: int = 20, block_size: int = 4, threshold: float = 0.9):\n",
    "    # Runs a bootstrap instance\n",
    "    # Steps:\n",
    "    # 1. Generate base model with a specified number of factors\n",
    "    # 2. Generate bs_instances number of bootstrap datasets\n",
    "    # 3. For each bootstrap dataset, use the base model H profiles for initialization. Run to convergence.\n",
    "    # 4. On each converged bs model, calculate the average correlation of the mapped profiles.\n",
    "    # 5. Reported metrics for each bootstrap run. K, % mapped profiles, mean mapped correlation, mean overall correlation, mean QTrue\n",
    "\n",
    "    # Base Model, step 1\n",
    "    base_sa = SA(V=bV, U=bU, factors=k, seed=bseed, verbose=False)\n",
    "    base_sa.initialize()\n",
    "    base_sa.train(max_iter=10000, converge_delta=0.1, converge_n=20)\n",
    "\n",
    "    # BS instance, steps 2-3\n",
    "    bs = Bootstrap(sa=base_sa, model_selected=-1, bootstrap_n=bs_instances, block_size=block_size, threshold=threshold, seed=bseed, parallel=True)\n",
    "    bs.run()\n",
    "\n",
    "    # Evaluate correlations, step 4\n",
    "    # for each bs result get the mapping correlations bs.bs_results[1]['mapping']\n",
    "    compare_count = 0\n",
    "    mapped = 0\n",
    "    mapped_correlations = []\n",
    "    notmapped_correlations = []\n",
    "    for i, i_result in bs.bs_results.items():\n",
    "        i_mapping = i_result[\"mapping\"]\n",
    "        for j, j_factor in i_mapping.items():\n",
    "            compare_count += 1\n",
    "            if j_factor[\"mapped\"]:\n",
    "                mapped_correlations.append(j_factor[\"r2\"])\n",
    "                mapped += 1\n",
    "            else:\n",
    "                notmapped_correlations.append(j_factor[\"r2\"])\n",
    "    # return results, step 5\n",
    "    bs_results = {\n",
    "        \"k\": k,\n",
    "        \"seed\": bseed,\n",
    "        \"% mapped\": round((mapped/compare_count) * 100, 2),\n",
    "        \"mean mapped r2\": round(np.mean(mapped_correlations), 4),\n",
    "        \"mean r2\": round((np.sum(mapped_correlations)+np.sum(notmapped_correlations))/(len(mapped_correlations)+len(notmapped_correlations)), 4),\n",
    "        \"mean QRobust\": round(np.mean(bs.q_results), 4)\n",
    "    }\n",
    "    return bs_results\n",
    "\n",
    "def run_bs_batch(k, n_batches, bV, bU, bseed, bs_instances: int = 20, block_size: int = 4, threshold: float = 0.9, ):\n",
    "    results = {\n",
    "        \"k\": k,\n",
    "        \"seed\": bseed,\n",
    "        \"% mapped\": [],\n",
    "        \"mean mapped r2\": [],\n",
    "        \"mean r2\": [],\n",
    "        \"mean QRobust\": []\n",
    "    }\n",
    "    for i in range(n_batches):\n",
    "        i_seed = rng.integers(low=0, high=1e10, endpoint=True, size=1)[0]\n",
    "        i_result = run_bs(k=i_factor, bV=bV, bU=bU, bseed=i_seed, bs_instances=bs_instances)\n",
    "        results[\"% mapped\"].append(i_result[\"% mapped\"])\n",
    "        results[\"mean mapped r2\"].append(i_result[\"mean mapped r2\"])\n",
    "        results[\"mean r2\"].append(i_result[\"mean r2\"])\n",
    "        results[\"mean QRobust\"].append(i_result[\"mean QRobust\"])\n",
    "    results[\"% mapped\"] = np.mean(results[\"% mapped\"])\n",
    "    results[\"mean mapped r2\"] = np.mean(results[\"mean mapped r2\"])\n",
    "    results[\"mean r2\"] = np.mean(results[\"mean r2\"])\n",
    "    results[\"mean QRobust\"] = np.mean(results[\"mean QRobust\"])\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf75aee-32ec-49d5-afc9-2dc5760ede24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Factors: 6, Features: 24, Samples: 222, Outliers %: 0.06, Outliers Magnitude: 1.8, Contribution Max: 9.26\n",
      "{'k': 2, 'seed': 6827920442, '% mapped': 67.75, 'mean mapped r2': 0.9704900000000001, 'mean r2': 0.9006700000000001, 'mean QRobust': 54494.49254000001}\n",
      "{'k': 3, 'seed': 2857205905, '% mapped': 46.334, 'mean mapped r2': 0.95902, 'mean r2': 0.76839, 'mean QRobust': 37212.303490000006}\n",
      "{'k': 4, 'seed': 7708876169, '% mapped': 90.375, 'mean mapped r2': 0.9725400000000001, 'mean r2': 0.9556600000000002, 'mean QRobust': 22614.765160000003}\n",
      "{'k': 5, 'seed': 3396816599, '% mapped': 89.6, 'mean mapped r2': 0.97094, 'mean r2': 0.9497, 'mean QRobust': 15767.494090000002}\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "true_k = 6\n",
    "i_V, i_U = generate_synthetic_data(true_factor=true_k)\n",
    "\n",
    "n_batches = 10\n",
    "bs_instances = 20\n",
    "min_factors = 2\n",
    "max_factors = 10\n",
    "\n",
    "results_list0 = []\n",
    "for i_factor in range(min_factors, max_factors+1):\n",
    "    t0 = time.time()\n",
    "    bseed = rng.integers(low=0, high=1e10, endpoint=True, size=1)[0]\n",
    "    i_results = run_bs_batch(k=i_factor, n_batches=n_batches, bV=i_V, bU=i_U, bseed=bseed, bs_instances=bs_instances)\n",
    "    results_list0.append(i_results)\n",
    "    t1 = time.time()\n",
    "    print(f\"Results: {i_results}, Runtime {round(t1-t0, 2)} sec(s)\")\n",
    "# results_list0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7939ea-d6ce-4c9c-8c3d-9aeec7d87ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ee36d66-7066-44d1-ae0b-b8b3e2413bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Factors: 4, Features: 15, Samples: 398, Outliers %: 0.09, Outliers Magnitude: 1.93, Contribution Max: 6.59\n",
      "Predicted K: 4, R2: 0.9912600000000001\n",
      "True Factors: 6, Features: 16, Samples: 922, Outliers %: 0.07, Outliers Magnitude: 1.64, Contribution Max: 1.84\n",
      "Predicted K: 6, R2: 0.99318\n",
      "True Factors: 3, Features: 22, Samples: 535, Outliers %: 0.07, Outliers Magnitude: 1.5, Contribution Max: 1.59\n",
      "Predicted K: 3, R2: 0.9806000000000001\n",
      "True Factors: 4, Features: 35, Samples: 795, Outliers %: 0.05, Outliers Magnitude: 1.93, Contribution Max: 2.2\n",
      "Predicted K: 4, R2: 0.9872\n",
      "True Factors: 7, Features: 22, Samples: 817, Outliers %: 0.09, Outliers Magnitude: 1.61, Contribution Max: 3.45\n",
      "Predicted K: 6, R2: 0.992\n",
      "True Factors: 6, Features: 21, Samples: 338, Outliers %: 0.07, Outliers Magnitude: 1.39, Contribution Max: 7.65\n",
      "Predicted K: 6, R2: 0.9901399999999999\n",
      "True Factors: 3, Features: 40, Samples: 284, Outliers %: 0.05, Outliers Magnitude: 1.71, Contribution Max: 1.96\n",
      "Predicted K: 3, R2: 0.9987\n",
      "True Factors: 3, Features: 30, Samples: 929, Outliers %: 0.08, Outliers Magnitude: 1.41, Contribution Max: 9.88\n",
      "Predicted K: 3, R2: 0.99982\n",
      "True Factors: 6, Features: 27, Samples: 457, Outliers %: 0.09, Outliers Magnitude: 1.58, Contribution Max: 7.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dsmith\\Anaconda3\\envs\\esat-dev\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\dsmith\\Anaconda3\\envs\\esat-dev\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted K: 6, R2: 0.98924\n",
      "True Factors: 7, Features: 15, Samples: 329, Outliers %: 0.06, Outliers Magnitude: 1.43, Contribution Max: 2.05\n",
      "Predicted K: 6, R2: 0.95954\n",
      "True Factors: 5, Features: 25, Samples: 698, Outliers %: 0.05, Outliers Magnitude: 1.61, Contribution Max: 1.15\n",
      "Predicted K: 5, R2: 0.9984\n",
      "True Factors: 7, Features: 28, Samples: 917, Outliers %: 0.06, Outliers Magnitude: 1.73, Contribution Max: 7.84\n",
      "Predicted K: 6, R2: 0.9943200000000001\n",
      "True Factors: 5, Features: 43, Samples: 208, Outliers %: 0.07, Outliers Magnitude: 1.59, Contribution Max: 3.26\n",
      "Predicted K: 5, R2: 0.9859399999999999\n",
      "True Factors: 8, Features: 32, Samples: 227, Outliers %: 0.06, Outliers Magnitude: 1.62, Contribution Max: 2.63\n",
      "Predicted K: 8, R2: 0.9487\n",
      "True Factors: 5, Features: 38, Samples: 913, Outliers %: 0.1, Outliers Magnitude: 1.43, Contribution Max: 8.51\n",
      "Predicted K: 5, R2: 0.999\n",
      "True Factors: 4, Features: 16, Samples: 218, Outliers %: 0.07, Outliers Magnitude: 1.91, Contribution Max: 9.72\n",
      "Predicted K: 4, R2: 0.99154\n",
      "True Factors: 7, Features: 34, Samples: 921, Outliers %: 0.07, Outliers Magnitude: 1.83, Contribution Max: 8.94\n",
      "Predicted K: 7, R2: 0.992\n",
      "True Factors: 5, Features: 15, Samples: 584, Outliers %: 0.09, Outliers Magnitude: 1.64, Contribution Max: 3.49\n",
      "Predicted K: 5, R2: 0.9968199999999999\n",
      "True Factors: 5, Features: 20, Samples: 964, Outliers %: 0.08, Outliers Magnitude: 1.31, Contribution Max: 2.19\n",
      "Predicted K: 5, R2: 0.99932\n",
      "True Factors: 7, Features: 44, Samples: 846, Outliers %: 0.09, Outliers Magnitude: 1.39, Contribution Max: 2.63\n",
      "Predicted K: 7, R2: 0.9944200000000001\n",
      "True Factors: 4, Features: 27, Samples: 553, Outliers %: 0.08, Outliers Magnitude: 1.21, Contribution Max: 4.92\n",
      "Predicted K: 4, R2: 0.9874600000000001\n",
      "True Factors: 6, Features: 30, Samples: 560, Outliers %: 0.09, Outliers Magnitude: 1.87, Contribution Max: 3.95\n",
      "Predicted K: 6, R2: 0.99458\n",
      "True Factors: 6, Features: 18, Samples: 578, Outliers %: 0.08, Outliers Magnitude: 1.91, Contribution Max: 2.85\n",
      "Predicted K: 4, R2: 0.9865\n",
      "True Factors: 4, Features: 20, Samples: 592, Outliers %: 0.06, Outliers Magnitude: 1.36, Contribution Max: 6.92\n",
      "Predicted K: 4, R2: 0.9983599999999999\n",
      "True Factors: 7, Features: 26, Samples: 427, Outliers %: 0.09, Outliers Magnitude: 1.99, Contribution Max: 6.98\n",
      "Predicted K: 5, R2: 0.97536\n",
      "True Factors: 5, Features: 44, Samples: 596, Outliers %: 0.08, Outliers Magnitude: 1.89, Contribution Max: 9.14\n",
      "Predicted K: 5, R2: 0.9953800000000002\n",
      "True Factors: 3, Features: 25, Samples: 889, Outliers %: 0.1, Outliers Magnitude: 1.4, Contribution Max: 3.08\n",
      "Predicted K: 3, R2: 0.99976\n",
      "True Factors: 3, Features: 41, Samples: 741, Outliers %: 0.06, Outliers Magnitude: 1.6, Contribution Max: 6.1\n",
      "Predicted K: 3, R2: 0.9900599999999999\n",
      "True Factors: 6, Features: 33, Samples: 225, Outliers %: 0.06, Outliers Magnitude: 1.49, Contribution Max: 5.22\n",
      "Predicted K: 7, R2: 0.9533799999999999\n",
      "True Factors: 3, Features: 29, Samples: 254, Outliers %: 0.09, Outliers Magnitude: 1.56, Contribution Max: 9.82\n",
      "Predicted K: 3, R2: 0.98608\n",
      "True Factors: 8, Features: 19, Samples: 848, Outliers %: 0.1, Outliers Magnitude: 1.26, Contribution Max: 9.66\n",
      "Predicted K: 2, R2: 0.9777000000000001\n",
      "True Factors: 7, Features: 30, Samples: 561, Outliers %: 0.05, Outliers Magnitude: 1.21, Contribution Max: 3.21\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:19\u001b[0m\n",
      "Cell \u001b[1;32mIn[4], line 100\u001b[0m, in \u001b[0;36mrun_bs_batch\u001b[1;34m(k, n_batches, bV, bU, bseed, bs_instances, block_size, threshold)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_batches):\n\u001b[0;32m     99\u001b[0m     i_seed \u001b[38;5;241m=\u001b[39m rng\u001b[38;5;241m.\u001b[39mintegers(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e10\u001b[39m, endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 100\u001b[0m     i_result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_bs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbV\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbU\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi_seed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs_instances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbs_instances\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m     results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m mapped\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(i_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m mapped\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    102\u001b[0m     results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean mapped r2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(i_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean mapped r2\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[4], line 57\u001b[0m, in \u001b[0;36mrun_bs\u001b[1;34m(k, bV, bU, bseed, bs_instances, block_size, threshold)\u001b[0m\n\u001b[0;32m     55\u001b[0m base_sa \u001b[38;5;241m=\u001b[39m SA(V\u001b[38;5;241m=\u001b[39mbV, U\u001b[38;5;241m=\u001b[39mbU, factors\u001b[38;5;241m=\u001b[39mk, seed\u001b[38;5;241m=\u001b[39mbseed, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     56\u001b[0m base_sa\u001b[38;5;241m.\u001b[39minitialize()\n\u001b[1;32m---> 57\u001b[0m \u001b[43mbase_sa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# BS instance, steps 2-3\u001b[39;00m\n\u001b[0;32m     60\u001b[0m bs \u001b[38;5;241m=\u001b[39m Bootstrap(sa\u001b[38;5;241m=\u001b[39mbase_sa, model_selected\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, bootstrap_n\u001b[38;5;241m=\u001b[39mbs_instances, block_size\u001b[38;5;241m=\u001b[39mblock_size, threshold\u001b[38;5;241m=\u001b[39mthreshold, seed\u001b[38;5;241m=\u001b[39mbseed, parallel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\esat-dev\\Lib\\site-packages\\esat\\model\\sa.py:438\u001b[0m, in \u001b[0;36mSA.train\u001b[1;34m(self, max_iter, converge_delta, converge_n, model_i, robust_mode, robust_n, robust_alpha, update_step, bump, bump_n, bump_range)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    437\u001b[0m W, H, q, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverged, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverge_steps, q_list \u001b[38;5;241m=\u001b[39m _results\n\u001b[1;32m--> 438\u001b[0m q_true \u001b[38;5;241m=\u001b[39m \u001b[43mq_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mV\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mU\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m q_robust, U_robust \u001b[38;5;241m=\u001b[39m qr_loss(V\u001b[38;5;241m=\u001b[39mV, U\u001b[38;5;241m=\u001b[39mU, W\u001b[38;5;241m=\u001b[39mW, H\u001b[38;5;241m=\u001b[39mH, alpha\u001b[38;5;241m=\u001b[39mrobust_alpha)\n\u001b[0;32m    440\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\esat-dev\\Lib\\site-packages\\esat\\metrics.py:16\u001b[0m, in \u001b[0;36mq_loss\u001b[1;34m(V, U, W, H, uncertainty)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mq_loss\u001b[39m(V, U, W, H, uncertainty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     15\u001b[0m     _wh \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmatmul(W, H)\n\u001b[1;32m---> 16\u001b[0m     residuals \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubtract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_wh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m uncertainty:\n\u001b[0;32m     18\u001b[0m         residuals_u \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdivide(residuals, U)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Sampling parameters\n",
    "n_batches = 5\n",
    "bs_instances = 10\n",
    "min_factors = 2\n",
    "max_factors = 10\n",
    "all_results = []\n",
    "\n",
    "# for j in range(100):\n",
    "#     t0 = time.time()\n",
    "#     true_k = rng.integers(low=syn_factors_min, high=syn_factors_max, endpoint=True, size=1)[0]\n",
    "#     i_V, i_U = generate_synthetic_data(true_factor=true_k)\n",
    "    \n",
    "#     results_list = []\n",
    "#     predicted_k = -1\n",
    "#     best_r2 = 0\n",
    "#     for i_factor in range(min_factors, max_factors+1):\n",
    "#         bseed = rng.integers(low=0, high=1e10, endpoint=True, size=1)[0]\n",
    "#         i_results = run_bs_batch(k=i_factor, n_batches=n_batches, bV=i_V, bU=i_U, bseed=bseed, bs_instances=bs_instances)\n",
    "#         if i_results['mean r2'] > best_r2:\n",
    "#             best_r2 = i_results['mean r2']\n",
    "#             predicted_k = i_results['k']\n",
    "#         results_list.append(i_results)\n",
    "#     t1 = time.time()\n",
    "#     print(f\"Predicted K: {predicted_k}, R2: {best_r2}. Runtime: {round(t1-t0, 4)} sec\")\n",
    "#     all_results.append(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a0b90b7-c68c-4d64-af4b-47033406e5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def accuracy(occurences, results):\n",
    "#     accuracy_list = [0]*len(occurences)\n",
    "#     for i in range(len(occurences)):\n",
    "#         if occurences[i] == 0:\n",
    "#             accuracy_list[i] = 0\n",
    "#         else:\n",
    "#             accuracy_list[i] = round(results[i]/occurences[i],2)\n",
    "#     return np.multiply(100, accuracy_list)\n",
    "\n",
    "# elements = max_factors-min_factors - 1\n",
    "\n",
    "# dr_results = [0]*elements\n",
    "# kest_results = [0]*elements\n",
    "# qtrue_results = [0]*elements\n",
    "# qrobust_results = [0]*elements\n",
    "# k_runs = [0]*elements\n",
    "\n",
    "# for _result in est_list:\n",
    "#     idx_k = _result['true K'] - min_factors - 1\n",
    "#     true_k = _result['true K']\n",
    "#     k_runs[idx_k] += 1\n",
    "#     if _result['delta ratio'] == true_k:\n",
    "#         dr_results[idx_k] += 1\n",
    "#     if _result['K estimate'] == true_k:\n",
    "#         kest_results[idx_k] += 1\n",
    "#     if _result['QTrue Overall'] == true_k:\n",
    "#         qtrue_results[idx_k] += 1\n",
    "#     if _result['QRobust Overall'] == true_k:\n",
    "#         qrobust_results[idx_k] += 1\n",
    "\n",
    "# labels = [f\"{i + min_factors} Factor(s)\" for i in range(min_factors-1,max_factors)]\n",
    "\n",
    "# result_fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "# result_fig.add_trace(go.Bar(name=\"Delta Ratio\", x=labels, y=accuracy(k_runs, dr_results)), secondary_y=False)\n",
    "# result_fig.add_trace(go.Bar(name=\"K Estimate\", x=labels, y=accuracy(k_runs, kest_results)), secondary_y=False)\n",
    "# result_fig.add_trace(go.Bar(name=\"QTrue Overall\", x=labels, y=accuracy(k_runs, qtrue_results)), secondary_y=False)\n",
    "# result_fig.add_trace(go.Bar(name=\"QRobust Overall\", x=labels, y=accuracy(k_runs, qrobust_results)), secondary_y=False)\n",
    "# result_fig.add_trace(go.Scatter(name=\"K Runs\", x=labels, y=k_runs, mode='markers'), secondary_y=True)\n",
    "\n",
    "# result_fig.update_layout(title=\"K Estimation Randomized Results\", barmode='group', height=600, width=1200)\n",
    "# result_fig.update_yaxes(title_text=\"Accuracy (%)\", range=[0, 100.0], secondary_y=False)\n",
    "# result_fig.update_yaxes(title_text=\"Run Count\", secondary_y=True)\n",
    "# result_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b055eab-32ea-411e-be35-8ca7ed64b733",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'k_runs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Runs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39msum(\u001b[43mk_runs\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDelta Ratio Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(dr_results)\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(k_runs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK Estimate Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(kest_results)\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(k_runs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'k_runs' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Total Runs: {np.sum(k_runs)}\")\n",
    "print(f\"Delta Ratio Accuracy: {100*np.sum(dr_results)/np.sum(k_runs)}%\")\n",
    "print(f\"K Estimate Accuracy: {100*np.sum(kest_results)/np.sum(k_runs)}%\")\n",
    "print(f\"QTrue Overall Accuracy: {100*np.sum(qtrue_results)/np.sum(k_runs)}%\")\n",
    "print(f\"QRobust Overall Accuracy: {100*np.sum(qrobust_results)/np.sum(k_runs)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3946a50-7822-4390-8c58-499aedc388a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# est_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a400b685-3750-47cc-917b-cf57e33cb175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.DataFrame(est_list)\n",
    "# results_df.to_csv(\"Run_Results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148df7cb-1f6d-4da7-9c2c-b8d578b34a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_results = {\"dr\": 0, \"k\": 0, \"qt\":0, \"qr\":0}\n",
    "for i_run in est_list:\n",
    "    rank_results['dr'] += i_run[\"DR Rank\"]/len(est_list)\n",
    "    rank_results['k'] += i_run[\"K Rank\"]/len(est_list)\n",
    "    rank_results['qt'] += i_run[\"QT Rank\"]/len(est_list)\n",
    "    rank_results['qr'] += i_run[\"QR Rank\"]/len(est_list)\n",
    "print(f\"Average Ranks - DR: {round(rank_results['dr'], 2)}, K: {round(rank_results['k'],2)}, QTrue: {round(rank_results['qt'],2)}, QRobust: {round(rank_results['qr'],2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f211c3a5-cf67-466c-8b41-8ea2029b8d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_list\n",
    "results_list_df = None\n",
    "\n",
    "for j,k in enumerate(results_list):\n",
    "    k = k.drop(\"Iteration Number\",errors=\"ignore\", axis=1)\n",
    "    actual_k = est_list[j][\"true K\"]\n",
    "    k_true = [actual_k]*k.shape[0]\n",
    "    k_rows = [j+1]*k.shape[0]\n",
    "    k.insert(0,\"Iteration Number\",k_rows)\n",
    "    k.insert(1,\"True k\",k_true)\n",
    "\n",
    "    w1 = 0.25\n",
    "    w2 = 0.50\n",
    "    w3 = 0.25\n",
    "\n",
    "    k[\"QTW\"] = w1*k[\"K Estimate\"]/k[\"K Estimate\"].max()+w2*k[\"Delta Ratio\"]/k[\"Delta Ratio\"].max()+w3*k[\"Q(True)\"].min()/k[\"Q(True)\"]\n",
    "    k[\"QRW\"] = w1*k[\"K Estimate\"]/k[\"K Estimate\"].max()+w2*k[\"Delta Ratio\"]/k[\"Delta Ratio\"].max()+w3*k[\"Q(Robust)\"].min()/k[\"Q(Robust)\"]\n",
    "\n",
    "    k_chopped = k.iloc[1:-1]\n",
    "    k[\"QTW Rank\"] =  [None]+list(k_chopped[\"QTW\"].rank(ascending=False))+ [None]\n",
    "    k[\"QRW Rank\"] =  [None]+list(k_chopped[\"QRW\"].rank(ascending=False))+ [None]\n",
    "\n",
    "    #qtr_ordered_list = k_chopped.sort_values(\"Weighted QRobust Overall\", ascending=False).reset_index()\n",
    "    #k[\"QTR Rank\"] = [None]+list(qtr_ordered_list.index)+[None]\n",
    "    \n",
    "    if results_list_df is None:\n",
    "        results_list_df = k\n",
    "    else:\n",
    "        results_list_df = results_list_df.merge(k,how=\"outer\")\n",
    "\n",
    "results_list_df = results_list_df.sort_values(by=[\"Iteration Number\",\"Factors\"]).reset_index(drop=True).round(decimals=4)\n",
    "\n",
    "# results_list_df.to_csv(\"Complete_Run_Results.csv\",index=False)\n",
    "\n",
    "results_list_df.loc[(results_list_df[\"Iteration Number\"]==1) | (results_list_df[\"Iteration Number\"]==2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1628b9e7-70ec-4ae2-9ee5-98e05e1f72de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882b8c85-86b3-4957-a694-ed0681e55011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
