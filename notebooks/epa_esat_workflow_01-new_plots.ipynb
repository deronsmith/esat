{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5686853f-4655-4e79-af04-fe24aac1756e",
   "metadata": {},
   "source": [
    "## Environmental Source Apportionment Toolkit (ESAT) Workflow\n",
    "\n",
    "The source apportionment workflow that is available in ESAT replicates the functionality that is found in PMF5. The workflow covers all of the steps for each feature that is available in PMF5:\n",
    " 1. Data preprocessing, cleanup and analysis.\n",
    " 2. Source apportionment model creation.\n",
    " 3. Post-processing analysis and visualizations.\n",
    " 4. Error estimation analysis of the source apportionment model.\n",
    "    1. Bootstrap (BS)\n",
    "    2. Displacement (DISP)\n",
    "    3. Bootstrap-Displacement (BS-DISP)\n",
    " 5. Constrained source apportionment model. \n",
    "\n",
    "The code provided in this notebook are intended to provide an example of how to implement the ESAT workflow programmaticaly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e819075f-8cd8-401c-9b28-0d703dd7f3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook imports\n",
    "import os\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208cc14-b39c-4095-a100-35b85f4d19e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When running from Google Colab or other Jupyter notebook cloud environment, the esat python package may need to be installed.\n",
    "# If the python package file is available locally run a pip install for the specific wheel for your current OS/Arch\n",
    "#! pip install esat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770385fe-48ab-47c4-bdbb-50bab6950d05",
   "metadata": {},
   "source": [
    "#### Code Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9de6b9e-89a5-4db8-94d4-2226605d9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from esat.data.datahandler import DataHandler\n",
    "from esat.model.sa import SA\n",
    "from esat.model.batch_sa import BatchSA\n",
    "from esat.data.analysis import ModelAnalysis, BatchAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb2b9c-98ee-4482-a0f9-c06b1eeb651f",
   "metadata": {},
   "source": [
    "#### Sample Dataset\n",
    "The three sample datasets from PMF5 are available for use, but a new dataset can be used in their place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6887c06-7ebe-4fe4-82fd-5bcd5ec82604",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "data_dir = os.path.join(cwd, \"..\", \"data\")\n",
    "\n",
    "# Baton Rouge Dataset\n",
    "br_input_file = os.path.join(data_dir, \"Dataset-BatonRouge-con.csv\")\n",
    "br_uncertainty_file = os.path.join(data_dir, \"Dataset-BatonRouge-unc.csv\")\n",
    "br_output_path = os.path.join(data_dir, \"output\", \"BatonRouge\")\n",
    "# Baltimore Dataset\n",
    "b_input_file = os.path.join(data_dir, \"Dataset-Baltimore_con.txt\")\n",
    "b_uncertainty_file = os.path.join(data_dir, \"Dataset-Baltimore_unc.txt\")\n",
    "b_output_path = os.path.join(data_dir, \"output\", \"Baltimore\")\n",
    "# Saint Louis Dataset\n",
    "sl_input_file = os.path.join(data_dir, \"Dataset-StLouis-con.csv\")\n",
    "sl_uncertainty_file = os.path.join(data_dir, \"Dataset-StLouis-unc.csv\")\n",
    "sl_output_path = os.path.join(data_dir, \"output\", \"StLouis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f532c5-1e71-4966-8edf-e8f68fa0bd57",
   "metadata": {},
   "source": [
    "#### Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2281601e-af6e-43b4-80eb-317ebfaf5ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_col = \"Date\"                  # the index of the input/uncertainty datasets\n",
    "factors = 6                         # the number of factors\n",
    "method = \"ls-nmf\"                   # \"ls-nmf\", \"ws-nmf\"\n",
    "models = 20                         # the number of models to train\n",
    "init_method = \"col_means\"           # default is column means \"col_means\", \"kmeans\", \"cmeans\"\n",
    "init_norm = True                    # if init_method=kmeans or cmeans, normalize the data prior to clustering.\n",
    "seed = 42                           # random seed for initialization\n",
    "max_iterations = 20000              # the maximum number of iterations for fitting a model\n",
    "converge_delta = 0.01               # convergence criteria for the change in loss, Q\n",
    "converge_n = 50                     # convergence criteria for the number of steps where the loss changes by less than converge_delta\n",
    "verbose = True                      # adds more verbosity to the algorithm workflow on execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90318121-9709-4e67-baf3-8f76432f68ed",
   "metadata": {},
   "source": [
    "#### Dataset Selection\n",
    "One of the three sample datasets can be selected or a new cleaned dataset can be used. Datasets should be cleaned, containing no missing data (either dropping missing/NaNs, or interpolating the missing values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f90ce2-d9d3-42cd-8a30-aa5b0b73fe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Baton Rouge dataset\n",
    "input_file = br_input_file\n",
    "uncertainty_file = br_uncertainty_file\n",
    "output_path = b_output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d605314-c34a-434d-a178-81b6f6b3766d",
   "metadata": {},
   "source": [
    "#### Load Data\n",
    "Assign the processed data and uncertainty datasets to the variables V and U. These steps will be simplified/streamlined in a future version of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc1f2f1-b5af-4913-bcb8-29b58e1b1d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = DataHandler(\n",
    "    input_path=input_file,\n",
    "    uncertainty_path=uncertainty_file,\n",
    "    index_col=index_col\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe791331-ba32-40d7-a0c5-6a03c83e7eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler.input_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412787cb-62cf-4a86-9a2d-f368030e11db",
   "metadata": {},
   "source": [
    "#### Input/Uncertainty Data Metrics and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eafe023-e783-4c17-bf08-7242f375cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.colors\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200e7656-5922-446e-a2fa-7b16eae2afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_correlation_heatmap(df: pd.DataFrame, method: str = \"pearson\", show: bool = True):\n",
    "    \"\"\"\n",
    "    Plots a correlation heatmap for the features in the DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame with features as columns.\n",
    "    method : str\n",
    "        Correlation method: 'pearson', 'spearman', or 'kendall'.\n",
    "    show : bool\n",
    "        Whether to display the plot immediately.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    plotly.graph_objects.Figure\n",
    "        The Plotly heatmap figure.\n",
    "    \"\"\"\n",
    "    corr = df.corr(method=method)\n",
    "    fig = go.Figure(\n",
    "        data=go.Heatmap(\n",
    "            z=corr.values,\n",
    "            x=corr.columns,\n",
    "            y=corr.columns,\n",
    "            colorscale=\"rdylbu\",\n",
    "            reversescale=True,\n",
    "            colorbar=dict(title=\"Correlation\"),\n",
    "            zmin=-1, zmax=1\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title=dict(x=0.5, xanchor=\"center\", text=f\"Feature Correlation Heatmap ({method.title()})\"),\n",
    "        xaxis_title=\"Features\",\n",
    "        yaxis_title=\"Features\",\n",
    "        width=800,\n",
    "        height=800,\n",
    "        margin=dict(l=0, r=0, t=50, b=0)\n",
    "    )\n",
    "    if show:\n",
    "        fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa9e6ed-3817-4fae-9821-c91ff32faf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_feature_correlation_heatmap(data_handler.input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63bcb3f-b4b1-43f5-8baf-c4e1883b1f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_superimposed_histograms(df, show: bool=True, nbins: int=50):\n",
    "    \"\"\"\n",
    "    Plots superimposed histograms for each feature in the input data using a colormap.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    # Use a qualitative color palette from Plotly\n",
    "    colors = getattr(plotly.colors.qualitative, \"Plotly\")\n",
    "    n_colors = len(colors)\n",
    "    for i, col in enumerate(df.columns):\n",
    "        fig.add_trace(go.Histogram(\n",
    "            x=df[col],\n",
    "            name=str(col),\n",
    "            opacity=0.5,\n",
    "            nbinsx=nbins,\n",
    "            marker_color=colors[i % n_colors]\n",
    "        ))\n",
    "    fig.update_layout(\n",
    "        barmode='overlay',\n",
    "        title=dict(x=0.5, xanchor=\"center\", text='Histograms of Features'),\n",
    "        xaxis_title='Value',\n",
    "        yaxis_title='Count',\n",
    "        width=1200,\n",
    "        height=800,\n",
    "        margin=dict(l=10, r=5, t=50, b=10),\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    if show:\n",
    "        fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35b5caf-ae9a-4554-849c-8b69167dc7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_superimposed_histograms(data_handler.input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25779ae6-593f-40b9-a0e4-39dbb37fe682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_histogram(df, x_col, y_col, show=True, nbins=100):\n",
    "    fig = go.Figure(data=go.Histogram2d(\n",
    "        x=df[x_col],\n",
    "        y=df[y_col],\n",
    "        nbinsx=nbins,\n",
    "        nbinsy=nbins,\n",
    "        colorscale='Blues'\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        title=dict(x=0.5, xanchor=\"center\", text=f'2D Histogram: {x_col} vs {y_col}'),\n",
    "        xaxis_title=x_col,\n",
    "        yaxis_title=y_col,\n",
    "        width=800,\n",
    "        height=800,\n",
    "        margin=dict(l=20, r=20, t=60, b=20),\n",
    "    )\n",
    "    if show:\n",
    "        fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84035a0-df03-49bd-9856-bda188c11afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_2d_histogram(data_handler.input_data, \"124-Trimethylbenzene\", \"224-Trimethylpentane\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ca1d96-fc7d-4af8-b480-4857a5f960cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_histogram_ribbons_log(df, nbins=50, show=True):\n",
    "    \"\"\"\n",
    "    Plots a 3D histogram where each feature is a separate 'ribbon' along the y-axis, with log-scaled x-axis.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    for i, col in enumerate(df.columns):\n",
    "        data = df[col].dropna().values\n",
    "        # Only positive values for log scale\n",
    "        data = data[data > 0]\n",
    "        if data.size == 0:\n",
    "            continue\n",
    "        counts, bin_edges = np.histogram(data, bins=nbins)\n",
    "        bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "        # Only keep bins with positive centers\n",
    "        mask = bin_centers > 0\n",
    "        bin_centers = bin_centers[mask]\n",
    "        counts = counts[mask]\n",
    "        y = np.full_like(bin_centers, i, dtype=float)\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=np.log10(bin_centers),\n",
    "            y=y,\n",
    "            z=counts,\n",
    "            mode='lines',\n",
    "            name=str(col),\n",
    "            line=dict(width=6),\n",
    "            hovertemplate=f'Feature: {col}<br>log10(Bin): %{{x}}<br>Count: %{{z}}<extra></extra>'\n",
    "        ))\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='log10(Value)',\n",
    "            yaxis_title='Feature',\n",
    "            yaxis=dict(\n",
    "                tickvals=list(range(len(df.columns))),\n",
    "                ticktext=[str(col) for col in df.columns]\n",
    "            ),\n",
    "            zaxis_title='Count',\n",
    "            xaxis_type='linear'  # log scale is handled by log10 transform\n",
    "        ),\n",
    "        title='3D Histogram Ribbon (Log Scale)',\n",
    "        width=900,\n",
    "        height=600\n",
    "    )\n",
    "    if show:\n",
    "        fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eee845-6756-43af-9c13-b3e85b71bf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_3d_histogram_ribbons_log(data_handler.input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66fdb23-a35e-4d08-b45c-86e2749251e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotly_ridgeline(df, log_x=False, fill=True, max_height=900, min_spacing=0.5, max_spacing=1.5, nbins=500, show=True):\n",
    "    n = len(df.columns)\n",
    "    spacing = min(max_spacing, max(min_spacing, (max_height - 100) / n / 50))\n",
    "    fig = go.Figure()\n",
    "    y_ticks = []\n",
    "    y_labels = []\n",
    "    for i, col in enumerate(df.columns):\n",
    "        data = df[col].dropna().values\n",
    "        if log_x:\n",
    "            data = data[data > 0]\n",
    "            log_data = np.log10(data)\n",
    "            x_grid = np.linspace(log_data.min(), log_data.max(), nbins)\n",
    "            actual_x = 10 ** x_grid\n",
    "            kde = gaussian_kde(log_data)\n",
    "            y = kde(x_grid)\n",
    "            feature_names = np.full_like(x_grid, col, dtype=object)\n",
    "            customdata = np.stack([actual_x, x_grid, feature_names], axis=-1)\n",
    "        else:\n",
    "            if len(data) < 2:\n",
    "                continue\n",
    "            x_grid = np.linspace(data.min(), data.max(), nbins)\n",
    "            kde = gaussian_kde(data)\n",
    "            y = kde(x_grid)\n",
    "            feature_names = np.full_like(x_grid, col, dtype=object)\n",
    "            customdata = np.stack([x_grid, np.log10(x_grid + 1e-12), feature_names], axis=-1)\n",
    "        y_offset = i * spacing\n",
    "        y_ticks.append(y_offset)\n",
    "        y_labels.append(str(col))\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=x_grid if log_x else x_grid,\n",
    "            y=y + y_offset,\n",
    "            mode='lines',\n",
    "            fill='tozeroy' if fill else None,\n",
    "            name=str(col),\n",
    "            line=dict(width=2),\n",
    "            customdata=customdata,\n",
    "            hovertemplate=(\n",
    "                \"Feature: %{customdata[2]}<br>\"\n",
    "                \"log10(Value): %{customdata[1]:.3f}<br>\"\n",
    "                \"Value: %{customdata[0]:.3g}<br>\"\n",
    "                \"Density: %{y:.3g}<extra></extra>\"\n",
    "            )\n",
    "        ))\n",
    "    fig.update_layout(\n",
    "        yaxis=dict(\n",
    "            tickvals=y_ticks,\n",
    "            ticktext=y_labels,\n",
    "            title='Feature'\n",
    "        ),\n",
    "        xaxis_title='log10(Value)' if log_x else 'Value',\n",
    "        title='Ridgeline Plot of Feature Distributions',\n",
    "        showlegend=False,\n",
    "        height=max_height,\n",
    "        width=900,\n",
    "        margin=dict(l=80, r=40, t=60, b=40)\n",
    "    )\n",
    "    if show:\n",
    "        fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da59bf5-e7a0-474f-9000-c0734d616287",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plotly_ridgeline(data_handler.input_data, log_x=True, fill=False, max_height=800, min_spacing=0.5, max_spacing=1.0, nbins=500, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a565f912-4495-4d67-911d-40f2fb27aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(\n",
    "    df: pd.DataFrame,\n",
    "    dim: int,\n",
    "    threshold: float = 3.0,\n",
    "    random_state: int = 42,\n",
    "    max_iter: int = 200\n",
    "):\n",
    "    \"\"\"\n",
    "    Detects outliers in a DataFrame using an MLPRegressor autoencoder.\n",
    "    Returns:\n",
    "        - DataFrame with reconstruction errors and row-wise outlier mask.\n",
    "        - DataFrame with value-wise outlier mask (same shape as df).\n",
    "    \"\"\"\n",
    "    X = df.values\n",
    "    autoencoder = MLPRegressor(hidden_layer_sizes=(dim,), activation='relu',\n",
    "                               max_iter=max_iter, random_state=random_state)\n",
    "    autoencoder.fit(X, X)\n",
    "    # Encode\n",
    "    hidden_weights = autoencoder.coefs_[0]\n",
    "    hidden_bias = autoencoder.intercepts_[0]\n",
    "    encoded = X @ hidden_weights + hidden_bias\n",
    "    encoded = np.maximum(encoded, 0)  # relu activation\n",
    "    # Decode (reconstruct)\n",
    "    output_weights = autoencoder.coefs_[1]\n",
    "    output_bias = autoencoder.intercepts_[1]\n",
    "    reconstructed = encoded @ output_weights + output_bias\n",
    "\n",
    "    # Row-wise (sample) outlier detection\n",
    "    errors = np.mean((X - reconstructed) ** 2, axis=1)\n",
    "    err_mean = np.mean(errors)\n",
    "    err_std = np.std(errors)\n",
    "    outlier_mask = errors > (err_mean + threshold * err_std)\n",
    "    row_result = pd.DataFrame({\n",
    "        \"reconstruction_error\": errors,\n",
    "        \"is_outlier\": outlier_mask\n",
    "    }, index=df.index)\n",
    "\n",
    "    # Value-wise outlier detection\n",
    "    value_errors = (X - reconstructed) ** 2\n",
    "    value_mean = np.mean(value_errors)\n",
    "    value_std = np.std(value_errors)\n",
    "    value_outlier_mask = value_errors > (value_mean + threshold * value_std)\n",
    "    # Build DataFrame of tuples\n",
    "    value_result = pd.DataFrame(\n",
    "        [\n",
    "            [(bool(value_outlier_mask[i, j]), float(value_errors[i, j]))\n",
    "             for j in range(value_errors.shape[1])]\n",
    "            for i in range(value_errors.shape[0])\n",
    "        ],\n",
    "        index=df.index,\n",
    "        columns=df.columns\n",
    "    )\n",
    "    return row_result, value_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0284c91f-af1b-42d1-bbca-33045bde53f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outlier_detection_results(results_df, value_results=None, feature=None, show=True):\n",
    "    \"\"\"\n",
    "    Plots reconstruction errors and highlights outliers.\n",
    "    If feature is None, plots sample-wise outliers.\n",
    "    If feature is set, plots value-wise outliers for that feature using the tuple DataFrame.\n",
    "    \"\"\"\n",
    "    if feature is None:\n",
    "        # Sample-wise plot (as before)\n",
    "        try:\n",
    "            results_df = results_df.copy()\n",
    "            results_df.index = pd.to_datetime(results_df.index)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        sorted_df = results_df.sort_index()\n",
    "        inliers = sorted_df[~sorted_df['is_outlier']]\n",
    "        outliers = sorted_df[sorted_df['is_outlier']]\n",
    "\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=inliers.index,\n",
    "            y=inliers['reconstruction_error'],\n",
    "            mode='markers',\n",
    "            name='Inlier',\n",
    "            marker=dict(color='blue', size=6),\n",
    "            showlegend=True\n",
    "        ))\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=outliers.index,\n",
    "            y=outliers['reconstruction_error'],\n",
    "            mode='markers',\n",
    "            name='Outlier',\n",
    "            marker=dict(color='red', size=8, symbol='x'),\n",
    "            showlegend=True\n",
    "        ))\n",
    "        fig.update_layout(\n",
    "            title='Reconstruction Error and Outlier Detection',\n",
    "            xaxis_title='Sample Index (Date)',\n",
    "            yaxis_title='Reconstruction Error',\n",
    "            width=800,\n",
    "            height=500\n",
    "        )\n",
    "    else:\n",
    "        # Value-wise plot for a specific feature using value_results\n",
    "        if value_results is None or feature not in value_results.columns:\n",
    "            raise ValueError(\"Provide value_results and a valid feature name.\")\n",
    "\n",
    "        idx = value_results.index\n",
    "        try:\n",
    "            idx = pd.to_datetime(idx)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Extract outlier mask and loss for the feature\n",
    "        outlier_mask = value_results[feature].apply(lambda x: x[0])\n",
    "        loss_values = value_results[feature].apply(lambda x: x[1])\n",
    "\n",
    "        inlier_idx = idx[~outlier_mask]\n",
    "        outlier_idx = idx[outlier_mask]\n",
    "        inlier_vals = loss_values[~outlier_mask]\n",
    "        outlier_vals = loss_values[outlier_mask]\n",
    "\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=inlier_idx,\n",
    "            y=inlier_vals,\n",
    "            mode='markers',\n",
    "            name='Inlier',\n",
    "            marker=dict(color='blue', size=6),\n",
    "            showlegend=True\n",
    "        ))\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=outlier_idx,\n",
    "            y=outlier_vals,\n",
    "            mode='markers',\n",
    "            name='Outlier',\n",
    "            marker=dict(color='red', size=8, symbol='x'),\n",
    "            showlegend=True\n",
    "        ))\n",
    "        fig.update_layout(\n",
    "            title=f'Value-wise Outlier Detection for {feature}',\n",
    "            xaxis_title='Sample Index (Date)',\n",
    "            yaxis_title=f'Reconstruction Loss ({feature})',\n",
    "            width=800,\n",
    "            height=500\n",
    "        )\n",
    "\n",
    "    if show:\n",
    "        fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3daeb5b-8a2b-422b-a186-e41a8daa25ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dim = 0\n",
    "best_results = None\n",
    "least_outliers = float(\"inf\")\n",
    "for dim in range(3, 11):\n",
    "    outliers_i, outliers_fi = detect_outliers_mlp(data_handler.input_data, dim=dim, max_iter=5000)\n",
    "    outlier_count = int(outliers_i[outliers_i[\"is_outlier\"] == True].count()[\"is_outlier\"])\n",
    "    print(f\"Dim: {dim} - Outliers: {outlier_count}\")\n",
    "    if outlier_count < least_outliers:\n",
    "        least_outliers = outlier_count\n",
    "        best_dim = dim\n",
    "        best_results = (outliers_i, outliers_fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5270b8-4f6b-4a22-835b-903bb0170b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_outlier_detection_results(best_results[0], best_results[1], feature=\"124-Trimethylbenzene\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72083bcb-457f-48de-a221-8a54f19b24a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_i, outliers_fi = detect_outliers(data_handler.input_data, dim=6, max_iter=5000)\n",
    "_ = plot_outlier_detection_results(outliers_i, outliers_fi, feature=\"Ethane\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f9c79a-534c-4ad6-b50d-b43fb54962a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_counts = outliers_fi.map(lambda x: x[0]).sum()\n",
    "print(outlier_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acb9ecf-c201-4023-b00a-a449d49e8307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
