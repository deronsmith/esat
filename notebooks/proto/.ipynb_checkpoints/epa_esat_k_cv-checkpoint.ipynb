{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5686853f-4655-4e79-af04-fe24aac1756e",
   "metadata": {},
   "source": [
    "## Factor Count Evaluation using CV\n",
    "\n",
    "This workflow investigates a single dataset factor count evaluation using cross validation on synthetic data. The workflow has the following steps:\n",
    "1. Generate synthetic dataset\n",
    "2. Create a subset of the synthetic dataset, using random sampling without replacement, for a % of the data to create a train and test dataset.\n",
    "3. Create a SA instance (base) using the train dataset for k factors.\n",
    "4. Take the base H matrix and run a new SA instance holding H constant on the test dataset (V_test).\n",
    "   1. Evaluate the loss of a direct calculation of W using V_test and H_base.\n",
    "5. Keep track of the RMSE of the test model.\n",
    "6. Repeat steps 3-5 increasing k.\n",
    "7. Evaluate/plot the change in RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ac55f-9259-4ce1-b87e-3302d2ed00f9",
   "metadata": {},
   "source": [
    "#### Code Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9de6b9e-89a5-4db8-94d4-2226605d9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from esat.data.datahandler import DataHandler\n",
    "from esat.model.batch_sa import BatchSA\n",
    "from esat.model.sa import SA\n",
    "from esat.data.analysis import ModelAnalysis, BatchAnalysis\n",
    "from esat_eval.simulator import Simulator\n",
    "from esat.estimator import FactorEstimator\n",
    "\n",
    "import logging\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb2b9c-98ee-4482-a0f9-c06b1eeb651f",
   "metadata": {},
   "source": [
    "#### Synthetic Dataset\n",
    "\n",
    "Generate a synthetic dataset where the factor profiles and contributions are pre-determined for model output analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f6ed6eb-46b4-4d3d-8085-19009c083e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synethic dataset parameters\n",
    "seed = 10\n",
    "syn_factors = 6                # Number of factors in the synthetic dataset\n",
    "syn_features = 40              # Number of features in the synthetic dataset\n",
    "syn_samples = 2000             # Number of samples in the synthetic dataset\n",
    "outliers = True                # Add outliers to the dataset\n",
    "outlier_p = 0.10               # Decimal percent of outliers in the dataset\n",
    "outlier_mag = 1.25                # Magnitude of outliers\n",
    "contribution_max = 2           # Maximum value of the contribution matrix (W) (Randomly sampled from a uniform distribution)\n",
    "noise_mean_min = 0.25          # Min value for the mean of noise added to the synthetic dataset, used to randomly determine the mean decimal percentage of the noise for each feature.\n",
    "noise_mean_max = 0.5          # Max value for the mean of noise added to the synthetic dataset, used to randomly determine the mean decimal percentage of the noise for each feature.\n",
    "noise_scale = 0.1             # Scale of the noise added to the synthetic dataset\n",
    "uncertainty_mean_min = 0.04    # Min value for the mean uncertainty of a data feature, used to randomly determine the mean decimal percentage for each feature in the uncertainty dataset. \n",
    "uncertainty_mean_max = 0.06    # Max value for the mean uncertainty of a data feature, used to randomly determine the mean decimal percentage for each feature in the uncertainty dataset. \n",
    "uncertainty_scale = 0.01       # Scale of the uncertainty matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cd06d50-6afb-4cdf-a20c-3a487b8a7a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24-Apr-25 15:55:07 - Synthetic profiles generated\n"
     ]
    }
   ],
   "source": [
    "# Initialize the simulator with the above parameters\n",
    "simulator = Simulator(seed=seed,\n",
    "                      factors_n=syn_factors,\n",
    "                      features_n=syn_features,\n",
    "                      samples_n=syn_samples,\n",
    "                      outliers=outliers,\n",
    "                      outlier_p=outlier_p,\n",
    "                      outlier_mag=outlier_mag,\n",
    "                      contribution_max=contribution_max,\n",
    "                      noise_mean_min=noise_mean_min,\n",
    "                      noise_mean_max=noise_mean_max,\n",
    "                      noise_scale=noise_scale,\n",
    "                      uncertainty_mean_min=uncertainty_mean_min,\n",
    "                      uncertainty_mean_max=uncertainty_mean_max,\n",
    "                      uncertainty_scale=uncertainty_scale\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68bb55be-07a1-44a3-98a0-d91855b9d81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example command for passing in a custom factor profile matrix, instead of the randomly generated profile matrix.\n",
    "# my_profile = np.ones(shape=(syn_factors, syn_features))\n",
    "# simulator.generate_profiles(profiles=my_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b5983ad-0d02-4938-9943-31bf0f889414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to customize the factor contributions. Curve_type options: 'uniform', 'decreasing', 'increasing', 'logistic', 'periodic'\n",
    "# simulator.update_contribution(factor_i=0, curve_type=\"logistic\", scale=0.1, frequency=0.5)\n",
    "# simulator.update_contribution(factor_i=1, curve_type=\"periodic\", minimum=0.0, maximum=1.0, frequency=0.5, scale=0.1)\n",
    "# simulator.update_contribution(factor_i=2, curve_type=\"increasing\", minimum=0.0, maximum=1.0, scale=0.1)\n",
    "# simulator.update_contribution(factor_i=3, curve_type=\"decreasing\", minimum=0.0, maximum=1.0, scale=0.1)\n",
    "# simulator.plot_synthetic_contributions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d605314-c34a-434d-a178-81b6f6b3766d",
   "metadata": {},
   "source": [
    "#### Load Data\n",
    "Assign the processed data and uncertainty datasets to the variables V and U. These steps will be simplified/streamlined in a future version of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ee36d66-7066-44d1-ae0b-b8b3e2413bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24-Apr-25 15:55:07 - Synthetic data generated\n",
      "24-Apr-25 15:55:07 - Synthetic uncertainty data generated\n",
      "24-Apr-25 15:55:07 - Synthetic dataframes completed\n",
      "24-Apr-25 15:55:07 - Synthetic source apportionment instance created.\n"
     ]
    }
   ],
   "source": [
    "syn_input_df, syn_uncertainty_df = simulator.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a66e8025-4430-4b57-b908-f29e959e480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = DataHandler.load_dataframe(input_df=syn_input_df, uncertainty_df=syn_uncertainty_df)\n",
    "V, U = data_handler.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "300f658b-ccc2-42f0-b74b-9249967c0fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24-Apr-25 15:55:07 - Input and output configured successfully\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "data_dir = os.path.join(cwd, \"..\", \"data\")\n",
    "\n",
    "# # Baton Rouge Dataset\n",
    "br_input_file = os.path.join(data_dir, \"Dataset-BatonRouge-con.csv\")\n",
    "br_uncertainty_file = os.path.join(data_dir, \"Dataset-BatonRouge-unc.csv\")\n",
    "# # Baltimore Dataset\n",
    "b_input_file = os.path.join(data_dir, \"Dataset-Baltimore_con.txt\")\n",
    "b_uncertainty_file = os.path.join(data_dir, \"Dataset-Baltimore_unc.txt\")\n",
    "# # Saint Louis Dataset\n",
    "sl_input_file = os.path.join(data_dir, \"Dataset-StLouis-con.csv\")\n",
    "sl_uncertainty_file = os.path.join(data_dir, \"Dataset-StLouis-unc.csv\")\n",
    "\n",
    "data_handler2 = DataHandler(\n",
    "    input_path=br_input_file,\n",
    "    uncertainty_path=br_uncertainty_file,\n",
    "    index_col=\"Date\"\n",
    ")\n",
    "V, U = data_handler2.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f532c5-1e71-4966-8edf-e8f68fa0bd57",
   "metadata": {},
   "source": [
    "#### Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af6db49d-8c25-467e-8456-0b53bb7aba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_col = \"Date\"                  # the index of the input/uncertainty datasets\n",
    "method = \"ls-nmf\"                   # \"ls-nmf\", \"ws-nmf\"\n",
    "models = 20                         # the number of models to train\n",
    "init_method = \"col_means\"           # default is column means \"col_means\", \"kmeans\", \"cmeans\"\n",
    "init_norm = True                    # if init_method=kmeans or cmeans, normalize the data prior to clustering.\n",
    "seed = 42                           # random seed for initialization\n",
    "max_iterations = 20000              # the maximum number of iterations for fitting a model\n",
    "converge_delta = 0.1                # convergence criteria for the change in loss, Q\n",
    "converge_n = 25                     # convergence criteria for the number of steps where the loss changes by less than converge_delta\n",
    "verbose = True                      # adds more verbosity to the algorithm workflow on execution.\n",
    "\n",
    "rng = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d60233-99a4-42de-9e94-6ba1ac84afce",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31cb958a-0795-43e5-b645-b2c4eb024a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_W(V, U, H):\n",
    "    H[H <= 0.0] = 1e-8\n",
    "    # W = np.matmul(V * np.divide(1, U ** 2), H.T)\n",
    "    W = np.matmul(V, H.T)\n",
    "    return W\n",
    "\n",
    "def q_loss(V, U, H, W):\n",
    "    residuals = ((V-np.matmul(W, H))/U)**2\n",
    "    return np.sum(residuals)\n",
    "\n",
    "def rmse(_V, _U, _H, _W, use_uncertainty: bool = False):\n",
    "    WH = np.matmul(_W, _H)\n",
    "    if use_uncertainty: \n",
    "        residuals = ((_V-WH)/_U)**2\n",
    "    else:\n",
    "        residuals = (_V-WH)**2\n",
    "    return np.sqrt(np.sum(residuals)/_V.size)\n",
    "\n",
    "def prepare_data(V, U, p, seed):\n",
    "    rng0 = np.random.default_rng(seed)\n",
    "    \n",
    "    samples_n = V.shape[0]\n",
    "    sample_index = rng0.permutation(samples_n)\n",
    "    \n",
    "    train_n = int(samples_n * p)\n",
    "    train_index = sample_index[0:train_n]\n",
    "    test_index = sample_index[train_n:]\n",
    "    \n",
    "    train_V = pd.DataFrame(V.copy()[train_index,:])\n",
    "    train_U = pd.DataFrame(U.copy()[train_index,:])\n",
    "    test_V = pd.DataFrame(V.copy()[test_index,:])\n",
    "    test_U = pd.DataFrame(U.copy()[test_index,:])\n",
    "    \n",
    "    for f in train_V.columns:\n",
    "        train_V[f] = pd.to_numeric(train_V[f])\n",
    "        train_U[f] = pd.to_numeric(train_U[f])\n",
    "        test_V[f] = pd.to_numeric(test_V[f])\n",
    "        test_U[f] = pd.to_numeric(test_U[f])\n",
    "    return train_V.to_numpy(), train_U.to_numpy(), test_V.to_numpy(), test_U.to_numpy()\n",
    "            \n",
    "def plot_results(train_loss, test_loss, min_k, max_k, base_loss=None, true_k=None):\n",
    "    x = np.arange(min_k, max_k+1)\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=x, y=train_loss, name=\"Train\"))\n",
    "    fig.add_trace(go.Scatter(x=x, y=test_loss, name=\"Test\"))\n",
    "    if true_k:\n",
    "        fig.add_vline(x=true_k, line_width=0.5, line_color=\"green\")\n",
    "    if base_loss:\n",
    "        fig.add_trace(go.Scatter(x=x, y=base_loss, name=\"Base\"))\n",
    "    fig.update_layout(title_text=f\"RMSE of Test data by Factor(k)\", width=800, height=600, hovermode=\"x unified\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "476336c3-0a41-4238-90b6-ac1f899f326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "# samples_n = V.shape[0]\n",
    "# p = 0.75\n",
    "\n",
    "# sample_index = rng.permutation(samples_n)\n",
    "\n",
    "# train_n = int(samples_n * p)\n",
    "# train_index = sample_index[0:train_n]\n",
    "# test_index = sample_index[train_n:]\n",
    "\n",
    "# train_V = pd.DataFrame(V.copy()[train_index,:])\n",
    "# train_U = pd.DataFrame(U.copy()[train_index,:])\n",
    "\n",
    "# for f in train_V.columns:\n",
    "#     train_V[f] = pd.to_numeric(train_V[f])\n",
    "#     train_U[f] = pd.to_numeric(train_U[f])\n",
    "# train_V = train_V.to_numpy()\n",
    "# train_U = train_U.to_numpy()\n",
    "\n",
    "# test_V = pd.DataFrame(V.copy()[test_index,:])\n",
    "# test_U = pd.DataFrame(U.copy()[test_index,:])\n",
    "\n",
    "# for f in test_V.columns:\n",
    "#     test_V[f] = pd.to_numeric(test_V[f])\n",
    "#     test_U[f] = pd.to_numeric(test_U[f])\n",
    "# test_V = test_V.to_numpy()\n",
    "# test_U = test_U.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "# _V = (V - np.min(V))/((np.max(V) - np.min(V) + 1e-8))\n",
    "# _V[_V <= 0.0] = 1e-12\n",
    "\n",
    "# VU_ratio = V/(U+1e-8)\n",
    "\n",
    "# _U = _V * VU_ratio\n",
    "# _U[_U <= 0.0] = 1e-12\n",
    "\n",
    "# print(f\"V: {V.shape}\")\n",
    "# print(f\"Number of samples - train: {train_V.shape[0]}, test: {test_V.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df71320b-dbc6-4786-9493-da3900e33b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "min_factors = 2\n",
    "max_factors = 10\n",
    "n_models = 20\n",
    "splits = 20\n",
    "max_iter = 20000\n",
    "\n",
    "test_rmse = []\n",
    "test_full_rmse = []\n",
    "train_rmse = []\n",
    "train_h_rmse = []\n",
    "base_rmse = []\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# V = _V\n",
    "# U = _U\n",
    "\n",
    "for i, k in enumerate(range(min_factors, max_factors+1)):\n",
    "    split_error0 = []\n",
    "    split_error0b = []\n",
    "    split_error = []\n",
    "    split_error2 = []\n",
    "\n",
    "    initialization_seed = rng.integers(low=0, high=1e8)\n",
    "\n",
    "    base_models = BatchSA(V=V, U=U, factors=k, models=n_models, method=method, seed=initialization_seed, max_iter=max_iter,\n",
    "                        converge_delta=converge_delta, converge_n=converge_n, verbose=False)\n",
    "    _ = base_models.train()\n",
    "    base_rmse.append(np.mean([rmse(_V=V, _U=U, _H=sa.H, _W=sa.W) for sa in base_models.results]))\n",
    " \n",
    "    for j in range(splits):       \n",
    "        train_V, train_U, test_V, test_U = prepare_data(V=V, U=U, p=0.5, seed=rng.integers(low=0, high=1e8))\n",
    "        sa_models = BatchSA(V=train_V, U=train_U, factors=k, models=n_models, method=method, seed=initialization_seed, max_iter=max_iter,\n",
    "                            converge_delta=converge_delta, converge_n=converge_n, verbose=False)\n",
    "        \n",
    "        _ = sa_models.train()\n",
    "        for sa in sa_models.results:\n",
    "            split_error0.append(rmse(_V=train_V, _U=train_U, _H=sa.H, _W=sa.W))\n",
    "        batch_H = np.array([sa.H for sa in sa_models.results])\n",
    "\n",
    "        sa_models_b = BatchSA(V=train_V, U=train_U, H=batch_H, factors=k, models=n_models, method=method, seed=initialization_seed, max_iter=max_iter,\n",
    "                            converge_delta=converge_delta, converge_n=converge_n, hold_h=True, verbose=False)\n",
    "        _ = sa_models_b.train()\n",
    "        for sa in sa_models_b.results:\n",
    "            split_error0b.append(rmse(_V=train_V, _U=train_U, _H=sa.H, _W=sa.W))\n",
    "        \n",
    "        sa_tests = BatchSA(V=test_V, U=test_U, H=batch_H, factors=k, models=n_models, method=method, seed=initialization_seed, max_iter=max_iter,\n",
    "                            converge_delta=converge_delta, converge_n=converge_n, hold_h=True, verbose=False)\n",
    "        _ = sa_tests.train()\n",
    "        for sa in sa_tests.results:\n",
    "            split_error.append(rmse(_V=test_V, _U=test_U, _H=sa.H, _W=sa.W))\n",
    "\n",
    "        sa_tests2 = BatchSA(V=test_V, U=test_U, H=batch_H, factors=k, models=n_models, method=method, seed=initialization_seed, max_iter=max_iter,\n",
    "                            converge_delta=converge_delta, converge_n=converge_n, verbose=False)\n",
    "        _ = sa_tests2.train()\n",
    "        for sa in sa_tests2.results:\n",
    "            split_error2.append(rmse(_V=test_V, _U=test_U, _H=sa.H, _W=sa.W))\n",
    "        \n",
    "    train_rmse.append(np.mean(split_error0))\n",
    "    train_h_rmse.append(np.mean(split_error0b))\n",
    "    test_rmse.append(np.mean(split_error))\n",
    "    test_full_rmse.append(np.mean(split_error2))\n",
    "    \n",
    "    logger.info(f\"Factor: {k}, Base RMSE: {base_rmse[i]:.4f}, Train RMSE: {np.mean(split_error0):.4f}, TrainB RMSE: {np.mean(split_error0b):.4f}, Test RMSE: {np.mean(split_error):.4f}, F-Test RMSE: {np.mean(split_error2):.4f}\")\n",
    "t1 = time.time()\n",
    "logger.info(f\"Runtime: {((t1-t0)/60):.2f} min(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39593478-f9a0-4858-bc3c-1c73db9a0a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(train_loss=train_rmse, test_loss=test_rmse, base_loss=base_rmse, min_k=min_factors, max_k=max_factors, true_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e90c3bf-a587-4112-bbde-e49147932ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "# k_list = np.arange(min_factors, max_factors+1)\n",
    "# k_list = k_list.reshape(len(k_list), 1)\n",
    "# k_model = LinearRegression()\n",
    "# k_model.fit(k_list,test_rmse)\n",
    "# y_pred = k_model.predict(k_list)\n",
    "\n",
    "# slope0 = k_model.coef_[0]\n",
    "# inter0 = k_model.intercept_\n",
    "# m, c, _, _ = np.linalg.lstsq(k_list, test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adcaf78-3aaa-46c2-85ca-d82fe5b85bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = go.Figure()\n",
    "# fig.add_trace(go.Scatter(x=k_list.flatten(), y=train_rmse, name=\"Train\", mode=\"markers\", marker_color=\"purple\"))\n",
    "# fig.add_trace(go.Scatter(x=k_list.flatten(), y=test_rmse, name=\"Test\", mode=\"markers\", marker_color=\"blue\"))\n",
    "# fig.add_trace(go.Scatter(x=k_list.flatten(), y=y_pred, name=\"Reg\", mode=\"lines\", marker_color=\"red\"))\n",
    "# fig.add_trace(go.Scatter(x=k_list.flatten(), y=k_list.flatten()*m, name=\"LST\", mode=\"lines\", marker_color=\"black\"))\n",
    "# fig.add_vline(x=syn_factors, line_width=0.5, line_color=\"green\")\n",
    "# fig.add_hline(y=np.mean(test_rmse), line_width=1.0, line_color=\"darkgreen\")\n",
    "# fig.update_layout(title_text=\"RMSE of Test data by Factor(k)\", width=800, height=600, hovermode=\"x unified\")\n",
    "# fig.show()\n",
    "\n",
    "# print(int(min_factors + np.argmin(np.abs(k_list.flatten()*m - y_pred))))\n",
    "# print(min_factors + np.argmin(np.abs(test_rmse - np.mean(test_rmse)))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbc7493-13e6-4c71-b781-e205585f339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(test_rmse)\n",
    "test_dif_b = np.array(test_rmse[1:]) - np.array(test_rmse[:n])\n",
    "test_dif_f = np.array(test_rmse[:n-1]) - np.array(test_rmse[1:])\n",
    "test_dif_ratio = test_dif_b/test_dif_f\n",
    "test_dif_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da882e67-9e18-4185-9d63-c5ca0c28edd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba984eff-d3ca-4286-8db3-bdac1f5c70de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
