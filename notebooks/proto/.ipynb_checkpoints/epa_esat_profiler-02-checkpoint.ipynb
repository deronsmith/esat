{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5686853f-4655-4e79-af04-fe24aac1756e",
   "metadata": {},
   "source": [
    "## Modeling Profiler\n",
    "\n",
    "Using the ESAT simulator to evaluate potential approaches to optimize modeling very large datasets.\n",
    "\n",
    "The first approach will look at implementing and validating the following workflow:\n",
    "1. Create a subset dataset of the input by randomly selecting N values from the input/uncertainty.\n",
    "2. Train a single model on that data until convergence.\n",
    "3. Use the factor profile H matrix to calculate a W for the complete dataset.\n",
    "4. Calculate Q(full)\n",
    "5. Take a new subset of the data, restart training with the prior H.\n",
    "6. Repeat until Q(full) is no longer decreasing.\n",
    "\n",
    "Run full dataset model with the same random seed and evaluate the difference in loss and factor profiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ac55f-9259-4ce1-b87e-3302d2ed00f9",
   "metadata": {},
   "source": [
    "#### Code Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9de6b9e-89a5-4db8-94d4-2226605d9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from esat.data.datahandler import DataHandler\n",
    "from esat.model.batch_sa import BatchSA\n",
    "from esat.model.sa import SA\n",
    "from esat.data.analysis import ModelAnalysis, BatchAnalysis\n",
    "from esat_eval.simulator import Simulator\n",
    "from esat.estimator import FactorEstimator\n",
    "from esat_eval.factor_catalog import FactorCatalog, Factor\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import min_weight_full_bipartite_matching\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model as kModel\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU, BatchNormalization, Dropout\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models as tf_models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError, KLDivergence\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.colors as pc\n",
    "import plotly.io as pio\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb2b9c-98ee-4482-a0f9-c06b1eeb651f",
   "metadata": {},
   "source": [
    "#### Synthetic Dataset\n",
    "\n",
    "Generate a synthetic dataset where the factor profiles and contributions are pre-determined for model output analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b796923c-4a78-441c-a3e5-f46ecaf24678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synethic dataset parameters\n",
    "seed = 3\n",
    "syn_factors = 7               # Number of factors in the synthetic dataset\n",
    "syn_features = 40              # Number of features in the synthetic dataset\n",
    "syn_samples = 10000             # Number of samples in the synthetic dataset\n",
    "outliers = True                # Add outliers to the dataset\n",
    "outlier_p = 0.2               # Decimal percent of outliers in the dataset\n",
    "outlier_mag = 3                # Magnitude of outliers\n",
    "contribution_max = 2           # Maximum value of the contribution matrix (W) (Randomly sampled from a uniform distribution)\n",
    "noise_mean_min = 0.3          # Min value for the mean of noise added to the synthetic dataset, used to randomly determine the mean decimal percentage of the noise for each feature.\n",
    "noise_mean_max = 0.4          # Max value for the mean of noise added to the synthetic dataset, used to randomly determine the mean decimal percentage of the noise for each feature.\n",
    "noise_scale = 0.12              # Scale of the noise added to the synthetic dataset\n",
    "uncertainty_mean_min = 0.04    # Min value for the mean uncertainty of a data feature, used to randomly determine the mean decimal percentage for each feature in the uncertainty dataset. \n",
    "uncertainty_mean_max = 0.07    # Max value for the mean uncertainty of a data feature, used to randomly determine the mean decimal percentage for each feature in the uncertainty dataset. \n",
    "uncertainty_scale = 0.01       # Scale of the uncertainty matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f5c330f-f0ce-4687-9427-82235d06c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cd06d50-6afb-4cdf-a20c-3a487b8a7a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-May-25 14:55:09 - Synthetic profiles generated\n"
     ]
    }
   ],
   "source": [
    "# Initialize the simulator with the above parameters\n",
    "simulator = Simulator(seed=rng.integers(low=0, high=1e10),\n",
    "                      factors_n=syn_factors,\n",
    "                      features_n=syn_features,\n",
    "                      samples_n=syn_samples,\n",
    "                      outliers=outliers,\n",
    "                      outlier_p=outlier_p,\n",
    "                      outlier_mag=outlier_mag,\n",
    "                      contribution_max=contribution_max,\n",
    "                      noise_mean_min=noise_mean_min,\n",
    "                      noise_mean_max=noise_mean_max,\n",
    "                      noise_scale=noise_scale,\n",
    "                      uncertainty_mean_min=uncertainty_mean_min,\n",
    "                      uncertainty_mean_max=uncertainty_mean_max,\n",
    "                      uncertainty_scale=uncertainty_scale\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d605314-c34a-434d-a178-81b6f6b3766d",
   "metadata": {},
   "source": [
    "#### Load Data\n",
    "Assign the processed data and uncertainty datasets to the variables V and U. These steps will be simplified/streamlined in a future version of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ee36d66-7066-44d1-ae0b-b8b3e2413bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-May-25 14:55:09 - Synthetic data generated\n",
      "21-May-25 14:55:09 - Synthetic uncertainty data generated\n",
      "21-May-25 14:55:09 - Synthetic dataframes completed\n",
      "21-May-25 14:55:09 - Synthetic source apportionment instance created.\n"
     ]
    }
   ],
   "source": [
    "syn_input_df, syn_uncertainty_df = simulator.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f44b1ec-b55d-42d2-9035-0a5534b229b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = DataHandler.load_dataframe(input_df=syn_input_df, uncertainty_df=syn_uncertainty_df)\n",
    "V, U = data_handler.get_data()\n",
    "synth_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62d03916-1ae6-46dd-9fe1-1506503d41cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-May-25 14:55:09 - Input and output configured successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418, 13)\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "data_dir = os.path.join(cwd, \"..\", \"data\")\n",
    "\n",
    "# Baton Rouge Dataset\n",
    "br_input_file = os.path.join(data_dir, \"Dataset-BatonRouge-con.csv\")\n",
    "br_uncertainty_file = os.path.join(data_dir, \"Dataset-BatonRouge-unc.csv\")\n",
    "br_output_path = os.path.join(data_dir, \"output\", \"BatonRouge\")\n",
    "# Baltimore Dataset\n",
    "b_input_file = os.path.join(data_dir, \"Dataset-Baltimore_con.txt\")\n",
    "b_uncertainty_file = os.path.join(data_dir, \"Dataset-Baltimore_unc.txt\")\n",
    "b_output_path = os.path.join(data_dir, \"output\", \"Baltimore\")\n",
    "# Saint Louis Dataset\n",
    "sl_input_file = os.path.join(data_dir, \"Dataset-StLouis-con.csv\")\n",
    "sl_uncertainty_file = os.path.join(data_dir, \"Dataset-StLouis-unc.csv\")\n",
    "sl_output_path = os.path.join(data_dir, \"output\", \"StLouis\")\n",
    "\n",
    "# data_handler = DataHandler(\n",
    "#     input_path=sl_input_file,\n",
    "#     uncertainty_path=sl_uncertainty_file,\n",
    "#     index_col=\"Date\"\n",
    "# )\n",
    "# V, U = data_handler.get_data()\n",
    "# print(f\"{V.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d60233-99a4-42de-9e94-6ba1ac84afce",
   "metadata": {},
   "source": [
    "### Train Batch Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20f6bb60-a22a-4be5-a9eb-0f5f93d61f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_W(V, U, H):\n",
    "#     H[H <= 0.0] = 1e-8\n",
    "#     W = np.matmul(V * np.divide(1, U), H.T)\n",
    "#     return W\n",
    "\n",
    "# def q_loss(V, U, H, W):\n",
    "#     residuals = (V-np.matmul(W, H))/U\n",
    "#     return np.sum(residuals)\n",
    "\n",
    "# def mse(V, U, H, W):\n",
    "#     WH = np.matmul(W, H)\n",
    "#     residuals = ((V-WH)/U)**2\n",
    "#     return np.sum(residuals)/V.size\n",
    "\n",
    "# def compare_H(H1, H2):\n",
    "#     correlation_matrix = np.zeros((H1.shape[0], H2.shape[0]))\n",
    "#     for i in range(H1.shape[0]):\n",
    "#         f1 = H1[i].astype(float)\n",
    "#         for j in range(H2.shape[0]):\n",
    "#             f2 = H2[j].astype(float)\n",
    "#             corr_matrix = np.corrcoef(f2, f1)\n",
    "#             corr = corr_matrix[0, 1]\n",
    "#             r_sq = corr ** 2\n",
    "#             correlation_matrix[i,j] = r_sq\n",
    "#     return correlation_matrix\n",
    "            \n",
    "# def plot_correlations(matrix):\n",
    "#     header = [f\"Factor {i}\" for i in range(matrix.shape[0])]\n",
    "#     fig = go.Figure(data=[go.Table(header=dict(values=header), cells=dict(values=matrix))])\n",
    "#     fig.show()\n",
    "\n",
    "def calculate_bic(X, W, H, n_params):\n",
    "    \"\"\"\n",
    "    Calculate the Bayesian Information Criterion (BIC) for a given factorization.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Original data matrix (n_samples, n_features)\n",
    "    - W: Basis matrix (n_samples, n_components)\n",
    "    - H: Coefficient matrix (n_components, n_features)\n",
    "    - n_params: Number of parameters in the model\n",
    "\n",
    "    Returns:\n",
    "    - BIC value\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    # Calculate the reconstruction error\n",
    "    X_reconstructed = np.dot(W, H)\n",
    "    mse = np.mean((X - X_reconstructed)**2)\n",
    "    # Calculate the log-likelihood\n",
    "    log_likelihood = -0.5 * n_samples * n_features * np.log(mse)\n",
    "    # Calculate BIC\n",
    "    bic = -2 * log_likelihood + n_params * np.log(n_samples * n_features)\n",
    "    return bic\n",
    "\n",
    "def prepare_data(V, U, i_selection):\n",
    "    _V = pd.DataFrame(V.copy()[i_selection,:])\n",
    "    _U = pd.DataFrame(U.copy()[i_selection,:])\n",
    "    \n",
    "    for f in _V.columns:\n",
    "        _V[f] = pd.to_numeric(_V[f])\n",
    "        _U[f] = pd.to_numeric(_U[f])\n",
    "    return _V.to_numpy(), _U.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c5e3ec-9c15-4b07-9a21-7fe2b8248fff",
   "metadata": {},
   "source": [
    "### Version 2 of the FactorCatalog\n",
    "\n",
    "FactorCatalog V2 takes a more robust approach to grouping factor profiles from multiple models with the grouping occuring after all factor profiles have been collected. The updated procedure\n",
    "is designed to be used to investigate potential solutions for creating models for very large datasets using subsets of the data. The algorithm is described as:\n",
    "1. Specify your hyper-parameters: samples_n, batches_n, models_n, random_seed, correlation_threshold, factors_k\n",
    "2. For each batch in batches_n.\n",
    "3. Create a subset dataset using samples_n randomly selected values from V/U.\n",
    "4. Created a batchSA instance of models_n, using random_seed and factors_k.\n",
    "5. Add each output factor to the FactorCatalog (factor model_i, fit_rmse, factor_i, H)\n",
    "6. Once all batches are completed, cluster the factor collection using a constrained k-means cluster function.\n",
    "7. Score the models based upon a heuristic, such as the sum of (cluster_cor_avg*cluster_members)\n",
    "8. Evaluate the clustered profile matrix, using the cluster centroid values, using the complete dataset.\n",
    "\n",
    "The primary modification of the FactorCatalog is to use the constrained k-means clustering function for grouping 'like' factor profiles. The procedure will work by:\n",
    "1. Starting with the factors_k=clusters_n, calculate the correlation of all factors/points to the centroids, by model.\n",
    "2. Initialize the clusters by randomly creating clusters or by selecting clusters_n 'dissimilar' factors.\n",
    "3. Randomly shuffle the model order for factor assignment:\n",
    "   1. Assign the factors to the clusters by order of correlation, closer to 1.0 goes first.\n",
    "   2. If more than one factor in a model would be assigned to the same cluster assign the factor with the highest cor to that cluster and then repeat excluding that cluster(s) until all factors are assigned.\n",
    "   3. If any given factor does not have a correlation above the specified threshold, create a new cluster centered at that point.\n",
    "4. At the end of each assignment iteration, remove any cluster which has no members.\n",
    "5. Once max_assignment_n iterations is reached stop, or when a reassignment doesn't change.\n",
    "\n",
    "The constrained k-means clustered is a standard clustering approach with the exception that the distance is calculated as 1/r2 of the point to the cluster centroid. Two other differences are a) the number of clusters can increase and decrease depending on correlation threshold and by the constraint that a model can only contribute one factor to any given cluster.\n",
    "\n",
    "Once all the factors for all models have been clustered, the FactorCatalog models can be scored based upon the heuristic stated in stage A7. The best model, or any selected model, factor profile matrix (H) can then be selected for final evaluation. The factor profile H is not what was produced by the model, but is the mean values of the FactorCatalog's factor (the centroid of the cluster that those factors were assigned to). This approach allows for the factor profile values to be provided as a distributed of possible values for each feature, or demonstrating potential uncertainty in the factor profile. The clustered factor profile is then used to fit the full dataset, but keeping H constant. The loss can then be evaluated against what is calculated for a long-running brute force approach. \n",
    "\n",
    "An evaluation of the impact on the model/W matrix and loss given a random selection, MC, simulation of the factor profile would be an interesting next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "695cc288-4f0c-407d-b360-172ae4c5254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the autoencoder\n",
    "def build_autoencoder(input_dim, encoding_dim):\n",
    "    # Encoder\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = Dense(128, activation='relu')(input_layer)\n",
    "    encoded = Dense(64, activation='relu')(encoded)\n",
    "    encoded = Dense(encoding_dim, activation='linear')(encoded)  # Latent space\n",
    "\n",
    "    # Decoder\n",
    "    decoded = Dense(64, activation='relu')(encoded)\n",
    "    decoded = Dense(128, activation='relu')(decoded)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "    # Autoencoder model\n",
    "    autoencoder = kModel(inputs=input_layer, outputs=decoded)\n",
    "\n",
    "    # Encoder model (for reduced representation)\n",
    "    encoder = kModel(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "    return autoencoder, encoder\n",
    "\n",
    "\n",
    "class Factor:\n",
    "    def __init__(self,\n",
    "                 factor_id,\n",
    "                 profile,\n",
    "                 model_id\n",
    "                ):\n",
    "        self.factor_id = factor_id\n",
    "        self.profile = profile\n",
    "        self.model_id = model_id\n",
    "        self.cluster_id = None\n",
    "        self.cor = None\n",
    "\n",
    "    def assign(self, cluster_id, cor):\n",
    "        self.cluster_id = cluster_id\n",
    "        self.cor = cor\n",
    "\n",
    "    def deallocate(self):\n",
    "        self.cluster_id = None\n",
    "        self.cor = None\n",
    "\n",
    "    def distance(self, cluster):\n",
    "        f1 = np.array(self.profile).astype(float)\n",
    "        f2 = np.array(cluster).astype(float)\n",
    "        corr_matrix = np.corrcoef(f2, f1)\n",
    "        corr = corr_matrix[0, 1]\n",
    "        r_sq = corr ** 2\n",
    "        return r_sq\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self,\n",
    "                 model_id):\n",
    "        self.model_id = model_id\n",
    "        self.factors = []\n",
    "\n",
    "        self.score = None\n",
    "        \n",
    "    def add_factor(self, factor):\n",
    "        self.factors.append(factor)\n",
    "\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(self,\n",
    "                 cluster_id,\n",
    "                 centroid: np.ndarray\n",
    "                ):\n",
    "        self.cluster_id = cluster_id\n",
    "        self.centroid = centroid\n",
    "        self.factors = []\n",
    "        self.count = 0\n",
    "\n",
    "        self.mean_r2 = 0\n",
    "        self.std = 0\n",
    "        self.wcss = -1\n",
    "        self.volume = 0\n",
    "        self.min_values = np.full(len(centroid), np.nan)\n",
    "        self.max_values = np.full(len(centroid), np.nan)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.count\n",
    "\n",
    "    def add(self, factor: Factor, cor: float):\n",
    "        factor.assign(cluster_id=self.cluster_id, cor=cor)\n",
    "        self.factors.append(factor)\n",
    "        self.count += 1\n",
    "        self.min_values = np.fmin(self.min_values, factor.profile)\n",
    "        self.max_values = np.fmax(self.max_values, factor.profile)\n",
    "        self.mean_r2 = np.mean([factor.cor for factor in self.factors])\n",
    "        self.std = np.std([factor.profile for factor in self.factors], axis=0)\n",
    "        self.wcss = np.sum([np.square(self.centroid - factor.profile) for factor in self.factors])\n",
    "        self.volume = np.prod(self.max_values - self.min_values)\n",
    "\n",
    "    def purge(self):\n",
    "        for factor in self.factors: factor.deallocate()\n",
    "        self.factors = []\n",
    "        self.count = 0\n",
    "        self.mean_r2 = 0\n",
    "        self.std = 0\n",
    "        self.wcss = -1\n",
    "        self.volume = 0\n",
    "        self.min_values = self.centroid\n",
    "        self.max_values = self.centroid\n",
    "\n",
    "    def recalculate(self):\n",
    "        if len(self.factors) > 0:\n",
    "            factor_matrix = np.array([factor.profile for factor in self.factors])\n",
    "            new_centroid = np.mean(factor_matrix, axis=0)\n",
    "            self.centroid = new_centroid\n",
    "\n",
    "    def plot(self):\n",
    "        n_features = len(self.centroid)\n",
    "        factor_matrix = np.array([factor.profile for factor in self.factors])\n",
    "        \n",
    "        box_plot = go.Figure()\n",
    "        for i in range(n_features):\n",
    "            box_plot.add_trace(go.Box(\n",
    "                y=factor_matrix[:,i], \n",
    "                boxpoints=\"all\", \n",
    "                jitter=0.5,\n",
    "                whiskerwidth=0.2,\n",
    "                fillcolor=cls,\n",
    "                marker_size=2,\n",
    "                line_width=1, \n",
    "                name=f\"Feature {i+1}\")\n",
    "            )\n",
    "        box_plot.add_trace(go.Scatter(\n",
    "            x=np.range(n_features), \n",
    "            y=self.centroid, \n",
    "            name=\"Centroid\",\n",
    "            mode='markers',\n",
    "            marker=dict(color='red', size=10)\n",
    "        ))\n",
    "        box_plot.update_layout(title=\"Clustered Factor Profile\", width=1200, height=800)\n",
    "        box_plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d56416c-f9e1-4db7-87cb-51c70813e2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchFactorCatalog:\n",
    "    def __init__(self,\n",
    "                 n_factors: int,\n",
    "                 n_features: int,\n",
    "                 threshold: float = 0.8,\n",
    "                 seed: int = 42\n",
    "                ):\n",
    "        self.n_factors = n_factors\n",
    "        self.n_features = n_features\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        self.models = {}\n",
    "        self.model_count = 0\n",
    "        self.factors = {}\n",
    "        self.factor_count = 0\n",
    "\n",
    "        # Min and max values for all factor vectors, used for random initialization of the centroids in clustering\n",
    "        self.factor_min = None\n",
    "        self.factor_max = None\n",
    "\n",
    "        self.clusters = {}\n",
    "        self.dropped_clusters = []\n",
    "        self.max_clusters_found = 0\n",
    "\n",
    "        self.bcss = float(\"inf\")\n",
    "        self.sil = float(\"-inf\")\n",
    "        self.membership_p = 0.05\n",
    "        self.primary_factors = []\n",
    "        self.primary_clusters = []\n",
    "\n",
    "        self.state = {}\n",
    "\n",
    "    def results(self):\n",
    "        results = {}\n",
    "        for c, cluster in self.clusters.items():\n",
    "            results[cluster.cluster_id] = {\n",
    "                \"count\": len(cluster),\n",
    "                \"mean_r2\": cluster.mean_r2,\n",
    "                \"std\": cluster.std\n",
    "            }\n",
    "        return results\n",
    "\n",
    "    def metrics(self, membership_p: float=None):\n",
    "        if membership_p is None:\n",
    "            membership_p = self.membership_p\n",
    "        else:\n",
    "            self.membership_p = membership_p\n",
    "        all_factors = np.array([v.profile for k, v in self.factors.items()])\n",
    "        model_assignment = np.array([v.model_id for k, v in self.factors.items()])\n",
    "        factor_assignments = np.array([v.cluster_id for k, v in self.factors.items()])\n",
    "        cluster_centroids = [(c, cluster.centroid) for c, cluster in self.clusters.items()]\n",
    "        \n",
    "        i_cluster, i_centroids = zip(*cluster_centroids)\n",
    "        i_length = len(i_cluster)\n",
    "        i_centroids = np.array(i_centroids)\n",
    "        \n",
    "        df_pca0 = pd.DataFrame(all_factors)\n",
    "        factor_columns = df_pca0.columns\n",
    "        df_pca0[\"Cluster\"] = factor_assignments\n",
    "        \n",
    "        df_centroids0 = pd.DataFrame(i_centroids, index=list(i_cluster))\n",
    "        cluster_columns = df_centroids0.columns\n",
    "        assigned_centroids, cluster_size = np.unique(factor_assignments, return_counts=True)\n",
    "        df_centroids0[\"Cluster\"] = list(i_cluster)\n",
    "        df_centroids0= df_centroids0.loc[assigned_centroids]\n",
    "        df_centroids0[\"count\"] = cluster_size\n",
    "                \n",
    "        point_cluster_n = []\n",
    "        for i in range(len(all_factors)):\n",
    "            i_cluster_count = df_centroids0[df_centroids0[\"Cluster\"] == df_pca0[\"Cluster\"].iloc[i]][\"count\"].values\n",
    "            point_cluster_n.append(i_cluster_count)\n",
    "        cluster_n_threshold = int(len(all_factors) * membership_p) \n",
    "        df_pca0[\"cluster_n\"] = point_cluster_n\n",
    "        df_pca0[\"cluster_n\"] = df_pca0[\"cluster_n\"].astype(int)\n",
    "        df_pca0 = df_pca0[df_pca0[\"cluster_n\"] > cluster_n_threshold]\n",
    "        \n",
    "        df_centroids0 = df_centroids0[df_centroids0[\"count\"] > cluster_n_threshold]\n",
    "        \n",
    "        all_factors = df_pca0[factor_columns].values\n",
    "        factor_assignments =  df_pca0[\"Cluster\"].values\n",
    "        i_centroids = df_centroids0[cluster_columns].values\n",
    "        # Calculate Silhouette Score\n",
    "        self.primary_factors = list(all_factors)\n",
    "        self.primary_clusters = list(set(factor_assignments))\n",
    "        if len(all_factors) > len(self.primary_clusters) > 1:\n",
    "            self.sil = silhouette_score(all_factors, factor_assignments)\n",
    "\n",
    "        # Calculate between-cluster sum of squares\n",
    "        cluster_centroids = [(len(cluster), cluster.centroid) for c_id, cluster in self.clusters.items() if c_id in df_centroids0[\"Cluster\"]]\n",
    "        overall_mean = np.mean(all_factors, axis=0)\n",
    "        bcss = 0.0\n",
    "        for c_count, c_centroid in cluster_centroids:\n",
    "            bcss += c_count * np.sum((c_centroid - overall_mean)**2)\n",
    "        self.bcss = bcss            \n",
    "\n",
    "    def plot(self, base_matrix = None, count_threshold: int = 3, method=\"mds\", membership_p: float = 0.0):        \n",
    "        all_factors = np.array([v.profile for k, v in self.factors.items()])\n",
    "        model_assignment = np.array([v.model_id for k, v in self.factors.items()])\n",
    "        factor_assignments = np.array([v.cluster_id for k, v in self.factors.items()])\n",
    "        cluster_centroids = [(c, cluster.centroid) for c, cluster in self.clusters.items()]\n",
    "        \n",
    "        i_cluster, i_centroids = zip(*cluster_centroids)\n",
    "        i_length = len(i_cluster)\n",
    "        i_centroids = np.array(i_centroids)\n",
    "        \n",
    "        df_pca0 = pd.DataFrame(all_factors)\n",
    "        factor_columns = df_pca0.columns\n",
    "        df_pca0[\"Cluster\"] = factor_assignments\n",
    "        df_pca0[\"text\"] = \"Profile \" + df_pca0[\"Cluster\"].astype(str) + \" Model: \" + model_assignment.astype(str)\n",
    "        \n",
    "        df_centroids0 = pd.DataFrame(i_centroids, index=list(i_cluster))\n",
    "        cluster_columns = df_centroids0.columns\n",
    "        assigned_centroids, cluster_size = np.unique(factor_assignments, return_counts=True)\n",
    "        df_centroids0[\"Cluster\"] = list(i_cluster)\n",
    "        df_centroids0= df_centroids0.loc[assigned_centroids]\n",
    "        df_centroids0[\"count\"] = cluster_size\n",
    "        \n",
    "        if membership_p > 0.0:\n",
    "            point_cluster_n = []\n",
    "            for i in range(len(all_factors)):\n",
    "                i_cluster_count = df_centroids0[df_centroids0[\"Cluster\"] == df_pca0[\"Cluster\"].iloc[i]][\"count\"].values\n",
    "                point_cluster_n.append(i_cluster_count)\n",
    "            cluster_n_threshold = int(len(all_factors) * membership_p) \n",
    "            df_pca0[\"cluster_n\"] = point_cluster_n\n",
    "            df_pca0[\"cluster_n\"] = df_pca0[\"cluster_n\"].astype(int)\n",
    "            df_pca0 = df_pca0[df_pca0[\"cluster_n\"] > cluster_n_threshold]\n",
    "            \n",
    "            df_centroids0 = df_centroids0[df_centroids0[\"count\"] > cluster_n_threshold]\n",
    "        elif count_threshold > 0:\n",
    "            df_centroids0 = df_centroids0[df_centroids0[\"count\"] > count_threshold]\n",
    "        \n",
    "        all_factors = df_pca0[factor_columns].values\n",
    "        i_centroids = df_centroids0[cluster_columns].values\n",
    "        \n",
    "        if method.lower() == \"tsne\":\n",
    "            samples = np.vstack((all_factors, i_centroids))\n",
    "            factor_model = TSNE(n_components=3, random_state=0, perplexity=min(50, len(samples)))\n",
    "            reduction_results = factor_model.fit_transform(samples)\n",
    "            factor_reduction = reduction_results[:len(all_factors),:]\n",
    "            pca_centroids = reduction_results[len(all_factors):,:]\n",
    "        elif method.lower() == \"mds\":\n",
    "            factor_model = MDS(n_components=3, random_state=0, metric=True, max_iter=300)\n",
    "            reduction_results = factor_model.fit_transform(np.vstack((all_factors, i_centroids)))\n",
    "            factor_reduction = reduction_results[:len(all_factors),:]\n",
    "            pca_centroids = reduction_results[len(all_factors):,:]\n",
    "        else:\n",
    "            autoencoder, encoder = build_autoencoder(input_dim=i_centroids.shape[1], encoding_dim=3)\n",
    "            autoencoder.compile(optimizer='adam', loss='mse')\n",
    "            autoencoder.fit(all_factors, all_factors, epochs=50, batch_size=16, shuffle=True, validation_split=0.2, verbose=0)\n",
    "            factor_reduction = encoder.predict(all_factors, verbose=0)\n",
    "            pca_centroids = encoder.predict(i_centroids, verbose=0)\n",
    "        \n",
    "        df_pca = pd.DataFrame(factor_reduction, columns=['PCA1', 'PCA2', 'PCA3'])\n",
    "        df_pca[\"Cluster\"] = df_pca0[\"Cluster\"].values\n",
    "        df_pca[\"text\"] = df_pca0[\"text\"].values\n",
    "        \n",
    "        df_centroids = pd.DataFrame(pca_centroids, columns=['PCA1', 'PCA2', 'PCA3'], index=list(df_centroids0.index))\n",
    "        df_centroids[\"text\"] = \"Centroid \" + df_centroids.index.astype(str)\n",
    "        df_centroids[\"Cluster\"] = df_centroids0[\"Cluster\"].values\n",
    "        df_centroids[\"count\"] = df_centroids0[\"count\"].values\n",
    "            \n",
    "        color_map = factor_catalog.generate_continuous_colormap(len(df_centroids[\"Cluster\"]), colormap_name='rainbow')\n",
    "        full_colormap = dict(zip(list(df_centroids[\"Cluster\"]), [color[1] for color in color_map]))\n",
    "        df_pca[\"color\"] = df_pca[\"Cluster\"].map(full_colormap)\n",
    "        df_centroids[\"color\"] = df_centroids[\"Cluster\"].map(full_colormap)\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=df_pca[\"PCA1\"],\n",
    "            y=df_pca[\"PCA2\"],\n",
    "            z=df_pca[\"PCA3\"],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=3,\n",
    "                color=df_pca[\"color\"],\n",
    "                opacity=0.5\n",
    "            ),\n",
    "            text=df_pca[\"text\"],\n",
    "            hoverinfo='text'\n",
    "        ))\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=df_centroids['PCA1'],\n",
    "            y=df_centroids['PCA2'],\n",
    "            z=df_centroids['PCA3'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=3,\n",
    "                color=\"black\",\n",
    "                symbol='x',\n",
    "                opacity=0.5\n",
    "            ),\n",
    "            text=df_centroids[\"text\"],\n",
    "            hoverinfo='text'\n",
    "        ))        \n",
    "        # Update layout for better visualization\n",
    "        fig.update_layout(\n",
    "            title=f\"{self.n_factors} Factor Clustering - Method: {method}\",\n",
    "            scene=dict(\n",
    "                xaxis_title=\"PCA1\",\n",
    "                yaxis_title=\"PCA2\",\n",
    "                zaxis_title=\"PCA3\"\n",
    "            ),\n",
    "            showlegend=False,\n",
    "            height=800,\n",
    "            width=1000,\n",
    "            margin=dict(l=5, r=5, b=5, t=50)\n",
    "        )\n",
    "        # Show the plot\n",
    "        fig.show()\n",
    "\n",
    "    def generate_continuous_colormap(self, n, colormap_name='viridis'):\n",
    "        # Get the colormap from matplotlib\n",
    "        cmap = plt.get_cmap(colormap_name)\n",
    "\n",
    "        # Generate colors from the colormap\n",
    "        colors = [cmap(i / (n - 1)) for i in range(n)]\n",
    "\n",
    "        # Convert RGBA to RGB and then to hex\n",
    "        colors_hex = ['#%02x%02x%02x' % (int(r * 255), int(g * 255), int(b * 255)) for r, g, b, a in colors]\n",
    "\n",
    "        # Create a Plotly color scale\n",
    "        plotly_color_scale = [(i / (n - 1), color) for i, color in enumerate(colors_hex)]\n",
    "\n",
    "        return plotly_color_scale\n",
    "\n",
    "    def animate(self, to_file: bool=False, base_matrix = None, profiled_matrix = None, count_threshold: int = 3, use_kernel: bool = False):        \n",
    "        all_factors = np.array([v.profile for k, v in self.factors.items()])\n",
    "        model_assignment = np.array([v.model_id for k, v in self.factors.items()])\n",
    "        required_members = int(len(all_factors)*membership_p)\n",
    "    \n",
    "        color_map = self.generate_continuous_colormap(len(self.clusters), colormap_name='rainbow')\n",
    "        full_colormap = dict(zip(list(self.clusters.keys()), [color[1] for color in color_map]))\n",
    "    \n",
    "        sample_lengths = []\n",
    "        samples = []\n",
    "        for i in range(len(self.state)):\n",
    "            cluster_centroids = self.state[i][\"cluster_centroids\"]\n",
    "            i_cluster, i_centroids = zip(*cluster_centroids)\n",
    "            i_centroids = np.array(i_centroids)\n",
    "            samples.append(i_centroids)\n",
    "            sample_lengths.append(len(i_centroids))\n",
    "        i_centroids = np.vstack(samples)\n",
    "    \n",
    "        sample_list = [all_factors, i_centroids]\n",
    "        samples_n = np.sum(sample_lengths) + len(all_factors)\n",
    "        plot_base = False\n",
    "        if base_matrix is not None:\n",
    "            sample_list.append(base_matrix)\n",
    "            plot_base = True\n",
    "        plot_profiled = False\n",
    "        if profiled_matrix is not None:\n",
    "            sample_list.append(profiled_matrix)\n",
    "            plot_profiled = True\n",
    "    \n",
    "        if method.lower() == \"tsne\":\n",
    "            samples = np.vstack(sample_list)\n",
    "            factor_model = TSNE(n_components=3, random_state=0, perplexity=min(50, len(samples)))\n",
    "            reduction_results = factor_model.fit_transform(samples)\n",
    "            factor_reduction = reduction_results[:len(all_factors),:]\n",
    "            cluster_centroids = reduction_results[len(all_factors):samples_n,:]\n",
    "            if plot_base:\n",
    "                base_points = reduction_results[i_max:i_max + base_matrix.shape[0],:]\n",
    "                i_max += base_matrix.shape[0]\n",
    "            if plot_profiled:\n",
    "                profiled_points = reduction_results[i_max:i_max + profiled_matrix.shape[0],:]\n",
    "        elif method.lower() == \"mds\":\n",
    "            samples = np.vstack(sample_list)\n",
    "            factor_model = MDS(n_components=3, random_state=0, metric=True, max_iter=300)\n",
    "            reduction_results = factor_model.fit_transform(samples)\n",
    "            factor_reduction = reduction_results[:len(all_factors),:]\n",
    "            cluster_centroids = reduction_results[len(all_factors):samples_n,:]\n",
    "            i_max = samples_n\n",
    "            if plot_base:\n",
    "                base_points = reduction_results[i_max:i_max + base_matrix.shape[0],:]\n",
    "                i_max += base_matrix.shape[0]\n",
    "            if plot_profiled:\n",
    "                profiled_points = reduction_results[i_max:i_max + profiled_matrix.shape[0],:]\n",
    "        else:\n",
    "            autoencoder, encoder = build_autoencoder(input_dim=i_centroids.shape[1], encoding_dim=3)\n",
    "            autoencoder.compile(optimizer='adam', loss='mse')\n",
    "            autoencoder.fit(all_factors, all_factors, epochs=50, batch_size=16, shuffle=True, validation_split=0.2, verbose=0)\n",
    "            factor_reduction = encoder.predict(all_factors, verbose=0)\n",
    "            cluster_centroids = encoder.predict(i_centroids, verbose=0)\n",
    "            if plot_base:\n",
    "                base_points = encoder.predict(base_matrix, verbose=0)\n",
    "            if plot_profiled:\n",
    "                profiled_points = encoder.predict(profiled_matrix, verbose=0)\n",
    "            \n",
    "        cumulative_lengths = np.cumsum([0] + sample_lengths)\n",
    "        state_centroids = [cluster_centroids[cumulative_lengths[i]: cumulative_lengths[i+1]] for i in range(len(sample_lengths))]\n",
    "    \n",
    "        if base_matrix is not None:\n",
    "            df_base = pd.DataFrame(base_points, columns=['x', 'y', 'z'])\n",
    "    \n",
    "        if profiled_matrix is not None:\n",
    "            df_profiled = pd.DataFrame(profiled_points, columns=['x', 'y', 'z'])\n",
    "    \n",
    "        df_pca0 = pd.DataFrame(factor_reduction, columns=['x', 'y', 'z'])\n",
    "        df_pca0[\"text\"] = \"P: \" + df_pca0.index.astype(str) + \", M: \" + model_assignment.astype(str)\n",
    "        \n",
    "        frames = []\n",
    "        for i in range(len(self.state)):\n",
    "            cluster_centroids = self.state[i][\"cluster_centroids\"]\n",
    "            i_cluster, i_centroids = zip(*cluster_centroids)\n",
    "            factor_assignments = factor_catalog.state[i][\"assignment\"]\n",
    "            i_centroids = state_centroids[i]\n",
    "                \n",
    "            i_length = len(i_cluster)\n",
    "            i_centroids = np.array(i_centroids)\n",
    "            \n",
    "            df_pca = pd.DataFrame(factor_reduction, columns=['x', 'y', 'z'])\n",
    "            factor_columns = df_pca.columns\n",
    "            df_pca[\"Cluster\"] = factor_assignments\n",
    "            df_pca[\"text\"] = \"P: \" + df_pca0.index.astype(str) + \", M: \" + model_assignment.astype(str) + \", C:\" + df_pca[\"Cluster\"].astype(str)\n",
    "            \n",
    "            df_centroids = pd.DataFrame(i_centroids, columns=['x', 'y', 'z'], index=list(i_cluster))\n",
    "            cluster_columns = df_centroids.columns\n",
    "            assigned_centroids, cluster_size = np.unique(factor_assignments, return_counts=True)\n",
    "            df_centroids[\"text\"] = \"Centroid \" + df_centroids.index.astype(str)\n",
    "            df_centroids[\"Cluster\"] = list(i_cluster)\n",
    "            df_centroids = df_centroids.loc[assigned_centroids]\n",
    "            df_centroids[\"count\"] = cluster_size\n",
    "            df_centroids = df_centroids[df_centroids[\"count\"] > required_members]\n",
    "                \n",
    "            df_pca[\"color\"] = df_pca[\"Cluster\"].map(full_colormap)\n",
    "            df_centroids[\"color\"] = df_centroids[\"Cluster\"].map(full_colormap)\n",
    "    \n",
    "            data = [\n",
    "                go.Scatter3d(\n",
    "                    x=df_pca[\"x\"],\n",
    "                    y=df_pca[\"y\"],\n",
    "                    z=df_pca[\"z\"],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=3,\n",
    "                        color=df_pca[\"color\"],\n",
    "                        opacity=0.4\n",
    "                    ),\n",
    "                    text=df_pca[\"text\"],\n",
    "                    hoverinfo='text'\n",
    "                ),\n",
    "                go.Scatter3d(\n",
    "                    x=df_centroids['x'],\n",
    "                    y=df_centroids['y'],\n",
    "                    z=df_centroids['z'],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=2,\n",
    "                        color=\"black\",\n",
    "                        symbol='x',\n",
    "                        opacity=0.75\n",
    "                    ),\n",
    "                    text=df_centroids[\"text\"],\n",
    "                    hoverinfo='text'\n",
    "                )\n",
    "            ]\n",
    "            if plot_base:\n",
    "                data.append(go.Scatter3d(\n",
    "                    x=df_base['x'],\n",
    "                    y=df_base['y'],\n",
    "                    z=df_base['z'],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=4,\n",
    "                        color=\"black\",\n",
    "                        symbol='cross'\n",
    "                    ),\n",
    "                    name=\"Base Factor\"\n",
    "                ))\n",
    "            if plot_profiled:\n",
    "                data.append(go.Scatter3d(\n",
    "                    x=df_profiled['x'],\n",
    "                    y=df_profiled['y'],\n",
    "                    z=df_profiled['z'],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=4,\n",
    "                        color=\"green\",\n",
    "                        symbol='cross'\n",
    "                    ),\n",
    "                    name=\"P Factor\"\n",
    "                ))\n",
    "            frames.append(\n",
    "                go.Frame(data=data, \n",
    "                         layout=go.Layout(\n",
    "                             annotations=[\n",
    "                                dict(\n",
    "                                    x=1,\n",
    "                                    y=1,\n",
    "                                    showarrow=False,\n",
    "                                    text=f\"Iteration: {i + 1}/{len(self.state)}\",\n",
    "                                    xref=\"paper\",\n",
    "                                    yref=\"paper\",\n",
    "                                    font=dict(size=14)\n",
    "                                )\n",
    "                            ]\n",
    "                        ),\n",
    "                         name=str(i)\n",
    "                        )\n",
    "            )\n",
    "            if i == 0:\n",
    "                state0 = [\n",
    "                    go.Scatter3d(\n",
    "                        x=df_pca0[\"x\"],\n",
    "                        y=df_pca0[\"y\"],\n",
    "                        z=df_pca0[\"z\"],\n",
    "                        mode='markers',\n",
    "                        marker=dict(\n",
    "                            size=3,\n",
    "                            color=\"gray\",\n",
    "                            opacity=0.4\n",
    "                        ),\n",
    "                        text=df_pca0[\"text\"],\n",
    "                        hoverinfo='text'\n",
    "                    ),\n",
    "                    go.Scatter3d(\n",
    "                        x=df_centroids['x'],\n",
    "                        y=df_centroids['y'],\n",
    "                        z=df_centroids['z'],\n",
    "                        mode='markers',\n",
    "                        marker=dict(\n",
    "                            size=2,\n",
    "                            color=\"black\",\n",
    "                            symbol='x',\n",
    "                            opacity=0.75\n",
    "                        ),\n",
    "                        text=df_centroids[\"text\"],\n",
    "                        hoverinfo='text'\n",
    "                    )\n",
    "                ]\n",
    "                if plot_base:\n",
    "                    state0.append(go.Scatter3d(\n",
    "                        x=df_base['x'],\n",
    "                        y=df_base['y'],\n",
    "                        z=df_base['z'],\n",
    "                        mode='markers',\n",
    "                        marker=dict(\n",
    "                            size=4,\n",
    "                            color=\"black\",\n",
    "                            symbol='cross'\n",
    "                        ),\n",
    "                        name=\"Base Factor\"\n",
    "                    ))\n",
    "                if plot_profiled:\n",
    "                    state0.append(go.Scatter3d(\n",
    "                        x=df_profiled['x'],\n",
    "                        y=df_profiled['y'],\n",
    "                        z=df_profiled['z'],\n",
    "                        mode='markers',\n",
    "                        marker=dict(\n",
    "                            size=4,\n",
    "                            color=\"green\",\n",
    "                            symbol='cross'\n",
    "                        ),\n",
    "                        name=\"P Factor\"\n",
    "                    ))\n",
    "        df_pca0 = pd.DataFrame(factor_reduction, columns=['x', 'y', 'z'])\n",
    "        min_values = [[df_pca0['x'].min()],[df_pca0['y'].min()],[df_pca0['z'].min()]]\n",
    "        max_values = [[df_pca0['x'].max()],[df_pca0['y'].max()],[df_pca0['z'].max()]]\n",
    "        \n",
    "        fig = go.Figure(\n",
    "            data=state0,\n",
    "            layout=go.Layout(\n",
    "                title=\"Factor Profile Clustering\",\n",
    "                height=1000,\n",
    "                width=1000,\n",
    "                # scene=dict(\n",
    "                #     xaxis=dict(range=[min(min_values[0])-1.25, max(max_values[0])+1.25], autorange=False),\n",
    "                #     yaxis=dict(range=[min(min_values[1])-1.25, max(max_values[1])+1.25], autorange=False),\n",
    "                #     zaxis=dict(range=[min(min_values[2])-1.25, max(max_values[2])+1.25], autorange=False),\n",
    "                #     aspectmode=\"manual\",\n",
    "                #     aspectratio=dict(x=1, y=1, z=1)\n",
    "                # ),\n",
    "                updatemenus=[dict(\n",
    "                    type=\"buttons\",\n",
    "                    showactive=False,\n",
    "                    buttons=[\n",
    "                        dict(label=\"Play\",\n",
    "                             method=\"animate\",\n",
    "                             args=[None, dict(frame=dict(duration=500, redraw=True), fromcurrent=True, mode=\"immediate\")]),\n",
    "                        dict(label=\"Pause\",\n",
    "                             method=\"animate\",\n",
    "                             args=[[None], dict(frame=dict(duration=0, redraw=False), mode=\"immediate\")]),\n",
    "                        dict(\n",
    "                            args=[[0], dict(frame=dict(duration=0, redraw=True), mode=\"immediate\")],\n",
    "                            label=\"Reset\",\n",
    "                            method=\"animate\"),\n",
    "                    ]\n",
    "                )],\n",
    "                annotations=[\n",
    "                    dict(\n",
    "                        x=1,\n",
    "                        y=1,\n",
    "                        showarrow=False,\n",
    "                        text=\"Iteration: NA\",\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\",\n",
    "                        font=dict(size=14)\n",
    "                    )\n",
    "                ],\n",
    "                showlegend=False\n",
    "            ),\n",
    "            frames=frames\n",
    "        )\n",
    "        if to_file:\n",
    "            pio.write_html(fig, file=\"factor_clustering.html\", auto_open=False)\n",
    "        else:\n",
    "            fig.show()\n",
    "        \n",
    "    def add_model(self, model: SA, norm: bool = True):\n",
    "        model_id = self.model_count\n",
    "        model_factor_ids = []\n",
    "        norm_H = model.H / np.sum(model.H, axis=0)\n",
    "        i_model = Model(model_id=model_id)\n",
    "        for i in range(model.H.shape[0]):\n",
    "            factor_id = self.factor_count\n",
    "            self.factor_count += 1\n",
    "            model_factor_ids.append(factor_id)\n",
    "            i_H = norm_H if norm else model.H \n",
    "            factor = Factor(factor_id=factor_id, profile=i_H[i], model_id=model_id)\n",
    "            \n",
    "            i_model.add_factor(factor)\n",
    "            self.factors[factor_id] = factor\n",
    "            self.update_ranges(i_H[i])\n",
    "            \n",
    "        self.models[str(model_id)] = i_model\n",
    "        self.model_count += 1\n",
    "\n",
    "    def compare(self, matrix):\n",
    "        compare_results = {}\n",
    "        for i in range(matrix.shape[0]):\n",
    "            i_H = matrix[i]\n",
    "            i_cor = 0.0\n",
    "            best_cluster = None\n",
    "            for c, cluster in self.clusters.items():\n",
    "                cluster_cor = self.distance(i_H, cluster.centroid)\n",
    "                if cluster_cor > i_cor:\n",
    "                    i_cor = cluster_cor\n",
    "                    best_cluster = cluster.cluster_id\n",
    "            compare_results[i] = {\"cluster_id\": best_cluster, \"r2\": i_cor}\n",
    "        return compare_results\n",
    "\n",
    "    def score(self):\n",
    "        # iterate over all models, get the membership count the cluster that each factor is mapped to.\n",
    "        for model_id, model in self.models.items():\n",
    "            model_score = 0.0\n",
    "            for factor in model.factors:\n",
    "                if factor.cluster_id not in self.clusters.keys():\n",
    "                    logger.info(f\"Factor {factor.factor_id} assigned to non-existent cluster {factor.cluster_id}\")\n",
    "                    factor_score = 0\n",
    "                    # factor.cluster_id = -1\n",
    "                else:\n",
    "                    factor_score = len(self.clusters[factor.cluster_id])\n",
    "                model_score += factor_score\n",
    "            model.score = model_score\n",
    "        # self.metrics()\n",
    "\n",
    "    def update_ranges(self, factor):\n",
    "        if self.factor_min is None and self.factor_max is None:\n",
    "            self.factor_min = copy.copy(factor)\n",
    "            self.factor_max = copy.copy(factor)\n",
    "        else:\n",
    "            self.factor_min = np.minimum(self.factor_min, factor)\n",
    "            self.factor_max = np.maximum(self.factor_max, factor)\n",
    "\n",
    "    def initialize_clusters(self):\n",
    "        for k in range(self.n_factors):\n",
    "            new_centroid = np.zeros(self.n_features)\n",
    "            for i in range(self.n_features):\n",
    "                i_v = self.rng.uniform(low=self.factor_min[i], high=self.factor_max[i])\n",
    "                new_centroid[i] = i_v\n",
    "            cluster = Cluster(cluster_id=k, centroid=new_centroid)\n",
    "            self.clusters[k] = cluster\n",
    "\n",
    "    def purge_clusters(self):\n",
    "        for c, cluster in self.clusters.items():\n",
    "            cluster.purge()\n",
    "\n",
    "    def distance(self, factor1, factor2):\n",
    "        f1 = np.array(factor1).astype(float)\n",
    "        f2 = np.array(factor2).astype(float)\n",
    "        corr_matrix = np.corrcoef(f2, f1)\n",
    "        corr = corr_matrix[0, 1]\n",
    "        r_sq = corr ** 2\n",
    "        return r_sq\n",
    "\n",
    "    def calculate_centroids(self):\n",
    "        new_centroid_matrix = []\n",
    "        for c, cluster in self.clusters.items(): \n",
    "            cluster.recalculate()\n",
    "            new_centroid_matrix.append(cluster.centroid)\n",
    "        return np.array(new_centroid_matrix)\n",
    "\n",
    "    def cluster_cleanup(self):\n",
    "        drop_clusters = set()\n",
    "        cluster_keys = list(self.clusters.keys())\n",
    "        for i, i_key in enumerate(cluster_keys[:len(cluster_keys)-1]):\n",
    "            cluster_i = self.clusters[i_key]\n",
    "            for j, j_key in enumerate(cluster_keys[i+1:]):\n",
    "                if j_key == i_key:\n",
    "                    continue\n",
    "                cluster_j = self.clusters[j_key]\n",
    "                ij_cor = self.distance(cluster_i.centroid, cluster_j.centroid)\n",
    "                if ij_cor > self.threshold:\n",
    "                    smaller_cluster = i_key if len(cluster_i) < len(cluster_j) else j_key\n",
    "                    if smaller_cluster not in drop_clusters:\n",
    "                        drop_clusters.add(smaller_cluster)\n",
    "        for i_key, cluster in self.clusters.items():\n",
    "            if len(cluster) == 0:\n",
    "                drop_clusters.add(i_key)\n",
    "        # new_clusters = {}\n",
    "        new_centroid_matrix = []\n",
    "        for i, cluster in self.clusters.items():\n",
    "            new_centroid_matrix.append(cluster.centroid)\n",
    "        #     cluster = self.clusters[i]\n",
    "        #     if i not in drop_clusters:\n",
    "        #         new_clusters[i] = cluster\n",
    "        #         new_centroid_matrix.append(cluster.centroid)\n",
    "        #     else:\n",
    "        #         self.dropped_clusters.append(i)\n",
    "        for cluster in drop_clusters:\n",
    "            self.clusters[cluster].purge()\n",
    "        # self.clusters = new_clusters\n",
    "        return np.array(new_centroid_matrix)\n",
    "\n",
    "    def save_state(self, iteration):\n",
    "        factor_assignment = np.array([v.cluster_id for k, v in factor_catalog.factors.items()])\n",
    "        cluster_centroids = [(c, cluster.centroid) for c, cluster in factor_catalog.clusters.items() if cluster is not None]\n",
    "        self.state[iteration] = {\"assignment\":factor_assignment, \"cluster_centroids\": cluster_centroids}\n",
    "        self.max_clusters_found = max(self.max_clusters_found, len(cluster_centroids))\n",
    "\n",
    "    def matrix_difference(self, i_centroids, j_centroids):\n",
    "        if i_centroids.shape == j_centroids.shape:\n",
    "            distance = np.linalg.norm(i_centroids - j_centroids, axis=1)\n",
    "            centroid_shifts = np.mean(distance)\n",
    "        else:\n",
    "            min_shape = (min(i_centroids.shape[0], j_centroids.shape[0]), min(i_centroids.shape[1], j_centroids.shape[1]))\n",
    "            centroid_shifts = np.mean(np.linalg.norm(i_centroids[:min_shape[0], :min_shape[1]] - j_centroids[:min_shape[0], :min_shape[1]], axis=1))\n",
    "            if i_centroids.shape[0] > j_centroids.shape[0]:\n",
    "                centroid_shifts += np.mean((len(i_centroids[min_shape[0]:])/i_centroids.shape[0]) * i_centroids[min_shape[0]:])\n",
    "            else:\n",
    "                centroid_shifts += np.mean((len(j_centroids[min_shape[0]:])/j_centroids.shape[0]) * j_centroids[min_shape[0]:])\n",
    "        # logger.info(f\"Diff Shift: {centroid_shifts}\")\n",
    "        return centroid_shifts     \n",
    "\n",
    "    def cluster(self, max_iterations: int = 20, threshold: float = None, early_stopping: bool = True):\n",
    "        self.initialize_clusters()        \n",
    "        centroids = self.calculate_centroids()\n",
    "        converged = False\n",
    "        current_iter = 0\n",
    "        if threshold is None:\n",
    "            threshold = self.threshold\n",
    "        else:\n",
    "            self.threshold = threshold\n",
    "        with tqdm(total=max_iterations, desc=\"Running clustering. N Clusters: NA, Added: NA\") as pbar:\n",
    "            while not converged:\n",
    "                if current_iter >= max_iterations:\n",
    "                    logger.info(f\"{self.n_factors} Factor Clustering did not converge after {max_iterations} iterations.\")\n",
    "                    break\n",
    "                self.purge_clusters()\n",
    "    \n",
    "                model_list = self.rng.permutation(list(self.models.keys()))\n",
    "                for model_i in model_list:\n",
    "                    model_factors = [factor.factor_id for factor in self.models[model_i].factors]\n",
    "                    factor_dist = {}\n",
    "                    factor_hi = {}\n",
    "                    # Calculate distances for all factors in the model to all centroids and then order the distances.\n",
    "                    for factor_i in model_factors:\n",
    "                        distances = [(j, self.distance(self.factors[factor_i].profile, cluster.centroid)) for j, cluster in self.clusters.items()]\n",
    "                        distances.sort(key=lambda x: x[1], reverse=True)\n",
    "                        factor_dist[str(factor_i)] = distances\n",
    "                        factor_hi[str(factor_i)] = distances[0]\n",
    "                    already_assigned = []\n",
    "                    factor_hi = dict(sorted(factor_hi.items(), key=lambda x: x[1], reverse=True))\n",
    "                    # Assign factors to clusters, if model hasn't contributed to the cluster already and if the correlation is above the threshold\n",
    "                    for factor_id in factor_hi.keys():\n",
    "                        # iterate through list of clusters in order of highest correlation.\n",
    "                        cluster_idx = -1\n",
    "                        for cluster_i, correlation_i in factor_dist[factor_id]:\n",
    "                            if cluster_i not in already_assigned and correlation_i >= threshold:\n",
    "                                cluster_idx = cluster_i\n",
    "                                break\n",
    "                        if cluster_idx != -1:\n",
    "                            self.clusters[cluster_idx].add(factor=self.factors[int(factor_id)], cor=factor_hi[factor_id][1])\n",
    "                            already_assigned.append(cluster_idx)\n",
    "                        else:\n",
    "                            new_cluster_id = self.dropped_clusters.pop(0) if len(self.dropped_clusters) > 0 else len(self.clusters)\n",
    "                                \n",
    "                            new_cluster = Cluster(cluster_id=new_cluster_id, centroid=self.factors[int(factor_id)].profile)\n",
    "                            new_cluster.add(factor=self.factors[int(factor_id)], cor=1.0)\n",
    "                            self.clusters[new_cluster_id] = new_cluster\n",
    "                            already_assigned.append(new_cluster_id)\n",
    "    \n",
    "                # Recalculate centroids of clusters\n",
    "                self.save_state(iteration=current_iter)\n",
    "                new_centroids = self.calculate_centroids()\n",
    "\n",
    "                if (self.matrix_difference(i_centroids=new_centroids, j_centroids=centroids) < 0.0001 and current_iter > 3 and early_stopping) or (current_iter >= max_iterations):\n",
    "                    converged = True\n",
    "                    \n",
    "                # if not converged:\n",
    "                #     # pass\n",
    "                #     new_centroids = self.cluster_cleanup()\n",
    "   \n",
    "                pbar.update(1) \n",
    "                pbar.set_description(f\"Running {self.n_factors} Factor Clustering. N Clusters: {len(new_centroids)}, Added: {len(new_centroids) - len(centroids)}\")\n",
    "                centroids = new_centroids\n",
    "                current_iter += 1\n",
    "        self.score()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d673c39-6f03-4fb1-a237-820852325dd4",
   "metadata": {},
   "source": [
    "### Workflow setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e729af03-8b4b-48eb-a310-95341d024a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5b5b1f63-c053-4d06-ae1d-02b03f955984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-May-25 15:23:54 - Total V: 418, Batch Size: 418, factors: 3-10\n"
     ]
    }
   ],
   "source": [
    "method = \"ls-nmf\"                   # \"ls-nmf\", \"ws-nmf\"\n",
    "models = 20                         # the number of models to train\n",
    "init_method = \"col_means\"           # default is column means \"col_means\", \"kmeans\", \"cmeans\"\n",
    "seed = 42                           # random seed for initialization\n",
    "converge_delta = 0.1                # convergence criteria for the change in loss, Q\n",
    "converge_n = 25                     # convergence criteria for the number of steps where the loss changes by less than converge_delta\n",
    "\n",
    "n_features = V.shape[1]\n",
    "samples_n = V.shape[0]\n",
    "batch_size = 100\n",
    "max_batches = 1\n",
    "i_batches = 0\n",
    "n_models = 5\n",
    "max_iter = 20000\n",
    "\n",
    "i_selection = rng.choice(V.shape[0], size=batch_size, replace=False, shuffle=True)\n",
    "i_V, i_U = prepare_data(V=V, U=U, i_selection=i_selection)\n",
    "\n",
    "subset_models = {}\n",
    "model_i = 0\n",
    "t0 = time.time()\n",
    "change_p = 1.0\n",
    "\n",
    "k_min = 3\n",
    "k_max = 10\n",
    "k_catalog = {}\n",
    "k_batches = {}\n",
    "\n",
    "logger.info(f\"Total V: {V.shape[0]}, Batch Size: {batch_size}, factors: {k_min}-{k_max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7575b1c0-1304-40ee-ba76-69e184a3002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "with tqdm(range(max_batches*(k_max-k_min+1)), desc=\"Generating subset profiles\") as pbar:\n",
    "    for k in range(k_min, k_max+1):\n",
    "        i_batches = []\n",
    "        for i in range(max_batches):\n",
    "            j_selection = rng.choice(i_V.shape[0], size=int(batch_size*change_p), replace=False, shuffle=True)\n",
    "            idx_change = rng.choice(batch_size, size=int(batch_size*change_p), replace=False, shuffle=True)\n",
    "            i_selection[idx_change] = j_selection\n",
    "            i_V, i_U = prepare_data(V=V, U=U, i_selection=i_selection)\n",
    "    \n",
    "            batch_sa = BatchSA(V=i_V, U=i_U, factors=k, models=n_models, method=method, seed=rng.integers(low=0, high=1e8), max_iter=max_iter,\n",
    "                                converge_delta=converge_delta, converge_n=converge_n, verbose=False)\n",
    "            _ = batch_sa.train()\n",
    "            i_batches.append(batch_sa) \n",
    "            pbar.update(1)\n",
    "            pbar.set_description(f\"Generating subset profiles. K: {k}\")\n",
    "        k_batches[k] = i_batches\n",
    "t1 = time.time()\n",
    "logger.info(f\"Runtime: {((t1-t0)/60):.2f} min(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e4f621-6d22-4a7c-bbd7-40279d2c7f26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k_loss = {}\n",
    "for k in range(k_min, k_max+1):\n",
    "    factor_catalog = BatchFactorCatalog(n_factors=k, n_features=n_features, threshold=0.8, seed=42)\n",
    "    batch_qtrue = []\n",
    "    batch_qrobust = []\n",
    "    for i_batch in k_batches[k]:\n",
    "        for sa in i_batch.results:\n",
    "            factor_catalog.add_model(model=sa, norm=True)\n",
    "            batch_qtrue.append(sa.Qtrue)\n",
    "            batch_qrobust.append(sa.Qrobust)\n",
    "    k_loss[k] = {\"Q(True)\": round(float(np.mean(batch_qtrue)),2), \"Q(Robust)\": round(float(np.mean(batch_qrobust)),2), \"ratio\": round(float(np.mean(batch_qtrue)/np.mean(batch_qrobust)),2)}\n",
    "    k_catalog[k] = factor_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11959a34-ac73-43be-bead-b97afd0c8ac3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t2 = time.time()\n",
    "for k, fc in k_catalog.items():\n",
    "    fc.cluster(max_iterations=15, threshold=.8)\n",
    "t3 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796cd873-590f-49cd-8abc-47abbb41295c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k, fc in k_catalog.items():\n",
    "    membership_searching = True\n",
    "    membership_p = 0.1\n",
    "    min_p = 0.0\n",
    "    max_p = 0.2\n",
    "    max_i = 10\n",
    "    p_clusters = -1\n",
    "    while membership_searching and max_i > 0:\n",
    "        fc.metrics(membership_p=membership_p)\n",
    "        p_clusters = len(fc.primary_clusters)\n",
    "        # print(f\"F: {k}, Min: {min_p}, Max: {max_p}, %: {membership_p}, P: {p_clusters}\")\n",
    "        if p_clusters > k:\n",
    "            min_p = membership_p\n",
    "            membership_p = (max_p + membership_p)/2.0\n",
    "        elif p_clusters < k:\n",
    "            max_p = membership_p\n",
    "            membership_p = (membership_p + min_p)/2.0\n",
    "        else:\n",
    "            membership_searching = False\n",
    "        max_i -= 1\n",
    "    # print(f\"Factor: {k}, Search i: {10-max_i}, %: {membership_p}, Clusters: {p_clusters}, Min: {min_p}, Max: {max_p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e954c-2ea1-4b58-b811-3acd21083a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_catalog = {}\n",
    "for factors, fc in k_catalog.items():\n",
    "    cluster_wcss = []\n",
    "    cluster_volume = []\n",
    "    cluster_r2 = []\n",
    "    count_threshold = int(fc.factor_count * membership_p)\n",
    "    for c_id, cluster in fc.clusters.items():\n",
    "        if len(cluster) >= count_threshold:\n",
    "            cluster_wcss.append(cluster.wcss)\n",
    "            cluster_r2.append(cluster.mean_r2)\n",
    "    range_catalog[factors] = {\n",
    "        \"count\": len(fc.clusters),\n",
    "        \"primary factors\": len(fc.primary_factors),\n",
    "        \"primary clusters\": len(fc.primary_clusters),\n",
    "        \"s/n\": len(fc.primary_factors)/len(fc.factors),\n",
    "        \"bcss\": np.round(fc.bcss, 4),\n",
    "        \"sil\": np.round(fc.sil, 4), \n",
    "        \"mean wcss\": np.round(np.mean(cluster_wcss), 4),\n",
    "        \"mean r2\": np.round(np.mean(cluster_r2), 4),\n",
    "        \"qtrue\": k_loss[factors][\"Q(True)\"],\n",
    "        \"qrobust\": k_loss[factors][\"Q(Robust)\"],\n",
    "        \"qratio\": k_loss[factors][\"ratio\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16270ff-d5ab-4b38-a635-6f791ad2b9ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "catalog_df = pd.DataFrame(range_catalog)\n",
    "# print(f\"True Factor Count: {syn_factors}\")\n",
    "catalog_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690a2d5b-46aa-400c-84ca-6d09cb69b0e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(catalog_df.loc[\"sil\"]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3104dc-b5a6-4d0b-bc9d-b48d0bbef0a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig2 = go.Figure()\n",
    "# fig2.add_trace(go.Scatter(x=np.arange(k_min, k_max+1), y=np.arange(k_min, k_max+1)/catalog_df.loc[\"primary clusters\"], mode='lines+markers', name=\"Modeled/Clustered\"))\n",
    "fig2.add_trace(go.Scatter(x=np.arange(k_min, k_max+1), y=catalog_df.loc[\"primary factors\"]/catalog_df.loc[\"primary factors\"].sum(), mode='lines+markers', name=\"N-Primary Factors\"))\n",
    "fig2.add_trace(go.Scatter(x=np.arange(k_min, k_max+1), y=catalog_df.loc[\"sil\"], mode='lines+markers', name=\"SIL\"))\n",
    "fig2.add_trace(go.Scatter(x=np.arange(k_min, k_max+1), y=catalog_df.loc[\"s/n\"], mode='lines+markers', name=\"S/N\"))\n",
    "norm_bcss = catalog_df.loc[\"bcss\"] / catalog_df.loc[\"bcss\"].sum()\n",
    "fig2.add_trace(go.Scatter(x=np.arange(k_min, k_max+1), y=norm_bcss, mode='lines+markers', name=\"Norm BCSS\"))\n",
    "norm_wcss = catalog_df.loc[\"mean wcss\"] / catalog_df.loc[\"mean wcss\"].sum()\n",
    "fig2.add_trace(go.Scatter(x=np.arange(k_min, k_max+1), y=norm_wcss, mode='lines+markers', name=\"Norm WCSS\"))\n",
    "# fig2.add_hline(y=1, line_width=1, line_dash=\"dash\", line_color=\"green\")\n",
    "# fig2.add_vline(x=syn_factors, line_width=1, line_dash=\"dash\", line_color=\"red\")\n",
    "fig2.update_layout(width=1200, height=800, title=\"Metrics Evaluating Optimal Factor Count\")\n",
    "fig2.update_xaxes(title_text=\"N Factors\")\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9405235b-0203-4bad-913c-37049ead3dce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ccd8b5c-462b-4b91-8620-b0d0b9d33db1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_scores = {}\n",
    "# for model_id, model in factor_catalog.models.items():\n",
    "#     model_factors = []\n",
    "#     for factor in model.factors:\n",
    "#         model_factors.append({\"id\": factor.factor_id, \"cluster_id\": factor.cluster_id, \"cluster_count\": len(factor_catalog.clusters[factor.cluster_id])})\n",
    "#     model_scores[model_id] = {\"score\": model.score, \"factors\": model_factors}\n",
    "# model_scores = dict(sorted(model_scores.items(), key=lambda item: item[1][\"score\"], reverse=True))\n",
    "# model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1006dc1-00db-4949-9e9a-a40ddda6153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_unique(fg, count_threshold: int = 1):\n",
    "    model_scores = {}\n",
    "    cluster_count = {}\n",
    "    for model_id, model in fg.models.items():\n",
    "        model_factors = []\n",
    "        for factor in model.factors:\n",
    "            model_factors.append({\"id\": factor.factor_id, \"cluster_id\": factor.cluster_id, \"cluster_count\": len(fg.clusters[factor.cluster_id])})\n",
    "            if factor.cluster_id not in cluster_count.keys():\n",
    "                cluster_count[factor.cluster_id] = len(fg.clusters[factor.cluster_id])\n",
    "        model_scores[model_id] = {\"score\": model.score, \"factors\": model_factors}\n",
    "    model_scores = dict(sorted(model_scores.items(), key=lambda item: item[1][\"score\"], reverse=True))\n",
    "\n",
    "    added_map = []\n",
    "    unique_models = {}\n",
    "    for model_id, model in model_scores.items():\n",
    "        model_mapping = {\"score\": model[\"score\"]}\n",
    "        cluster_mapping = {}\n",
    "        f_map = []\n",
    "        for factor in model[\"factors\"]:\n",
    "            cluster_mapping[factor[\"id\"]] = factor[\"cluster_id\"]\n",
    "            f_map.append(factor[\"cluster_id\"])\n",
    "        f_map = sorted(f_map)\n",
    "        if len(added_map) == 0:\n",
    "            added_map.append(f_map)\n",
    "            unique_models[model_id] = cluster_mapping\n",
    "        else:\n",
    "            add = False\n",
    "            for i_map in added_map:\n",
    "                if f_map != i_map:\n",
    "                    dif = list(set(f_map) - set(i_map))\n",
    "                    for d in dif:\n",
    "                        if cluster_count[d] > count_threshold:\n",
    "                            add = True\n",
    "            if add:\n",
    "                added_map.append(f_map)\n",
    "                unique_models[model_id] = cluster_mapping\n",
    "    return unique_models\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dda33273-19f1-4a70-9b65-a0cf82925e10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# unique_models = select_unique(fg=factor_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93f6a3ee-97a1-4682-b016-b06f9d599e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile_batches = min(20, len(unique_models.keys()))\n",
    "# profile_H = []\n",
    "# for model_i, mapping in unique_models.items():\n",
    "#     i_factors = list(unique_models[model_i].values())\n",
    "#     i_H = np.array([cluster.centroid for c, cluster in factor_catalog.clusters.items() if cluster.cluster_id in i_factors])\n",
    "#     profile_H.append(i_H)\n",
    "#     if len(profile_H) == profile_batches:\n",
    "#         break\n",
    "# profile_H = np.array(profile_H)\n",
    "\n",
    "# profile_H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c25bf66e-08b6-4a3a-b8c2-3b50854435b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = list(unique_models.keys())[0]\n",
    "# i_factors = list(unique_models[i].values())\n",
    "# i_H = np.array([cluster.centroid for c, cluster in factor_catalog.clusters.items() if cluster.cluster_id in i_factors])\n",
    "# i_H = i_H / i_H.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2967f22e-a14a-40d1-bed8-a8d84139a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "\n",
    "# ta0 = time.time()\n",
    "# run_batch = True\n",
    "# batch_seed = rng.integers(low=0, high=1e8)\n",
    "# if run_batch:\n",
    "#     batch_sa = BatchSA(V=V, U=U, factors=factors, models=20, method=method, seed=batch_seed, max_iter=50000,\n",
    "#                        converge_delta=converge_delta, converge_n=converge_n, verbose=False)\n",
    "#     _ = batch_sa.train()\n",
    "#     batch_sa.details()\n",
    "# else:\n",
    "#     base_sa = SA(factors=factors, method=method, V=V, U=U, seed=seed, verbose=True)\n",
    "#     base_sa.initialize(H=None, W=None)\n",
    "#     _ = base_sa.train(max_iter=50000, converge_delta=converge_delta, converge_n=converge_n)\n",
    "#     base_sa.summary()\n",
    "# ta1 = time.time()\n",
    "# logger.info(f\"Total BatchSA Runtime: {(ta1-ta0)/60:.2f} min(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca89ca29-9fc9-4f80-aae4-53f82c703cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc_base = BatchFactorCatalog(n_factors=factors, n_features=V.shape[1], threshold=0.8, seed=42)\n",
    "# for model in batch_sa.results:\n",
    "#     fc_base.add_model(model=model, norm=True)\n",
    "# fc_base.cluster(max_iterations=50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfdf15e8-1a37-48bb-a63a-9356ab156e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc_base.plot(method=\"tsne\", membership_p=0.03)\n",
    "# fc_base.plot(method=\"ae\", membership_p=0.03)\n",
    "# fc_base.plot(method=\"mds\", membership_p=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48d54240-98b4-4c3e-ad34-2013a96b5782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# t4 = time.time()\n",
    "\n",
    "# samples_n = V.shape[0]\n",
    "# n_slices = int(np.ceil(samples_n/2000))\n",
    "# logger.info(f\"Slices: {n_slices}\")\n",
    "# if run_batch:\n",
    "#     i_W = None\n",
    "#     i_H = None\n",
    "#     V_slices = np.array_split(V, n_slices, axis=0)\n",
    "#     U_slices = np.array_split(U, n_slices, axis=0)\n",
    "#     for i in range(n_slices):\n",
    "#         i_V = V_slices[i]\n",
    "#         i_U = U_slices[i]\n",
    "#         profile_batch = BatchSA(V=i_V, U=i_U, H=profile_H, factors=factors, models=profile_batches, method=method, seed=batch_seed, max_iter=5000,\n",
    "#                                 converge_delta=0.1, converge_n=25, verbose=False, delay_h=500)\n",
    "#         _ = profile_batch.train()\n",
    "#         if i_W is None:\n",
    "#             i_W = np.array([model.W for model in profile_batch.results])\n",
    "#             i_H = [np.array([model.H for model in profile_batch.results])]\n",
    "#         else:\n",
    "#             j_W = [model.W for model in profile_batch.results]\n",
    "#             i_W = np.hstack((i_W, j_W))\n",
    "#             i_H.append(np.array([model.H for model in profile_batch.results]))\n",
    "#     p_H = np.array(i_H).mean(axis=0)     \n",
    "#     profile_batch2 = BatchSA(V=V, U=U, H=profile_H, W=i_W, factors=factors, models=profile_batches, method=method, seed=batch_seed, max_iter=5000,\n",
    "#                              converge_delta=converge_delta, converge_n=converge_n, verbose=False)\n",
    "#     _ = profile_batch2.train()\n",
    "#     profile_batch2.details()  \n",
    "# else:\n",
    "#     final_sa1 = SA(factors=factors, method=method, V=V, U=U, seed=seed, verbose=True)\n",
    "#     final_sa1.initialize(H=i_H, W=None)\n",
    "#     _ = final_sa1.train(max_iter=1000, converge_delta=converge_delta, converge_n=converge_n, hold_h=True)\n",
    "#     final_sa2 = SA(factors=factors, method=method, V=V, U=U, seed=seed, verbose=True)\n",
    "#     final_sa2.initialize(H=final_sa1.H, W=final_sa1.W)\n",
    "#     _ = final_sa2.train(max_iter=10000, converge_delta=converge_delta, converge_n=converge_n, hold_h=False)\n",
    "#     final_sa2.summary()\n",
    "# t5 = time.time()\n",
    "# total_runtime = ((t1-t0) + (t3-t2) + (t5-t4))/60\n",
    "# logger.info(f\"Total Profiler Runtime: {(t5-t4)/60:.2f} min(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2265a28b-dc90-4b19-b61c-4dede4d155d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc_profiled = BatchFactorCatalog(n_factors=factors, n_features=V.shape[1], threshold=0.8, seed=42)\n",
    "# for model in profile_batch2.results:\n",
    "#     fc_profiled.add_model(model=model, norm=True)\n",
    "# fc_profiled.cluster(max_iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cece791-9bce-463e-8ac2-ae3dda8bef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc_profiled.plot(method=\"tsne\", membership_p=0.03)\n",
    "# fc_profiled.plot(method=\"ae\", membership_p=0.03)\n",
    "# fc_profiled.plot(method=\"mds\", membership_p=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b184b735-275f-433b-ae9f-beb6a9443ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b03bdba1-13b4-4eb3-9e15-8f080e11edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if run_batch:\n",
    "#     base_matrix = np.vstack([model.H for model in batch_sa.results])\n",
    "#     profile_matrix = np.vstack([model.H for model in profile_batch.results])\n",
    "# else:\n",
    "#     base_matrix=base_sa.H\n",
    "#     profiled_matrix=final_sa2.H\n",
    "# factor_catalog.animate(base_matrix=base_matrix, profiled_matrix=profile_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81164b9b-465d-4587-9b50-f32fba3e84ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# factor_catalog.animate(base_matrix=final_sa2.H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28f56fe2-889f-4fbd-b8e6-8cce523be476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_test = factor_catalog.compare(matrix=base_sa.H)\n",
    "# base_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a71bcc6-49e5-4e27-b99d-789d703a1b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_test = factor_catalog.compare(matrix=final_sa2.H)\n",
    "# final_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c0f9805-2e32-436c-8a31-443597754382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def distance(factor1, factor2):\n",
    "#     f1 = np.array(factor1).astype(float)\n",
    "#     f2 = np.array(factor2).astype(float)\n",
    "#     corr_matrix = np.corrcoef(f2, f1)\n",
    "#     corr = corr_matrix[0, 1]\n",
    "#     r_sq = corr ** 2\n",
    "#     return r_sq\n",
    "\n",
    "# complete_mapping = {}\n",
    "# for i_key, i_model in enumerate(profile_batch2.results):\n",
    "#     n_factors = i_model.H.shape[0]\n",
    "#     i_h = i_model.H\n",
    "#     best_j = -1\n",
    "#     best_cor = 0.0\n",
    "#     i_results = {}\n",
    "#     for j_key, j_base in enumerate(batch_sa.results):\n",
    "#         j_h = j_base.H\n",
    "#         final_base_test = np.zeros(shape=(n_factors, n_factors))\n",
    "#         for i in range(n_factors):\n",
    "#             i_base = i_h[i]\n",
    "#             for j in range(n_factors):\n",
    "#                 j_final = j_h[j]\n",
    "#                 ij_cor = distance(factor1=i_base, factor2=j_final)\n",
    "#                 final_base_test[i, j] = ij_cor\n",
    "#         final_matrix = csr_matrix(final_base_test)\n",
    "#         final_mapping = min_weight_full_bipartite_matching(final_matrix, maximize=True)\n",
    "#         final_mapping = list(zip(final_mapping[0], final_mapping[1]))\n",
    "#         final_cor = float(np.round(np.mean([final_matrix[i] for i in final_mapping]),4))\n",
    "#         # i_results[j_key] = {\"map\": final_mapping, \"cor\": final_cor}\n",
    "#         if final_cor > best_cor:\n",
    "#             best_cor = final_cor\n",
    "#             best_j = j_key\n",
    "#     i_results[\"best\"] = best_j\n",
    "#     i_results[\"best_cor\"] = best_cor\n",
    "#     complete_mapping[i_key] = i_results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b299c941-9efd-4510-aaf1-7953067e1e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1532a47-9639-4b7b-b791-f13def969256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def animate(factor_catalog, base_matrix = None, profiled_matrix = None, method=\"mds\", membership_p: float = 0.03):\n",
    "#     all_factors = np.array([v.profile for k, v in factor_catalog.factors.items()])\n",
    "#     model_assignment = np.array([v.model_id for k, v in factor_catalog.factors.items()])\n",
    "#     required_members = int(len(all_factors)*membership_p)\n",
    "\n",
    "#     color_map = factor_catalog.generate_continuous_colormap(len(factor_catalog.clusters), colormap_name='rainbow')\n",
    "#     full_colormap = dict(zip(list(factor_catalog.clusters.keys()), [color[1] for color in color_map]))\n",
    "\n",
    "#     sample_lengths = []\n",
    "#     samples = []\n",
    "#     for i in range(len(factor_catalog.state)):\n",
    "#         cluster_centroids = factor_catalog.state[i][\"cluster_centroids\"]\n",
    "#         i_cluster, i_centroids = zip(*cluster_centroids)\n",
    "#         i_centroids = np.array(i_centroids)\n",
    "#         samples.append(i_centroids)\n",
    "#         sample_lengths.append(len(i_centroids))\n",
    "#     i_centroids = np.vstack(samples)\n",
    "\n",
    "#     sample_list = [all_factors, i_centroids]\n",
    "#     samples_n = np.sum(sample_lengths) + len(all_factors)\n",
    "#     plot_base = False\n",
    "#     if base_matrix is not None:\n",
    "#         sample_list.append(base_matrix)\n",
    "#         plot_base = True\n",
    "#     plot_profiled = False\n",
    "#     if profiled_matrix is not None:\n",
    "#         sample_list.append(profiled_matrix)\n",
    "#         plot_profiled = True\n",
    "\n",
    "#     if method.lower() == \"tsne\":\n",
    "#         samples = np.vstack(sample_list)\n",
    "#         factor_model = TSNE(n_components=3, random_state=0, perplexity=min(50, len(samples)))\n",
    "#         reduction_results = factor_model.fit_transform(samples)\n",
    "#         factor_reduction = reduction_results[:len(all_factors),:]\n",
    "#         cluster_centroids = reduction_results[len(all_factors):samples_n,:]\n",
    "#         if plot_base:\n",
    "#             base_points = reduction_results[i_max:i_max + base_matrix.shape[0],:]\n",
    "#             i_max += base_matrix.shape[0]\n",
    "#         if plot_profiled:\n",
    "#             profiled_points = reduction_results[i_max:i_max + profiled_matrix.shape[0],:]\n",
    "#     elif method.lower() == \"mds\":\n",
    "#         samples = np.vstack(sample_list)\n",
    "#         factor_model = MDS(n_components=3, random_state=0, metric=True, max_iter=300)\n",
    "#         reduction_results = factor_model.fit_transform(samples)\n",
    "#         factor_reduction = reduction_results[:len(all_factors),:]\n",
    "#         cluster_centroids = reduction_results[len(all_factors):samples_n,:]\n",
    "#         i_max = samples_n\n",
    "#         if plot_base:\n",
    "#             base_points = reduction_results[i_max:i_max + base_matrix.shape[0],:]\n",
    "#             i_max += base_matrix.shape[0]\n",
    "#         if plot_profiled:\n",
    "#             profiled_points = reduction_results[i_max:i_max + profiled_matrix.shape[0],:]\n",
    "#     else:\n",
    "#         autoencoder, encoder = build_autoencoder(input_dim=i_centroids.shape[1], encoding_dim=3)\n",
    "#         autoencoder.compile(optimizer='adam', loss='mse')\n",
    "#         autoencoder.fit(all_factors, all_factors, epochs=50, batch_size=16, shuffle=True, validation_split=0.2, verbose=0)\n",
    "#         factor_reduction = encoder.predict(all_factors, verbose=0)\n",
    "#         cluster_centroids = encoder.predict(i_centroids, verbose=0)\n",
    "#         if plot_base:\n",
    "#             base_points = encoder.predict(base_matrix, verbose=0)\n",
    "#         if plot_profiled:\n",
    "#             profiled_points = encoder.predict(profiled_matrix, verbose=0)\n",
    "        \n",
    "#     cumulative_lengths = np.cumsum([0] + sample_lengths)\n",
    "#     state_centroids = [cluster_centroids[cumulative_lengths[i]: cumulative_lengths[i+1]] for i in range(len(sample_lengths))]\n",
    "\n",
    "#     if base_matrix is not None:\n",
    "#         df_base = pd.DataFrame(base_points, columns=['x', 'y', 'z'])\n",
    "\n",
    "#     if profiled_matrix is not None:\n",
    "#         df_profiled = pd.DataFrame(profiled_points, columns=['x', 'y', 'z'])\n",
    "\n",
    "#     df_pca0 = pd.DataFrame(factor_reduction, columns=['x', 'y', 'z'])\n",
    "#     df_pca0[\"text\"] = \"P: \" + df_pca0.index.astype(str) + \", M: \" + model_assignment.astype(str)\n",
    "    \n",
    "#     frames = []\n",
    "#     for i in range(len(factor_catalog.state)):\n",
    "#         cluster_centroids = factor_catalog.state[i][\"cluster_centroids\"]\n",
    "#         i_cluster, i_centroids = zip(*cluster_centroids)\n",
    "#         factor_assignments = factor_catalog.state[i][\"assignment\"]\n",
    "#         i_centroids = state_centroids[i]\n",
    "            \n",
    "#         i_length = len(i_cluster)\n",
    "#         i_centroids = np.array(i_centroids)\n",
    "        \n",
    "#         df_pca = pd.DataFrame(factor_reduction, columns=['x', 'y', 'z'])\n",
    "#         factor_columns = df_pca.columns\n",
    "#         df_pca[\"Cluster\"] = factor_assignments\n",
    "#         df_pca[\"text\"] = \"P: \" + df_pca0.index.astype(str) + \", M: \" + model_assignment.astype(str) + \", C:\" + df_pca[\"Cluster\"].astype(str)\n",
    "        \n",
    "#         df_centroids = pd.DataFrame(i_centroids, columns=['x', 'y', 'z'], index=list(i_cluster))\n",
    "#         cluster_columns = df_centroids.columns\n",
    "#         assigned_centroids, cluster_size = np.unique(factor_assignments, return_counts=True)\n",
    "#         df_centroids[\"text\"] = \"Centroid \" + df_centroids.index.astype(str)\n",
    "#         df_centroids[\"Cluster\"] = list(i_cluster)\n",
    "#         df_centroids = df_centroids.loc[assigned_centroids]\n",
    "#         df_centroids[\"count\"] = cluster_size\n",
    "#         df_centroids = df_centroids[df_centroids[\"count\"] > required_members]\n",
    "            \n",
    "#         df_pca[\"color\"] = df_pca[\"Cluster\"].map(full_colormap)\n",
    "#         df_centroids[\"color\"] = df_centroids[\"Cluster\"].map(full_colormap)\n",
    "\n",
    "#         data = [\n",
    "#             go.Scatter3d(\n",
    "#                 x=df_pca[\"x\"],\n",
    "#                 y=df_pca[\"y\"],\n",
    "#                 z=df_pca[\"z\"],\n",
    "#                 mode='markers',\n",
    "#                 marker=dict(\n",
    "#                     size=3,\n",
    "#                     color=df_pca[\"color\"],\n",
    "#                     opacity=0.4\n",
    "#                 ),\n",
    "#                 text=df_pca[\"text\"],\n",
    "#                 hoverinfo='text'\n",
    "#             ),\n",
    "#             go.Scatter3d(\n",
    "#                 x=df_centroids['x'],\n",
    "#                 y=df_centroids['y'],\n",
    "#                 z=df_centroids['z'],\n",
    "#                 mode='markers',\n",
    "#                 marker=dict(\n",
    "#                     size=2,\n",
    "#                     color=\"black\",\n",
    "#                     symbol='x',\n",
    "#                     opacity=0.75\n",
    "#                 ),\n",
    "#                 text=df_centroids[\"text\"],\n",
    "#                 hoverinfo='text'\n",
    "#             )\n",
    "#         ]\n",
    "#         if plot_base:\n",
    "#             data.append(go.Scatter3d(\n",
    "#                 x=df_base['x'],\n",
    "#                 y=df_base['y'],\n",
    "#                 z=df_base['z'],\n",
    "#                 mode='markers',\n",
    "#                 marker=dict(\n",
    "#                     size=4,\n",
    "#                     color=\"black\",\n",
    "#                     symbol='cross'\n",
    "#                 ),\n",
    "#                 name=\"Base Factor\"\n",
    "#             ))\n",
    "#         if plot_profiled:\n",
    "#             data.append(go.Scatter3d(\n",
    "#                 x=df_profiled['x'],\n",
    "#                 y=df_profiled['y'],\n",
    "#                 z=df_profiled['z'],\n",
    "#                 mode='markers',\n",
    "#                 marker=dict(\n",
    "#                     size=4,\n",
    "#                     color=\"green\",\n",
    "#                     symbol='cross'\n",
    "#                 ),\n",
    "#                 name=\"P Factor\"\n",
    "#             ))\n",
    "#         frames.append(\n",
    "#             go.Frame(data=data, \n",
    "#                      layout=go.Layout(\n",
    "#                          annotations=[\n",
    "#                             dict(\n",
    "#                                 x=1,\n",
    "#                                 y=1,\n",
    "#                                 showarrow=False,\n",
    "#                                 text=f\"Iteration: {i + 1}/{len(factor_catalog.state)}\",\n",
    "#                                 xref=\"paper\",\n",
    "#                                 yref=\"paper\",\n",
    "#                                 font=dict(size=14)\n",
    "#                             )\n",
    "#                         ]\n",
    "#                     ),\n",
    "#                      name=str(i)\n",
    "#                     )\n",
    "#         )\n",
    "#         if i == 0:\n",
    "#             state0 = [\n",
    "#                 go.Scatter3d(\n",
    "#                     x=df_pca0[\"x\"],\n",
    "#                     y=df_pca0[\"y\"],\n",
    "#                     z=df_pca0[\"z\"],\n",
    "#                     mode='markers',\n",
    "#                     marker=dict(\n",
    "#                         size=3,\n",
    "#                         color=\"gray\",\n",
    "#                         opacity=0.4\n",
    "#                     ),\n",
    "#                     text=df_pca0[\"text\"],\n",
    "#                     hoverinfo='text'\n",
    "#                 ),\n",
    "#                 go.Scatter3d(\n",
    "#                     x=df_centroids['x'],\n",
    "#                     y=df_centroids['y'],\n",
    "#                     z=df_centroids['z'],\n",
    "#                     mode='markers',\n",
    "#                     marker=dict(\n",
    "#                         size=2,\n",
    "#                         color=\"black\",\n",
    "#                         symbol='x',\n",
    "#                         opacity=0.75\n",
    "#                     ),\n",
    "#                     text=df_centroids[\"text\"],\n",
    "#                     hoverinfo='text'\n",
    "#                 )\n",
    "#             ]\n",
    "#             if plot_base:\n",
    "#                 state0.append(go.Scatter3d(\n",
    "#                     x=df_base['x'],\n",
    "#                     y=df_base['y'],\n",
    "#                     z=df_base['z'],\n",
    "#                     mode='markers',\n",
    "#                     marker=dict(\n",
    "#                         size=4,\n",
    "#                         color=\"black\",\n",
    "#                         symbol='cross'\n",
    "#                     ),\n",
    "#                     name=\"Base Factor\"\n",
    "#                 ))\n",
    "#             if plot_profiled:\n",
    "#                 state0.append(go.Scatter3d(\n",
    "#                     x=df_profiled['x'],\n",
    "#                     y=df_profiled['y'],\n",
    "#                     z=df_profiled['z'],\n",
    "#                     mode='markers',\n",
    "#                     marker=dict(\n",
    "#                         size=4,\n",
    "#                         color=\"green\",\n",
    "#                         symbol='cross'\n",
    "#                     ),\n",
    "#                     name=\"P Factor\"\n",
    "#                 ))\n",
    "#     df_pca0 = pd.DataFrame(factor_reduction, columns=['x', 'y', 'z'])\n",
    "#     min_values = [[df_pca0['x'].min()],[df_pca0['y'].min()],[df_pca0['z'].min()]]\n",
    "#     max_values = [[df_pca0['x'].max()],[df_pca0['y'].max()],[df_pca0['z'].max()]]\n",
    "    \n",
    "#     fig = go.Figure(\n",
    "#         data=state0,\n",
    "#         layout=go.Layout(\n",
    "#             title=\"Factor Profile Clustering\",\n",
    "#             height=1000,\n",
    "#             width=1000,\n",
    "#             # scene=dict(\n",
    "#             #     xaxis=dict(range=[min(min_values[0])-1.25, max(max_values[0])+1.25], autorange=False),\n",
    "#             #     yaxis=dict(range=[min(min_values[1])-1.25, max(max_values[1])+1.25], autorange=False),\n",
    "#             #     zaxis=dict(range=[min(min_values[2])-1.25, max(max_values[2])+1.25], autorange=False),\n",
    "#             #     aspectmode=\"manual\",\n",
    "#             #     aspectratio=dict(x=1, y=1, z=1)\n",
    "#             # ),\n",
    "#             updatemenus=[dict(\n",
    "#                 type=\"buttons\",\n",
    "#                 showactive=False,\n",
    "#                 buttons=[\n",
    "#                     dict(label=\"Play\",\n",
    "#                          method=\"animate\",\n",
    "#                          args=[None, dict(frame=dict(duration=500, redraw=True), fromcurrent=True, mode=\"immediate\")]),\n",
    "#                     dict(label=\"Pause\",\n",
    "#                          method=\"animate\",\n",
    "#                          args=[[None], dict(frame=dict(duration=0, redraw=False), mode=\"immediate\")]),\n",
    "#                     dict(\n",
    "#                         args=[[0], dict(frame=dict(duration=0, redraw=True), mode=\"immediate\")],\n",
    "#                         label=\"Reset\",\n",
    "#                         method=\"animate\"),\n",
    "#                 ]\n",
    "#             )],\n",
    "#             annotations=[\n",
    "#                 dict(\n",
    "#                     x=1,\n",
    "#                     y=1,\n",
    "#                     showarrow=False,\n",
    "#                     text=\"Iteration: NA\",\n",
    "#                     xref=\"paper\",\n",
    "#                     yref=\"paper\",\n",
    "#                     font=dict(size=14)\n",
    "#                 )\n",
    "#             ],\n",
    "#             showlegend=False\n",
    "#         ),\n",
    "#         frames=frames\n",
    "    # )\n",
    "    # # Show the figure\n",
    "    # fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a641b75-d941-44c2-ac4a-a7fc1a192657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_H = np.vstack([r.H for r in profile_batch2.results])\n",
    "# profiled_H = np.vstack([r.H for r in batch_sa.results])\n",
    "# animate(factor_catalog=factor_catalog, base_matrix=base_H, profiled_matrix=profiled_H, membership_p=0.03)\n",
    "# animate(factor_catalog=factor_catalog, method='ae', membership_p=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3930663a-7832-427a-8b32-5f79edc8c34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "# tf.random.set_seed(42)\n",
    "\n",
    "# class SynGAN:\n",
    "#     def __init__(self, n_features, latent_dim_factor: int = 5, initial_lr: float = 0.0002):\n",
    "#         self.n_features = n_features\n",
    "#         self.latent_dim = latent_dim_factor * n_features\n",
    "\n",
    "#         self.scaler = MinMaxScaler()\n",
    "\n",
    "#         # Initialize learning rates\n",
    "#         self.generator_lr = initial_lr\n",
    "#         self.discriminator_lr = initial_lr\n",
    "\n",
    "#         # Build and compile the discriminator\n",
    "#         self.discriminator = self.build_discriminator()\n",
    "#         self.discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=self.discriminator_lr)\n",
    "#         self.discriminator.compile(\n",
    "#             loss='binary_crossentropy', \n",
    "#             optimizer=self.discriminator_optimizer, \n",
    "#             metrics=['accuracy']\n",
    "#         )\n",
    "\n",
    "#         # Build the generator\n",
    "#         self.generator = self.build_generator()\n",
    "#         self.generator_optimizer = tf.keras.optimizers.Adam(learning_rate=self.generator_lr)\n",
    "\n",
    "#         # Build and compile the GAN\n",
    "#         self.discriminator.trainable = False\n",
    "#         self.model = self.build_gan()\n",
    "#         self.model.compile(loss='binary_crossentropy', \n",
    "#                            optimizer=self.generator_optimizer\n",
    "#                           )\n",
    "\n",
    "#     def build_generator(self):\n",
    "#         model = Sequential([\n",
    "#             Dense(self.latent_dim*2, input_dim=self.latent_dim),\n",
    "#             LeakyReLU(negative_slope=0.2),\n",
    "#             BatchNormalization(momentum=0.8),\n",
    "#             Dense(self.latent_dim * 4),\n",
    "#             LeakyReLU(negative_slope=0.2),\n",
    "#             BatchNormalization(momentum=0.8),\n",
    "#             Dense(self.n_features, activation='sigmoid')\n",
    "#         ])\n",
    "#         return model\n",
    "\n",
    "#     def build_discriminator(self):\n",
    "#         model = Sequential([\n",
    "#             Dense(self.n_features * 4, input_dim=self.n_features),\n",
    "#             LeakyReLU(negative_slope=0.2),\n",
    "#             Dropout(0.3),\n",
    "#             Dense(self.n_features * 2),\n",
    "#             LeakyReLU(negative_slope=0.2),\n",
    "#             Dropout(0.3),\n",
    "#             Dense(1, activation='sigmoid')\n",
    "#         ])\n",
    "#         return model\n",
    "\n",
    "#     def build_gan(self):\n",
    "#         model = Sequential([self.generator, self.discriminator])\n",
    "#         return model\n",
    "\n",
    "#     def custom_generator_loss(self, y_true, y_pred):\n",
    "#         # Calculate the mean of real and generated data\n",
    "#         real_mean = tf.reduce_mean(real_data, axis=0)\n",
    "#         generated_mean = tf.reduce_mean(generated_data, axis=0)\n",
    "#         mean_loss = tf.reduce_mean(tf.square(real_mean - generated_mean))\n",
    "\n",
    "#         # Calculate the std of real and generated data\n",
    "#         real_std = tf.math.reduce_std(real_data, axis=0)\n",
    "#         generated_std = tf.math.reduce_std(generated_data, axis=0)\n",
    "#         std_loss = tf.reduce_mean(tf.square(real_std - generated_std))\n",
    "    \n",
    "#         # Distribution Loss: KL Divergence or other distribution-based metrics\n",
    "#         epsilon = 1e-10\n",
    "#         real_data = tf.clip_by_value(real_data, epsilon, 1)\n",
    "#         generated_data = tf.clip_by_value(generated_data, epsilon, 1)\n",
    "#         kl_divergence = tf.reduce_mean(real_data * tf.math.log(real_data / generated_data))\n",
    "    \n",
    "#         # L2 Regularization: Penalize large weights in the generator\n",
    "#         l2_loss = tf.add_n([tf.nn.l2_loss(var) for var in self.generator.trainable_variables])\n",
    "    \n",
    "#         # Combine losses (adjust weights as needed)\n",
    "#         lambda_mean = 1.0\n",
    "#         lambda_std = 0.01\n",
    "#         lambda_kl = 0.01\n",
    "#         lambda_l2 = 0.01\n",
    "#         total_loss = lambda_mean * mean_loss + lambda_std * std_loss + lambda_kl * kl_divergence + lambda_l2 * l2_loss\n",
    "        \n",
    "#         return total_loss\n",
    "\n",
    "#     def adjust_learning_rate(self, optimizer, factor=0.5, min_lr=1e-6):\n",
    "#         # Reduce learning rate by factor if it's above min_lr\n",
    "#         new_lr = max(optimizer.learning_rate * factor, min_lr)\n",
    "#         optimizer.learning_rate.assign(new_lr)\n",
    "\n",
    "#     def generate_random_noise(self, batch_size):\n",
    "#         return np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "#     def train(self, data, epochs, batch_size):\n",
    "#         data_scaled = self.scaler.fit_transform(data)\n",
    "#         with tqdm(total=int(epochs), desc=\"Training Progress\") as pbar:\n",
    "#             for epoch in range(epochs):\n",
    "#                 # Train discriminator\n",
    "#                 real_samples = data_scaled[np.random.randint(0, data_scaled.shape[0], batch_size)]\n",
    "#                 noise = self.generate_random_noise(batch_size)\n",
    "#                 fake_samples = self.generator.predict(noise, verbose=0)\n",
    "    \n",
    "#                 X = np.concatenate([real_samples, fake_samples])\n",
    "#                 y = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n",
    "#                 # y = np.concatenate([np.ones((batch_size, 1)) * 0.9, np.zeros((batch_size, 1))])   # label Smoothing\n",
    "\n",
    "#                 d_loss, d_acc = self.discriminator.train_on_batch(X, y)\n",
    "    \n",
    "#                 # Train generator\n",
    "#                 noise = self.generate_random_noise(batch_size)\n",
    "#                 y_gen = np.ones((batch_size, 1))  # We want the generator to fool the discriminator\n",
    "    \n",
    "#                 g_loss = self.model.train_on_batch(noise, y_gen)\n",
    "\n",
    "#                 # Adjust learning rates based on loss\n",
    "#                 if d_loss < 0.1:  # Example threshold for adjusting learning rate\n",
    "#                     self.adjust_learning_rate(self.discriminator_optimizer)\n",
    "#                 if g_loss > 1.0:  # Example threshold for adjusting learning rate\n",
    "#                     self.adjust_learning_rate(self.generator_optimizer)\n",
    "                    \n",
    "#                 pbar.update(1)\n",
    "#                 pbar.set_description(f\"Epoch: {epoch}, D Loss: {d_loss:.4f}, D Acc: {d_acc:.4f}, G Loss: {g_loss:.4f}\")\n",
    "\n",
    "#     def generate_synthetic_data(self, num_samples):\n",
    "#         noise = self.generate_random_noise(num_samples)\n",
    "#         generated_data = self.generator.predict(noise, verbose=0)\n",
    "#         generated_data_rescaled = self.scaler.inverse_transform(generated_data)\n",
    "#         return generated_data_rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "87c0a0fe-4b63-490e-84b2-02939f70c39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# syn_gen = SynGAN(n_features=V.shape[1], latent_dim_factor=3)\n",
    "# syn_gen.train(V, epochs=1000, batch_size=64)\n",
    "# syn_V = syn_gen.generate_synthetic_data(num_samples=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a7227afa-f991-4b3c-a9d7-6ef856600edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df0 = pd.DataFrame(V)\n",
    "# df0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b9c3d467-5657-4843-a52e-6288ded3ceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.DataFrame(syn_V)\n",
    "# df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "20a3b54c-2453-4957-99e5-59422ca82d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df0[2].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7dc3d9f-ecb9-4b64-addc-5bf3cab8a25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1[2].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d956a225-f74c-4290-ac43-92ab6ac0e464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
