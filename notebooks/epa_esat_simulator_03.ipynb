{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5686853f-4655-4e79-af04-fe24aac1756e",
   "metadata": {},
   "source": [
    "## Batch Profile Modeling\n",
    "\n",
    "Using the ESAT simulator to evaluate potential approaches to optimize modeling very large datasets.\n",
    "\n",
    "The first approach will look at implementing and validating the following workflow:\n",
    "1. Create a subset dataset of the input by randomly selecting N values from the input/uncertainty.\n",
    "2. Train a single model on that data until convergence.\n",
    "3. Use the factor profile H matrix to calculate a W for the complete dataset.\n",
    "4. Calculate Q(full)\n",
    "5. Take a new subset of the data, restart training with the prior H.\n",
    "6. Repeat until Q(full) is no longer decreasing.\n",
    "\n",
    "Run full dataset model with the same random seed and evaluate the difference in loss and factor profiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ac55f-9259-4ce1-b87e-3302d2ed00f9",
   "metadata": {},
   "source": [
    "#### Code Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9de6b9e-89a5-4db8-94d4-2226605d9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from esat.data.datahandler import DataHandler\n",
    "from esat.model.batch_sa import BatchSA\n",
    "from esat.model.sa import SA\n",
    "from esat.data.analysis import ModelAnalysis, BatchAnalysis\n",
    "from esat_eval.simulator import Simulator\n",
    "from esat.estimator import FactorEstimator\n",
    "from esat_eval.factor_catalog import FactorCatalog, Factor\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import min_weight_full_bipartite_matching\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb2b9c-98ee-4482-a0f9-c06b1eeb651f",
   "metadata": {},
   "source": [
    "#### Synthetic Dataset\n",
    "\n",
    "Generate a synthetic dataset where the factor profiles and contributions are pre-determined for model output analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f6ed6eb-46b4-4d3d-8085-19009c083e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synethic dataset parameters\n",
    "seed = 42\n",
    "syn_factors = 6                # Number of factors in the synthetic dataset\n",
    "syn_features = 40              # Number of features in the synthetic dataset\n",
    "syn_samples = 10000             # Number of samples in the synthetic dataset\n",
    "outliers = True                # Add outliers to the dataset\n",
    "outlier_p = 0.10               # Decimal percent of outliers in the dataset\n",
    "outlier_mag = 1.25                # Magnitude of outliers\n",
    "contribution_max = 2           # Maximum value of the contribution matrix (W) (Randomly sampled from a uniform distribution)\n",
    "noise_mean_min = 0.03          # Min value for the mean of noise added to the synthetic dataset, used to randomly determine the mean decimal percentage of the noise for each feature.\n",
    "noise_mean_max = 0.05          # Max value for the mean of noise added to the synthetic dataset, used to randomly determine the mean decimal percentage of the noise for each feature.\n",
    "noise_scale = 0.1              # Scale of the noise added to the synthetic dataset\n",
    "uncertainty_mean_min = 0.04    # Min value for the mean uncertainty of a data feature, used to randomly determine the mean decimal percentage for each feature in the uncertainty dataset. \n",
    "uncertainty_mean_max = 0.06    # Max value for the mean uncertainty of a data feature, used to randomly determine the mean decimal percentage for each feature in the uncertainty dataset. \n",
    "uncertainty_scale = 0.01       # Scale of the uncertainty matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cd06d50-6afb-4cdf-a20c-3a487b8a7a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-Apr-25 16:07:12 - Synthetic profiles generated\n"
     ]
    }
   ],
   "source": [
    "# Initialize the simulator with the above parameters\n",
    "simulator = Simulator(seed=seed,\n",
    "                      factors_n=syn_factors,\n",
    "                      features_n=syn_features,\n",
    "                      samples_n=syn_samples,\n",
    "                      outliers=outliers,\n",
    "                      outlier_p=outlier_p,\n",
    "                      outlier_mag=outlier_mag,\n",
    "                      contribution_max=contribution_max,\n",
    "                      noise_mean_min=noise_mean_min,\n",
    "                      noise_mean_max=noise_mean_max,\n",
    "                      noise_scale=noise_scale,\n",
    "                      uncertainty_mean_min=uncertainty_mean_min,\n",
    "                      uncertainty_mean_max=uncertainty_mean_max,\n",
    "                      uncertainty_scale=uncertainty_scale\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68bb55be-07a1-44a3-98a0-d91855b9d81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example command for passing in a custom factor profile matrix, instead of the randomly generated profile matrix.\n",
    "# my_profile = np.ones(shape=(syn_factors, syn_features))\n",
    "# simulator.generate_profiles(profiles=my_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b5983ad-0d02-4938-9943-31bf0f889414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to customize the factor contributions. Curve_type options: 'uniform', 'decreasing', 'increasing', 'logistic', 'periodic'\n",
    "# simulator.update_contribution(factor_i=0, curve_type=\"logistic\", scale=0.1, frequency=0.5)\n",
    "# simulator.update_contribution(factor_i=1, curve_type=\"periodic\", minimum=0.0, maximum=1.0, frequency=0.5, scale=0.1)\n",
    "# simulator.update_contribution(factor_i=2, curve_type=\"increasing\", minimum=0.0, maximum=1.0, scale=0.1)\n",
    "# simulator.update_contribution(factor_i=3, curve_type=\"decreasing\", minimum=0.0, maximum=1.0, scale=0.1)\n",
    "# simulator.plot_synthetic_contributions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d605314-c34a-434d-a178-81b6f6b3766d",
   "metadata": {},
   "source": [
    "#### Load Data\n",
    "Assign the processed data and uncertainty datasets to the variables V and U. These steps will be simplified/streamlined in a future version of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ee36d66-7066-44d1-ae0b-b8b3e2413bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-Apr-25 16:07:13 - Synthetic data generated\n",
      "21-Apr-25 16:07:14 - Synthetic uncertainty data generated\n",
      "21-Apr-25 16:07:14 - Synthetic dataframes completed\n",
      "21-Apr-25 16:07:14 - Synthetic source apportionment instance created.\n"
     ]
    }
   ],
   "source": [
    "syn_input_df, syn_uncertainty_df = simulator.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a0b90b7-c68c-4d64-af4b-47033406e5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = DataHandler.load_dataframe(input_df=syn_input_df, uncertainty_df=syn_uncertainty_df)\n",
    "V, U = data_handler.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f532c5-1e71-4966-8edf-e8f68fa0bd57",
   "metadata": {},
   "source": [
    "#### Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af6db49d-8c25-467e-8456-0b53bb7aba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_col = \"Date\"                  # the index of the input/uncertainty datasets\n",
    "# factors = syn_factors               # the number of factors\n",
    "factors = 6\n",
    "method = \"ls-nmf\"                   # \"ls-nmf\", \"ws-nmf\"\n",
    "models = 20                         # the number of models to train\n",
    "init_method = \"col_means\"           # default is column means \"col_means\", \"kmeans\", \"cmeans\"\n",
    "init_norm = True                    # if init_method=kmeans or cmeans, normalize the data prior to clustering.\n",
    "seed = 42                           # random seed for initialization\n",
    "max_iterations = 20000              # the maximum number of iterations for fitting a model\n",
    "converge_delta = 0.1                # convergence criteria for the change in loss, Q\n",
    "converge_n = 25                     # convergence criteria for the number of steps where the loss changes by less than converge_delta\n",
    "verbose = True                      # adds more verbosity to the algorithm workflow on execution.\n",
    "optimized = True                    # use the Rust code if possible\n",
    "parallel = True                     # execute the model training in parallel, multiple models at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d60233-99a4-42de-9e94-6ba1ac84afce",
   "metadata": {},
   "source": [
    "### Train Batch Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f6bb60-a22a-4be5-a9eb-0f5f93d61f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_W(V, U, H):\n",
    "    H[H <= 0.0] = 1e-8\n",
    "    W = np.matmul(V * np.divide(1, U), H.T)\n",
    "    return W\n",
    "\n",
    "def q_loss(V, U, H, W):\n",
    "    residuals = (V-np.matmul(W, H))/U\n",
    "    return np.sum(residuals)\n",
    "\n",
    "def mse(V, U, H, W):\n",
    "    WH = np.matmul(W, H)\n",
    "    residuals = ((V-WH)/U)**2\n",
    "    return np.sum(residuals)/V.size\n",
    "\n",
    "def compare_H(H1, H2):\n",
    "    correlation_matrix = np.zeros((H1.shape[0], H2.shape[0]))\n",
    "    for i in range(H1.shape[0]):\n",
    "        f1 = H1[i].astype(float)\n",
    "        for j in range(H2.shape[0]):\n",
    "            f2 = H2[j].astype(float)\n",
    "            corr_matrix = np.corrcoef(f2, f1)\n",
    "            corr = corr_matrix[0, 1]\n",
    "            r_sq = corr ** 2\n",
    "            correlation_matrix[i,j] = r_sq\n",
    "    return correlation_matrix\n",
    "            \n",
    "def plot_correlations(matrix):\n",
    "    header = [f\"Factor {i}\" for i in range(matrix.shape[0])]\n",
    "    fig = go.Figure(data=[go.Table(header=dict(values=header), cells=dict(values=matrix))])\n",
    "    fig.show()\n",
    "\n",
    "def prepare_data(V, U, i_selection):\n",
    "    _V = pd.DataFrame(V.copy()[i_selection,:])\n",
    "    _U = pd.DataFrame(U.copy()[i_selection,:])\n",
    "    \n",
    "    for f in _V.columns:\n",
    "        _V[f] = pd.to_numeric(_V[f])\n",
    "        _U[f] = pd.to_numeric(_U[f])\n",
    "    return _V.to_numpy(), _U.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a498165d-af9f-4a2f-baf4-71a575001f71",
   "metadata": {},
   "source": [
    "### Version 2 of the FactorCatalog\n",
    "\n",
    "FactorCatalog V2 takes a more robust approach to grouping factor profiles from multiple models with the grouping occuring after all factor profiles have been collected. The updated procedure\n",
    "is designed to be used to investigate potential solutions for creating models for very large datasets using subsets of the data. The algorithm is described as:\n",
    "1. Specify your hyper-parameters: samples_n, batches_n, models_n, random_seed, correlation_threshold, factors_k\n",
    "2. For each batch in batches_n.\n",
    "3. Create a subset dataset using samples_n randomly selected values from V/U.\n",
    "4. Created a batchSA instance of models_n, using random_seed and factors_k.\n",
    "5. Add each output factor to the FactorCatalog (factor model_i, fit_rmse, factor_i, H)\n",
    "6. Once all batches are completed, cluster the factor collection using a constrained k-means cluster function.\n",
    "7. Score the models based upon a heuristic, such as the sum of (cluster_cor_avg*cluster_members)\n",
    "8. Evaluate the clustered profile matrix, using the cluster centroid values, using the complete dataset.\n",
    "\n",
    "The primary modification of the FactorCatalog is to use the constrained k-means clustering function for grouping 'like' factor profiles. The procedure will work by:\n",
    "1. Starting with the factors_k=clusters_n, calculate the correlation of all factors/points to the centroids, by model.\n",
    "2. Initialize the clusters by randomly creating clusters or by selecting clusters_n 'dissimilar' factors.\n",
    "3. Randomly shuffle the model order for factor assignment:\n",
    "   1. Assign the factors to the clusters by order of correlation, closer to 1.0 goes first.\n",
    "   2. If more than one factor in a model would be assigned to the same cluster assign the factor with the highest cor to that cluster and then repeat excluding that cluster(s) until all factors are assigned.\n",
    "   3. If any given factor does not have a correlation above the specified threshold, create a new cluster centered at that point.\n",
    "4. At the end of each assignment iteration, remove any cluster which has no members.\n",
    "5. Once max_assignment_n iterations is reached stop, or when a reassignment doesn't change.\n",
    "\n",
    "The constrained k-means clustered is a standard clustering approach with the exception that the distance is calculated as 1/r2 of the point to the cluster centroid. Two other differences are a) the number of clusters can increase and decrease depending on correlation threshold and by the constraint that a model can only contribute one factor to any given cluster.\n",
    "\n",
    "Once all the factors for all models have been clustered, the FactorCatalog models can be scored based upon the heuristic stated in stage A7. The best model, or any selected model, factor profile matrix (H) can then be selected for final evaluation. The factor profile H is not what was produced by the model, but is the mean values of the FactorCatalog's factor (the centroid of the cluster that those factors were assigned to). This approach allows for the factor profile values to be provided as a distributed of possible values for each feature, or demonstrating potential uncertainty in the factor profile. The clustered factor profile is then used to fit the full dataset, but keeping H constant. The loss can then be evaluated against what is calculated for a long-running brute force approach. \n",
    "\n",
    "An evaluation of the impact on the model/W matrix and loss given a random selection, MC, simulation of the factor profile would be an interesting next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b21130-43de-4a22-8ffb-123a3fa674fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Factor:\n",
    "    def __init__(self,\n",
    "                 factor_id,\n",
    "                 profile,\n",
    "                 model_id\n",
    "                ):\n",
    "        self.factor_id = factor_id\n",
    "        self.profile = profile\n",
    "        self.model_id = model_id\n",
    "        self.cluster_id = None\n",
    "        self.cor = None\n",
    "\n",
    "    def assign(self, cluster_id, cor):\n",
    "        self.cluster_id = cluster_id\n",
    "        self.cor = cor\n",
    "\n",
    "    def distance(self, cluster):\n",
    "        f1 = np.array(self.profile).astype(float)\n",
    "        f2 = np.array(cluster).astype(float)\n",
    "        corr_matrix = np.corrcoef(f2, f1)\n",
    "        corr = corr_matrix[0, 1]\n",
    "        r_sq = corr ** 2\n",
    "        return r_sq\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self,\n",
    "                 model_id):\n",
    "        self.model_id = model_id\n",
    "        self.factors = []\n",
    "\n",
    "        self.score = None\n",
    "        \n",
    "    def add_factor(self, factor):\n",
    "        self.factors.append(factor)\n",
    "\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(self,\n",
    "                 centroid: np.ndarray\n",
    "                ):\n",
    "        self.centroid = centroid\n",
    "        self.factors = []\n",
    "        self.count = 0\n",
    "\n",
    "        self.mean_r2 = 0\n",
    "        self.std = 0\n",
    "        self.min_values = np.full(len(centroid), np.nan)\n",
    "        self.max_values = np.full(len(centroid), np.nan)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.count\n",
    "\n",
    "    def add(self, factor: Factor):\n",
    "        self.factors.append(factor)\n",
    "        self.count += 1\n",
    "        self.min_values = np.minimum(self.min_values, factor)\n",
    "        self.max_values = np.maximum(self.max_values, factor)\n",
    "        self.mean_r2 = np.mean([factor.cor for factor in self.factors])\n",
    "        self.std = np.std([factor.profile for factor in self.factors], axis=0)\n",
    "\n",
    "    def purge(self):\n",
    "        self.factors = []\n",
    "        self.count = 0\n",
    "        self.mean_r2 = 0\n",
    "        self.std = 0\n",
    "        self.min_values = np.full(len(centroid), np.nan)\n",
    "        self.max_values = np.full(len(centroid), np.nan)\n",
    "\n",
    "    def recalculate(self):\n",
    "        profile_list = np.array([factor.profile for factor in self.factors])\n",
    "        new_centroid = np.mean(profile_list, axis=0)\n",
    "        self.centroid = new_centroid\n",
    "\n",
    "\n",
    "class BatchFactorCatalog:\n",
    "    def __init__(self\n",
    "                 n_factors: int,\n",
    "                 threshold: float = 0.8,\n",
    "                 seed: int = 42\n",
    "                ):\n",
    "        self.n_factors = n_factors\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        self.models = {}\n",
    "        self.model_count = 0\n",
    "        self.factor_collection = {}\n",
    "        self.factor_count = 0\n",
    "\n",
    "        # Min and max values for all factor vectors, used for random initialization of the centroids in clustering\n",
    "        self.factor_min = None\n",
    "        self.factor_max = None\n",
    "\n",
    "        self.clusters = []\n",
    "\n",
    "    def add_model(self, model: SA, norm: bool = True):\n",
    "        model_id = self.model_count\n",
    "        model_factor_ids = []\n",
    "        norm_H = model.H / np.sum(model.H, axis=0)\n",
    "        model = Model(model_id=model_id)\n",
    "        for i in range(model.H.shape[0]):\n",
    "            factor_id = self.factor_count\n",
    "            self.factor_count += 1\n",
    "            model_factor_ids.append(factor_id)\n",
    "            i_H = norm_H if norm else model.H \n",
    "            factor = Factor(factor_id=factor_id, profile=i_H[i], model_id=model_id)\n",
    "            \n",
    "            model.add_factor(factor)\n",
    "            self.factor_collection[str(factor_id)] = factor\n",
    "            self.update_ranges(i_H[i])\n",
    "            \n",
    "        self.models[str(model_id)] = model\n",
    "        self.model_count += 1\n",
    "\n",
    "    def score(self):\n",
    "        pass\n",
    "\n",
    "    def update_ranges(self, factor):\n",
    "        if self.factor_min is None and self.factor_max is None:\n",
    "            self.factor_min = copy.copy(factor.profile)\n",
    "            self.factor_max = copy.copy(factor.profile)\n",
    "        else:\n",
    "            self.factor_min = np.minimum(self.factor_min, factor.profile)\n",
    "            self.factor_max = np.maximum(self.factor_max, factor.profile)\n",
    "\n",
    "    def initialize_clusters(self):\n",
    "        for k in range(self.n_factors):\n",
    "            new_centroid = np.zeros(len(self.factor_min))\n",
    "            for i in range(len(self.factor_min)):\n",
    "                i_v = self.rng.uniform(low=self.factor_min[i], high=self.factor_max[i])\n",
    "                new_centroid[i] = i_v\n",
    "            cluster = Cluster(centroid=new_centroid)\n",
    "            self.clusters.append(cluster)\n",
    "\n",
    "    def purge_clusters(self):\n",
    "        for cluster in self.clusters:\n",
    "            cluster.purge()\n",
    "\n",
    "    def distance(self, factor1, factor2):\n",
    "        f1 = np.array(factor1).astype(float)\n",
    "        f2 = np.array(factor2).astype(float)\n",
    "        corr_matrix = np.corrcoef(f2, f1)\n",
    "        corr = corr_matrix[0, 1]\n",
    "        r_sq = corr ** 2\n",
    "        return r_sq\n",
    "\n",
    "    def calculate_centroids(self, cluster):\n",
    "        new_centroid = list(np.mean(np.array(cluster), axis=0))\n",
    "        return new_centroid\n",
    "\n",
    "    def cluster_cleanup(self):\n",
    "        drop_clusters = []\n",
    "        for i in range(len(self.centroids)):\n",
    "            centroid_i = self.centroids[i]\n",
    "            for j in range(i, len(self.centroids)):\n",
    "                centroid_j = centroids[j]\n",
    "                ij_cor = self.distance(centroid_i.centroid centroid_j.centroid)\n",
    "                if ij > self.threshold:\n",
    "                    smaller_cluster = i if len(clusters[i]) < len(clusters[j]) else j\n",
    "                    if smaller_cluster not in drop_clusters:\n",
    "                        drop_clusters.append(smaller_cluster)\n",
    "        new_clusters = []\n",
    "        for i in range(len(self.centroids)):\n",
    "            if i not in drop_clusters:\n",
    "                new_clusters.append(clusters[i])\n",
    "        self.clusters = new_clusters             \n",
    "\n",
    "    def run(self, max_iterations: int = 200):\n",
    "        self.initialize_clusters()\n",
    "        # The initial number of clusters is equal to n_factors\n",
    "        n_clusters = self.n_factors\n",
    "        \n",
    "        converged = False\n",
    "        current_iter = 0\n",
    "        while not converged:\n",
    "            if current_iter >= max_iterations:\n",
    "                logger.info(f\"Factor clustering did not converge after {max_iterations} iterations.\")\n",
    "                break\n",
    "            self.purge_clusters()\n",
    "\n",
    "            model_list = self.rng.permutation(self.models.keys())\n",
    "            for model_i in model_list:\n",
    "                model_factors = self.models[model_i].factors\n",
    "                factor_dist = {}\n",
    "                factor_hi = {}\n",
    "                # Calculate distances for all factors in the model to all centroids and then order the distances.\n",
    "                for factor_i in model_factors:\n",
    "                    distances = [(j, self.distance(self.factor_collection[str(factor_i)][1], centroid)) for j, centroid in enumerate(centroids)]\n",
    "                    distances.sorted(key=lambda x: x[1], reverse=True)\n",
    "                    factor_dist[str(factor_i)] = distances\n",
    "                    factor_hi[str(factor_i)] = distances[0]\n",
    "                already_assigned = []\n",
    "                factor_hi = dict(sorted(x.items(), key=lambda x: x[1], reverse=True))\n",
    "                # Assign factors to clusters, if model hasn't contributed to the cluster already and if the correlation is above the threshold\n",
    "                for factor_id in factor_hi.keys():\n",
    "                    # iterate through list of clusters until the first one that hasn't already been assigned.\n",
    "                    cluster_idx = -1\n",
    "                    for cluster_i, correlation_i in factor_dist[factor_id].items():\n",
    "                        if cluster_i not in already_assigned and correlation_i >= self.threshold:\n",
    "                            cluster_idx = cluster_i\n",
    "                            break\n",
    "                    if cluster_idx != -1:\n",
    "                        clusters[cluster_idx].append(factor_id)\n",
    "                        already_assigned.append(cluster_idx)\n",
    "                    else:\n",
    "                        n_clusters += 1\n",
    "                        clusters.append([factor_id])\n",
    "                        already_assigned.append(len(clusters))\n",
    "\n",
    "                # Recalculate centroids of clusters\n",
    "                new_centroids = [self.calculate_centroids([factor_collection[str(factor_i)][1] for factor_i in cluster]) for cluster in clusters]\n",
    "                new_centroids, clusters =  self.cluster_cleanup(new_centroids, clusters)\n",
    "                if new_centroids == centroids:\n",
    "                    converge = True\n",
    "                else:\n",
    "                    centroids = new_centroids\n",
    "                current_iter += 1\n",
    "                    \n",
    "        self.centroids = centroids\n",
    "        self.clusters = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ca56a-a1a7-47fe-ab68-c743fab19038",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rng = np.random.default_rng(seed)\n",
    "batch_size = 1000\n",
    "max_batches = 10\n",
    "i_batches = 0\n",
    "\n",
    "best_mse = float('inf')\n",
    "best_model = 0\n",
    "\n",
    "i_H = None\n",
    "i_selection = rng.choice(syn_samples, size=batch_size, replace=False, shuffle=True)\n",
    "i_V, i_U = prepare_data(V=V, U=U, i_selection=i_selection)\n",
    "\n",
    "factor_catalog = BatchFactorCatalog(n_factors=factors, threshold=0.8, seed=42)\n",
    "\n",
    "change_p = 0.1\n",
    "with tqdm(range(max_batches*2), desc=\"Generating subset profiles. \") as pbar:\n",
    "    for i in range(max_batches):\n",
    "        if i > 0:\n",
    "            j_selection = rng.choice(syn_samples, size=int(batch_size*change_p), replace=False, shuffle=True)\n",
    "            idx_change = rng.choice(batch_size, size=int(batch_size*change_p), replace=False, shuffle=True)\n",
    "            i_selection[idx_change] = j_selection\n",
    "            i_V, i_U = prepare_data(V=V, U=U, i_selection=i_selection)\n",
    "\n",
    "        batch_sa = BatchSA(V=train_V, U=train_U, H=batch_H, factors=k, models=n_models, method=method, seed=initialization_seed, max_iter=max_iter,\n",
    "                            converge_delta=converge_delta, converge_n=converge_n, verbose=False)\n",
    "        _ = sa_models_b.train()\n",
    "        pbar.update(1)\n",
    "\n",
    "        factor_catalog.add_model(model=sa, norm=True)\n",
    "        \n",
    "        pbar.set_description(f\"Generating subset profiles.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c070656-72c6-4b5d-a72e-5be2d225f6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c393cf-4e2b-41a4-97ab-e3aaf28a0f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3263369146d543f8b6a63d8f037caeaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating common profile. MSE: NA:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_sa = SA(factors=factors, method=method, V=V, U=U, seed=seed, verbose=True)\n",
    "final_sa.initialize(H=i_H, W=None)\n",
    "run = final_sa.train(max_iter=2000, converge_delta=converge_delta, converge_n=converge_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "288fbc3c-006b-4991-97dc-67f441966297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_bi_matrix = csr_matrix(correlation)\n",
    "# model_mapping = list(min_weight_full_bipartite_matching(m_bi_matrix, maximize=True))\n",
    "# model_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b8af10a-cd1a-43ea-9a36-6b33f4531606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cor_values = correlation[model_mapping[0], model_mapping[1]]\n",
    "# cor_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5ce040-cef1-4274-a0f4-a259f9fe6624",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=go.Scatter(x=list(range(len(batch_mse))), y=batch_mse, mode='lines'))\n",
    "fig.update_layout(\n",
    "    title=\"Aggregated MSE over batches\",\n",
    "    xaxis_title=\"Batches\",\n",
    "    yaxis_title=\"MSE\",\n",
    "    width=1200,\n",
    "    height=800\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f561829-6c19-4eeb-b8d3-3f83180b9794",
   "metadata": {},
   "source": [
    "#### Train Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "018eb975-f9dd-4e73-bf1d-8c5b696e6dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08-Apr-25 15:50:34 - Batch Source Apportionment Instance Configuration\n",
      "08-Apr-25 15:50:34 - -------------------------------------------------\n",
      "08-Apr-25 15:50:34 - Factors: 6, Method: ls-nmf, Models: 20\n",
      "08-Apr-25 15:50:34 - Max Iterations: 20000, Converge Delta: 0.1, Converge N: 25\n",
      "08-Apr-25 15:50:34 - Random Seed: 42, Init Method: col_means\n",
      "08-Apr-25 15:50:34 - Parallel: True, Verbose: True\n",
      "08-Apr-25 15:50:34 - -------------------------------------------------\n",
      "08-Apr-25 15:50:34 - Estimated memory available: 103.7799 Gb\n",
      "08-Apr-25 15:50:34 - Estimated memory per model: 183.4595 MB\n",
      "08-Apr-25 15:50:34 - Estimated maximum number of cores: 16\n",
      "08-Apr-25 15:50:34 - Using 12 cores for parallel processing.\n",
      "08-Apr-25 15:50:34 - -------------------------------------------------\n",
      "08-Apr-25 15:50:34 - Running batch SA models in parallel using 12 cores.\n",
      "08-Apr-25 16:35:57 - Model: 1, Q(true): 1931058.125, MSE(true): 4.827600002288818, Q(robust): 1402891.375, MSE(robust): 3.507200002670288, Seed: 8925, Converged: True, Steps: 10134/20000\n",
      "08-Apr-25 16:35:57 - Model: 2, Q(true): 1566901.875, MSE(true): 3.91729998588562, Q(robust): 1163277.625, MSE(robust): 2.9082000255584717, Seed: 77395, Converged: False, Steps: 19999/20000\n",
      "08-Apr-25 16:35:57 - Model: 3, Q(true): 1473080.75, MSE(true): 3.6826999187469482, Q(robust): 1124798.0, MSE(robust): 2.812000036239624, Seed: 65457, Converged: True, Steps: 5146/20000\n",
      "08-Apr-25 16:35:57 - Model: 4, Q(true): 1141289.375, MSE(true): 2.8531999588012695, Q(robust): 1006053.0625, MSE(robust): 2.5151000022888184, Seed: 43887, Converged: False, Steps: 19999/20000\n",
      "08-Apr-25 16:35:57 - Model: 5, Q(true): 2213198.75, MSE(true): 5.5329999923706055, Q(robust): 1455481.5, MSE(robust): 3.638700008392334, Seed: 43301, Converged: True, Steps: 14182/20000\n",
      "08-Apr-25 16:35:57 - Model: 6, Q(true): 1143238.625, MSE(true): 2.858099937438965, Q(robust): 1005367.3125, MSE(robust): 2.513400077819824, Seed: 85859, Converged: True, Steps: 4236/20000\n",
      "08-Apr-25 16:35:57 - Model: 7, Q(true): 1567352.625, MSE(true): 3.9184000492095947, Q(robust): 1177293.0, MSE(robust): 2.94320011138916, Seed: 8594, Converged: False, Steps: 19999/20000\n",
      "08-Apr-25 16:35:57 - Model: 8, Q(true): 2286038.75, MSE(true): 5.715099811553955, Q(robust): 1447606.875, MSE(robust): 3.61899995803833, Seed: 69736, Converged: True, Steps: 3581/20000\n",
      "08-Apr-25 16:35:57 - Model: 9, Q(true): 1765669.25, MSE(true): 4.4141998291015625, Q(robust): 1239612.25, MSE(robust): 3.0989999771118164, Seed: 20146, Converged: True, Steps: 15582/20000\n",
      "08-Apr-25 16:35:57 - Model: 10, Q(true): 1171629.375, MSE(true): 2.9291000366210938, Q(robust): 1022638.8125, MSE(robust): 2.5566000938415527, Seed: 9417, Converged: False, Steps: 19999/20000\n",
      "08-Apr-25 16:35:57 - Model: 11, Q(true): 1163082.625, MSE(true): 2.9077000617980957, Q(robust): 1014815.625, MSE(robust): 2.5369999408721924, Seed: 52647, Converged: True, Steps: 5016/20000\n",
      "08-Apr-25 16:35:57 - Model: 12, Q(true): 1767402.125, MSE(true): 4.418499946594238, Q(robust): 1234565.25, MSE(robust): 3.086400032043457, Seed: 97562, Converged: True, Steps: 7430/20000\n",
      "08-Apr-25 16:35:57 - Model: 13, Q(true): 1773076.25, MSE(true): 4.432700157165527, Q(robust): 1240438.0, MSE(robust): 3.101099967956543, Seed: 73575, Converged: False, Steps: 19999/20000\n",
      "08-Apr-25 16:35:57 - Model: 14, Q(true): 1140279.875, MSE(true): 2.8506999015808105, Q(robust): 1004895.75, MSE(robust): 2.512200117111206, Seed: 76113, Converged: True, Steps: 16592/20000\n",
      "08-Apr-25 16:35:57 - Model: 15, Q(true): 2057827.75, MSE(true): 5.144599914550781, Q(robust): 1293522.5, MSE(robust): 3.233799934387207, Seed: 71747, Converged: False, Steps: 19999/20000\n",
      "08-Apr-25 16:35:57 - Model: 16, Q(true): 1462389.0, MSE(true): 3.6559998989105225, Q(robust): 1124415.125, MSE(robust): 2.811000108718872, Seed: 78606, Converged: True, Steps: 7922/20000\n",
      "08-Apr-25 16:35:57 - Model: 17, Q(true): 1473427.25, MSE(true): 3.6835999488830566, Q(robust): 1138073.375, MSE(robust): 2.8452000617980957, Seed: 51322, Converged: True, Steps: 15759/20000\n",
      "08-Apr-25 16:35:57 - Model: 18, Q(true): 1990831.25, MSE(true): 4.977099895477295, Q(robust): 1312666.625, MSE(robust): 3.2816998958587646, Seed: 12811, Converged: True, Steps: 4369/20000\n",
      "08-Apr-25 16:35:57 - Model: 19, Q(true): 1350639.875, MSE(true): 3.3766000270843506, Q(robust): 1056610.875, MSE(robust): 2.6414999961853027, Seed: 83974, Converged: True, Steps: 4748/20000\n",
      "08-Apr-25 16:35:57 - Model: 20, Q(true): 1391578.375, MSE(true): 3.4788999557495117, Q(robust): 1097394.0, MSE(robust): 2.743499994277954, Seed: 45038, Converged: True, Steps: 14150/20000\n",
      "08-Apr-25 16:35:57 - Results - Best Model: 14, Q(true): 1140279.875, MSE(true): 2.8506999015808105, Q(robust): 1004895.75, MSE(robust): 2.512200117111206, Converged: True\n",
      "08-Apr-25 16:35:57 - Runtime: 45.38 min(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.06 s\n",
      "Wall time: 45min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training multiple models, optional parameters are commented out.\n",
    "sa_models = BatchSA(V=V, U=U, factors=factors, models=models, method=method, seed=seed, max_iter=max_iterations,\n",
    "                    init_method=init_method, init_norm=init_norm,\n",
    "                    converge_delta=converge_delta, converge_n=converge_n, \n",
    "                    parallel=parallel,\n",
    "                    verbose=True\n",
    "                   )\n",
    "_ = sa_models.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682471c-d3d6-4f5e-9fc1-5bb33f1d8fdd",
   "metadata": {},
   "source": [
    "#### Batch Analysis\n",
    "\n",
    "These methods allow for plotting and reviewing of the overall results of the collection of models produced by the BatchSA training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d908b2f-5bef-4a13-968d-bfa4a8d1b002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_models.results[0].H[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5f719a-f7c2-45d9-8e0d-787d311a4ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform batch model analysis\n",
    "batch_analysis = BatchAnalysis(batch_sa=sa_models, data_handler=data_handler)\n",
    "# Plot the loss of the models over iterations\n",
    "batch_analysis.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bb4e83-0fe9-464d-afcc-bee53123fafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss distribution for the batch models\n",
    "batch_analysis.plot_loss_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c65c8d-7826-420d-a4f7-67bc0fcae49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the temporal residuals for each model, the loss by sample, for a specified feature\n",
    "batch_analysis.plot_temporal_residuals(feature_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1215ea-feba-4c9f-9377-77097fd16408",
   "metadata": {},
   "source": [
    "### Compare to Synthetic Data\n",
    "\n",
    "Compare the set of batch models to the original synthetic factor data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eafe023-e783-4c17-bf08-7242f375cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator.compare(batch_sa=sa_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857aca0f-44bd-4af7-be54-63812f3647e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator.plot_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f83183-e723-49fe-888c-cfa1e43d7595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best mapping of the most optimal model (by loss), plot those mapping results\n",
    "# simulator.plot_comparison(model_i=sa_models.best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfd4755-3533-4cc0-9aa3-4a82a2233691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Simulator instance, saves the instance as a pickle file and saves the synthetic profiles, contributions, data and uncertainty as csv files.\n",
    "# sim_name = \"synthetic\"\n",
    "# sim_output_dir = \"D:/git/esat/notebooks/\"\n",
    "# simulator.save(sim_name=sim_name, output_directory=sim_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316925f4-4e66-4a32-a086-310168b28d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a previously saved Simulator instance\n",
    "# simulator_file = \"D:/git/esat/notebooks/esat_simulator.pkl\"\n",
    "# simulator_2 = Simulator.load(file_path=simulator_file)\n",
    "# simulator_2.factor_compare.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34bd6f8-b78e-42de-8277-912bb142c51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selet the highest correlated model\n",
    "best_model = simulator.factor_compare.best_model\n",
    "sa_model = sa_models.results[best_model]\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c57f33-ef70-4fae-b47d-04220b773598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Model Analysis module\n",
    "model_analysis = ModelAnalysis(datahandler=data_handler, model=sa_model, selected_model=best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d2bff7-b36a-4837-95a9-1ff2bd90c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Analysis shows the scaled residual histogram, along with metrics and distribution curves. The abs_threshold parameter specifies the condition for the returned values of the function call as those residuals which exceed the absolute value of that threshold.\n",
    "abs_threshold = 3.0\n",
    "threshold_residuals = model_analysis.plot_residual_histogram(feature_idx=5, abs_threshold=abs_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df0f1b-747b-4519-8d74-b30b2cfd3db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"List of Absolute Scaled Residual Greather than: {abs_threshold}. Count: {threshold_residuals.shape[0]}\")\n",
    "threshold_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462d33c4-51b4-4c1d-b01e-7b9667638e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model output statistics for the estimated V, including SE: Standard Error metrics, and 3 normal distribution tests of the residuals (KS Normal is used in PMF5)\n",
    "model_analysis.calculate_statistics()\n",
    "model_analysis.statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d23a88-47f5-4692-ba9b-07e8630d5486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model feature observed vs predicted plot with regression and one-to-one lines. Feature/Column specified by index.\n",
    "model_analysis.plot_estimated_observed(feature_idx=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102a0280-d275-46d5-973e-48fa55fd51af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model feature timeseries analysis plot showing the observed vs predicted values of the feature, along with the residuals shown below. Feature/column specified by index.\n",
    "model_analysis.plot_estimated_timeseries(feature_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da54785-9c7f-4396-b776-f086612ace02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor profile plot showing the factor sum of concentrations by feature (blue bars), the percentage of the feature as the red dot, and in the bottom plot the normalized contributions by date (values are resampled at a daily timestep for timeseries consistency).\n",
    "# Factor specified by index.\n",
    "model_analysis.plot_factor_profile(factor_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e332d6-75b2-4682-ba27-612dfaf26b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model factor fingerprint specifies the feature percentage of each factor.\n",
    "model_analysis.plot_factor_fingerprints(grouped=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8bb85b-b48e-4031-b861-d6f6af5ad090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor G-Space plot shows the normalized contributions of one factor vs another factor. Factor specified by index.\n",
    "model_analysis.plot_g_space(factor_1=2, factor_2=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e299347b-a88c-432b-a68f-270ba15f35ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor contribution pie chart shows the percentage of factor contributions for the specified feature, and the corresponding normalized contribution of each factor for that feature (bottom plot). Feature specified by index.\n",
    "model_analysis.plot_factor_contributions(feature_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ecbf01-3cce-4fb4-a6c0-f38037e84d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f1d08-dfcb-4e52-92e8-78fecd27d6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb9320d-7179-48af-9f3e-e291e877aaf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c225f5-0f5c-4588-95eb-db15797dab8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
